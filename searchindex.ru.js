var relearn_searchindex = [
  {
    "breadcrumb": "",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Basics",
    "uri": "/basics/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.\nБаза. Утверждение верно для $n=0,1$.\nПереход. Пусть $n\u003e1$, тогда $F_{n}=F_{n-1}+F_{n-2} \\leqslant 2^{n-1}+2^{n-2}\u003c2^{n}$.\nС другой стороны, $F_{n} \\geqslant 2 \\cdot F_{n-2} \\geqslant 2 \\cdot 2^{\\lfloor(n-2) / 2\\rfloor}=2^{\\lfloor n / 2\\rfloor}$.\nЭкспоненциальный алгоритм Следующий рекурсивный алгоритм вычисляет $n$-ое число Фибоначчи, точно следуя определению:\n1 2 3 4 fib(n): if n \u003c= 1: return 1 return fib(n - 1) + fib(n - 2) Каково время работы этого алгоритма? Оценим $T(n)$ - суммарное количество вызовов fib , происходящих при выполнении $\\mathrm{fib}(\\mathrm{n})$ : если $n \\leqslant 1$, то $T(n)=1$; иначе\n$$ T(n)=1+T(n-1)+T(n-2) . $$Несложно доказать по индукции, что $F_{n} \\leqslant T(n)\u003c2 F_{n}$. В каждом вызове fib совершается ограниченное число (скажем, не больше пяти) операций, поэтому время работы алгоритма примерно пропорционально $F_{n}$ (чуть позже у нас появится формальное определение этого “примерно”).\nИз леммы 1.1.1 следует, например, что, $F_{300} \\geqslant 2^{150}\u003e10^{45}$. На компьютере, выполняющем $10^{9}$ операций в секунду, fib(300) будет выполняться больше $10^{36}$ секунд.\nПолиномиальный алгоритм Посмотрим на дерево рекурсивных вызовов алгоритма fib. Видно, что алгоритм много раз вычисляет одно и то же. Давайте сохранять результаты промежуточных вычислений в массив: 1 2 3 4 5 6 7 8 fibFast(n): if n \u003c= 1: return 1 int f[n + 1] # создаём массив с индексами 0..n f[0] = f[1] = 1 for i = 2..n: f[i] = f[i - 1] + f[i - 2] return f[n] На каждой итерации цикла совершается одно сложение, всего итераций примерно $n$, поэтому количество сложений, выполняемых в ходе нового алгоритма, примерно пропорционально $n$. Есть ещё одна тонкость - из леммы 1.1.1 следует, что двоичная запись $F_{n}$ имеет длину порядка $n$. Чуть позже мы увидим, что сложение двух $n$-битовых чисел требует порядка $n$ элементарных операций, значит общее время работы нового алгоритма примерно пропорционально $n^{2}$. С помощью fibFast можно уже за разумное время вычислить не только $F_{300}$, но и $F_{100000}$.\n1.2 О-символика Хорошо себя зарекомендовал и стал общепринятым следующий подход - оценивать время работы алгоритма некоторой функцией от входных параметров, при этом пренебрегая ограниченными множителями. Это позволяет эффективно сравнивать алгоритмы между собой, при этом не нужно заниматься точным подсчётом количества элементарных операций. Примерно так мы и рассуждали об алгоритмах вычисления чисел Фибоначчи. Введём несколько обозначений, которые помогут проводить подобные рассуждения более кратко и точно.\nВремя работы алгоритма - это, конечно, всегда неотрицательная функция. Тем не менее, мы даём следующие определения для функций, принимающих произвольные вещественные значения, поскольку такие функции часто возникают в процессе рассуждений.\nОпределение 1.2.1 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=O(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$. $f=\\Omega(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$. $f=\\Theta(g)$, если существуют такие $C_{1}\u003e0, C_{2}\u003e0, N\u003e0$, что для любого $n\u003eN$ : $C_{1} \\cdot|g(n)| \\leqslant|f(n)| \\leqslant C_{2} \\cdot|g(n)|$. Запись $f=O(g)$ можно понимать как \" $|f| \\leqslant|g|$ с точностью до константы\". Аналогично, $\\Omega(\\cdot)$ можно считать аналогом $\\geqslant$, а $\\Theta(\\cdot)$ - аналогом $=$. Ещё один способ понимать запись $f=O(g)$ - отношение $\\frac{|f(n)|}{|g(n)|}$ ограничено сверху некоторой константой.\nМногие естественные свойства операторов сравнения $\\leqslant,=, \\geqslant$ выполняются и для их асимптотических аналогов. Сформулируем некоторые из этих свойств:\nСвойства 1.2.1 $f=\\Theta(g)$ тогда и только тогда, когда $f=O(g)$ и $f=\\Omega(g)$. $f=O(g)$ тогда и только тогда, когда $g=\\Omega(f)$. $f=\\Theta(g)$ тогда и только тогда, когда $g=\\Theta(f)$. Доказательство Докажем для примера п. 2.\nПусть $C\u003e0$, тогда $|f(n)| \\leqslant C \\cdot|g(n)|$ равносильно $|g(n)| \\geqslant \\frac{1}{C} \\cdot|f(n)|$.\nТаким образом, $f=O(g)$ с константой $C$ тогда и только тогда, когда $g=\\Omega(f)$ с константой $\\frac{1}{C}$.\nСуществуют также асимптотические аналоги $\u003c$ и $\u003e: o(\\cdot)$ и $\\omega(\\cdot)$. Определение 1.2.2 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=o(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$.\n$f=\\omega(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$.\nЗапись $f=o(g)$ можно понимать как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к нулю с увеличением $n$ “, а запись $f=\\omega(g)$ - как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к бесконечности с увеличением $n$ “.\nСвойства 1.2.2 $f=o(g)$ тогда и только тогда, когда $g=\\omega(f)$. Если $f=O(g)$ и $g=O(h)$, то $f=O(h)$. То же верно для $\\Omega, \\Theta, o, \\omega$. Если $f=O(g), g=o(h)$, то $f=o(h)$. Если $f=\\Omega(g), g=\\omega(h)$, то $f=\\omega(h)$. Если $f=O(h)$ и $g=O(h)$, то $f+g=O(h)$. То же верно для $o$. Если $f, g \\geqslant 0$, то то же верно и для $\\Omega, \\Theta, \\omega$. Если $f=o(g)$, то $f+g=\\Theta(g)$. Если $f, g \\geqslant 0, f=O(g)$, то $f+g=\\Theta(g)$. Для любого $C \\neq 0$ верно $C \\cdot f=\\Theta(f)$. Заметим, что у последних трёх свойств нет аналогов для обычных операторов сравнения.\nДоказательство Докажем для примера п. 3 (вариант с $O$ и $о$).\nНужно показать, что для любого $C\u003e0$ найдётся $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|h(n)|$. Зафиксируем $C\u003e0$.\n$f=O(g)$, поэтому найдутся $C_{1}\u003e0, N_{1}\u003e0$, что для любого $n\u003eN_{1}:|f(n)| \\leqslant C_{1} \\cdot|g(n)|$. $g=o(h)$, поэтому найдётся такое $N_{2}\u003e0$, что для любого $n\u003eN_{2}:|g(n)| \\leqslant \\frac{C}{C_{1}} \\cdot|h(n)|$.\nТогда для любого $n\u003e\\max \\left(N_{1}, N_{2}\\right):|f(n)| \\leqslant C_{1} \\cdot|g(n)| \\leqslant C_{1} \\cdot \\frac{C}{C_{1}} \\cdot|h(n)|=C \\cdot|h(n)|$.\nВернемся к алгоритмам вычисления чисел Фибоначчи. Пользуясь новыми обозначениями, мы показали, что время работы fibFast(n) можно оценить как $O\\left(n^{2}\\right)$. При этом время работы $\\mathrm{fib}(\\mathrm{n})$ можно оценить (сверху) как $O\\left(2^{n} \\cdot n\\right)$, и (снизу) как $\\Omega\\left(2^{\\lfloor n / 2\\rfloor}\\right)$.\n1.3 Многочлены, экспоненты и логарифмы Очень часто время работы алгоритма удаётся оценить функцией, являющейся комбинацией каких-то из трёх базовых типов: многочленов, экспонент и логарифмов. Так, время работы fibFast мы оценили многочленом $n^{2}$, а время работы fib - произведением экспоненты на многочлен: $2^{n} \\cdot n$. В связи с этим полезно изучить асимптотические свойства этих функций и научиться сравнивать их между собой.\nЛемма 1.3.1 Для любых $l\u003ek$ верно $n^{k}=o\\left(n^{l}\\right)$. Доказательство Для любого $C\u003e0$, при $n \\geqslant\\left(\\frac{1}{C}\\right)^{\\frac{1}{l-k}}$ верно\n$$ n^{k} \\leqslant C \\cdot \\frac{1}{C} \\cdot n^{k} \\leqslant C \\cdot n^{l-k} \\cdot n^{k}=C \\cdot n^{l} $$ Определение 1.3.1 Многочлен - это функция, которую можно записать в виде\n$$ f(n)=a_{0}+a_{1} n+a_{2} n^{2}+\\cdots+a_{d} n^{d} $$для некоторого $d \\geqslant 0$, так, что $a_{d} \\neq 0$. Это $d$ называют степенью многочлена и обозначают как $\\operatorname{deg}(f)$.\nСледствие 1.3.2 Пусть $f(n)$ - многочлен степени $d$. Тогда $f(n)=\\Theta\\left(n^{d}\\right)$. Доказательство Пусть $f(n) = a_{0} + a_{1} n + a_{2} n^{2} + \\cdots + a_{d} n^{d}$. Из леммы 1.3.1 и п. 6 предложения 1.2.2 следует, что $a_{j} n^{j} = o\\left(n^{d}\\right)$ для любого $0 \\leqslant j",
    "description": "Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.",
    "tags": [],
    "title": "1. Анализ сложности алгоритмов",
    "uri": "/basics/complexity/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\nВ произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.\n2.1 Сложение Используем всем знакомый со школы способ сложения чисел в столбик:\n1 2 3 4 5 6 7 add(a, b, n): # a и b - двоичные записи чисел int c[n + 1] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: c[i] = a[i] + b[i] if c[i] \u003e= 2: c[i + 1] += 1, c[i] -= 2 return c Время работы алгоритма - $O(n)$, существенно быстрее нельзя, потому что столько времени занимает уже считывание входных данных или вывод ответа.\nАналогичный алгоритм можно написать для чисел, записанных в $b$-ичной системе счисления. Если сумма трёх $b$-ичных чисел помещается в 32(64)-битный тип данных, то алгоритм всё ещё будет корректен. При этом $n$-битное число будет иметь примерно $\\frac{n}{\\log b}$ цифр в $b$-ичной записи, то есть алгоритм будет работать за $O\\left(\\frac{n}{\\log b}\\right)=O(n)$, так как $\\frac{1}{\\log b}$ - это константа. Тем не менее, это может дать ускорение в несколько десятков раз, что безусловно бывает полезно на практике.\n2.2 Умножение Вспомним теперь и школьное умножение чисел в столбик (заметим лишь, что ответ имеет длину не больше $2 n$, поскольку $\\left.\\left(2^{n}-1\\right) \\cdot\\left(2^{n}-1\\right)\u003c2^{2 n}\\right)$ :\n1 2 3 4 5 6 7 8 9 10 multiply(a, b, n): # a и b - двоичные записи чисел int c[2 * n] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: for j = 0..n - 1: c[i + j] += a[i] * b[j] for i = 0..2 * n - 2: if c[i] \u003e= 2: c[i + 1] += c[i] / 2 c[i] %= 2 return c Из-за двух вложенных циклов время работы этого алгоритма - уже $O\\left(n^{2}\\right)$. Приведём альтернативный рекурсивный алгоритм умножения двух чисел, пользующийся следующим правилом:\n$$ a \\cdot b= \\begin{cases}2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { если } b \\text { чётно, } \\\\ a+2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { иначе. }\\end{cases} $$ 1 2 3 4 5 6 7 8 multiply(a, b): # a и b - двоичные записи чисел if b == 0: return 0 c = multiply(a, b / 2) # деление нацело if b % 2 == 0: return 2 * c else: return 2 * c + a В этой схематичной записи под делением на два имеется ввиду битовый сдвиг вправо (то есть взятие двоичной записи без младшего бита), под умножением на два - битовый сдвиг влево (добавление нуля в начало битовой записи). Остаток по модулю два - это младший бит числа.\nАлгоритм произведёт $O(n)$ рекурсивных вызовов, поскольку при каждом вызове длина битовой записи $b$ уменьшается на один. В каждом вызове функции происходят битовый сдвиг влево, битовый сдвиг вправо, и, возможно сложение - всего $O(n)$ элементарных операций. Таким образом, общее время работы снова $O\\left(n^{2}\\right)$.\nЗаметим, что если длины битовых записей $a$ и $b$ равны $n$ и $m$, то время работы обоих алгоритмов можно оценить как $O(n m)$.\n2.3 Деление Пусть теперь мы хотим поделить $a$ на $b$, то есть найти такие $q, r$, что $a=q b+r$ и $0 \\leqslant r \u003c b$. Здесь работает похожая идея: обозначим за $q^{\\prime}, r^{\\prime}$ результат деления $\\left\\lfloor\\frac{a}{2}\\right\\rfloor$ на $b$, тогда:\n$$ (q, r)= \\begin{cases}\\left(2 \\cdot q^{\\prime}+\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor, 2 \\cdot r^{\\prime}-\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor \\cdot b\\right), \u0026 \\text { если } a \\text { чётно, } \\\\ \\left(2 \\cdot q^{\\prime}+\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor, 2 \\cdot r^{\\prime}+1-\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor \\cdot b\\right), \u0026 \\text { иначе. }\\end{cases} $$При этом $\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor,\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor \\leqslant 1$. Получаем следующий рекурсивный алгоритм:\n1 2 3 4 5 6 7 8 9 10 divide(a, b): # a и b - двоичные записй чисел if a == 0: return 0, 0 # q = r = 0 q, r = divide(a / 2, b) # деление нацело q = 2 * q, r = 2 * r if a % 2 == 1: r += 1 if r \u003e= b: q += 1, r -= b return q, r Снова имеем $O(n)$ рекурсивных вызовов, в каждом из которых происходит константное число битовых сдвигов и сложений (вычитаний), поэтому время работы снова оценивается как $O\\left(n^{2}\\right)$.\nАльтернативный способ - школьное деление в столбик:\n1 2 3 4 5 6 7 8 9 10 divide(a, b): # a и b - двоичные записи чисел n = len(a), m = len(b) q=0 for i = (n - m)..0: # умножение на 2 ** k - битовый сдвиг числа на k влево c = b * 2 ** i if a \u003e= c: a = a - c q = q + 2 **i return q, a Время работы, как и в предыдущем случае, оценивается как $O\\left(n^{2}\\right)$. Есть и более точная в некоторых случаях оценка: если длины битовых записей $a$ и $b$ равны $n$ и $m$, то количество итераций цикла не превосходит $n-m+1$, поэтому получаем оценку $O(n(n-m+1))$.\n2.4 Алгоритм Карацубы Можно ли перемножать числа быстрее? Оказывается, что да! Следующий алгоритм был придуман советским математиком Анатолием Карацубой в 1962 году.\nБудем для удобства считать, что длина битовой записи чисел $n$ - степень двойки (если это не так, добавим ведущих нулей, при этом длина чисел увеличится не более чем вдвое, что не повлияет на асимптотическую оценку).\nРазобьём каждое из чисел на две равных половины: $a=2^{\\frac{n}{2}} a_{l}+a_{r}, b=2^{\\frac{n}{2}} b_{l}+b_{r}$. Заметим, что\n$$ a b=\\left(2^{\\frac{n}{2}} a_{l}+a_{r}\\right) \\cdot\\left(2^{\\frac{n}{2}} b_{l}+b_{r}\\right)=2^{n} a_{l} b_{l}+2^{\\frac{n}{2}}\\left(a_{l} b_{r}+a_{r} b_{l}\\right)+a_{r} b_{r} $$Напишем рекурсивный алгоритм, пользующийся этим равенством: найдём четыре произведения вдвое более коротких чисел ( $a_{l} b_{l}, a_{l} b_{r}, a_{r} b_{l}, a_{r} b_{r}$ ) рекурсивно, после чего вычислим с их помощью $a b$. Для этого нам понадобится сделать константное число сложений и битовых сдвигов (и то, и другое делается за $\\Theta(n)$ ). Обозначим за $T(n)$ время работы алгоритма на $n$-битовых числах, тогда мы получаем следующее рекуррентное соотношение:\n$$ \\begin{equation*} T(n)=4 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n) \\tag{2.1} \\end{equation*} $$Совсем скоро мы докажем теорему, из которой следует, что $T(n)=\\Theta\\left(n^{2}\\right)$. Пока мы не получили никакого выигрыша по сравнению с предыдущими алгоритмами умножения. Но оказывается, что этот алгоритм можно ускорить, заметив, что\n$$ a_{l} b_{r}+a_{r} b_{l}=\\left(a_{l}+b_{l}\\right) \\cdot\\left(a_{r}+b_{r}\\right)-a_{l} b_{l}-a_{r} b_{r} . $$Значит, достаточно рекурсивно вычислить три произведения, и время работы алгоритма будет удовлетворять уже\n$$ \\begin{equation*} T(n)=3 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n) \\tag{2.2} \\end{equation*} $$Время работы такого алгоритма по той же теореме можно оценить уже как $\\Theta\\left(n^{\\log _{2} 3}\\right)=$ $\\Theta\\left(n^{1.585 \\ldots}\\right)$.\nСтрого говоря, длина чисел $a_{l}+b_{l}, a_{r}+b_{r}$ может оказаться равна $\\frac{n}{2}+1$. Чтобы честно получить рекуррентное соотношение 2.2, можно за линейное время свести умножение $\\left(\\frac{n}{2}+1\\right)$-битных чисел к умножению $\\frac{n}{2}$-битных. На самом деле можно показать, что соотношение $T(n)=3 \\cdot T\\left(\\frac{n}{2}+1\\right)+\\Theta(n)$ даёт такую же асимптотическую оценку.\n1 2 3 4 5 6 7 8 9 10 11 multiply(a, b): # a и b - двоичные записи чисел n = max(len(a), len(b)) if n == 1: return [a[0] * b[0]] # число из одного бита a --\u003e al, ar # делим число а на две равные части b --\u003e bl, br # делим число b на две равные части x = multiply(al, bl) y = multiply(ar, br) z = multiply(al + ar, bl + br) # умножение на 2 ** k - битовый сдвиг числа на k влево return x * 2 ** n + (z - x - y) * 2 ** (n / 2) + y На практике, дойдя в рекурсии до $16(32)$-битных чисел, уже стоит воспользоваться стандартной операцией умножения.\nСуществуют и более быстрые методы умножения чисел (например, метод, основанный на быстром преобразовании Фурье), но о них мы поговорим позже.\nПример Рассмотрим произведение $1234 * 4321$.\nЗадача делится на 3 подзадачи:\n$a_{l} b_{l} = 12 \\cdot 43$ $a_{r} b_{r} = 34 \\cdot 21$ $e = \\left(a_{l}+b_{l}\\right) \\cdot\\left(a_{r}+b_{r}\\right)-a_{l} b_{l}-a_{r} b_{r} = \\left(12 + 34\\right) \\cdot \\left(43 + 21\\right) - a_{l} b_{l} - a_{r} b_{r}$ Первая подзадача ($a_{l} b_{l} = 12 \\cdot 43$) в свою очередь также резделяется на 3 подпроблемы:\n$a_{l}^\\prime b_{l}^\\prime = 1 \\cdot 4 = 4$ $a_{r}^\\prime b_{r}^\\prime = 2 \\cdot 3 = 6$ $\\left(a_{l}^\\prime+b_{l}^\\prime\\right) \\cdot\\left(a_{r}^\\prime+b_{r}^\\prime\\right)-a_{l}^\\prime b_{l}^\\prime-a_{r} b_{r}^\\prime = \\left(1 + 2\\right) \\cdot \\left(4 + 3\\right) - 4 - 6 = 11$ Ответ: $4 \\cdot 10^2 + 11 \\cdot 10 + 6 = 516$ Вторая подзадача ($a_{r} b_{r} = 34 \\cdot 21$):\n$a_{l}^{\\prime\\prime} b_{l}^{\\prime\\prime} = 3 \\cdot 2 = 6$ $a_{r}^{\\prime\\prime} b_{r}^{\\prime\\prime} = 4 \\cdot 1 = 4$ $\\left(a_{l}^{\\prime\\prime}+b_{l}^{\\prime\\prime}\\right) \\cdot\\left(a_{r}^{\\prime\\prime}+b_{r}^{\\prime\\prime}\\right)-a_{l}^{\\prime\\prime} b_{l}^{\\prime\\prime}-a_{r} b_{r}^{\\prime\\prime} = \\left(3 + 4\\right) \\cdot \\left(2 + 1\\right) - 6 - 4 = 11$ Ответ: $6 \\cdot 10^2 + 11 \\cdot 10 + 4 = 714$ Третья подзадача ($e = 46 \\cdot 64 - 516 - 714$). Аналогично вычисляем $46 \\cdot 64$:\n$4 \\cdot 6 = 24$ $6 \\cdot 4 = 24$ $\\left(4 + 6 \\right) \\cdot (6 + 4) - 24 - 24 = 52$ Ответ: $24 \\cdot 10^2 + 52 \\cdot 10 + 24 - 516 - 714 = 1714$ Финальный ответ: $1234 \\cdot 4321 = 516 \\cdot 10^4 + 1714 \\cdot 10^2 + 714 = 5,332,114$",
    "description": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\nВ произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.",
    "tags": [],
    "title": "2. Элементарная арифметика",
    "uri": "/basics/elementary_arithmetic/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nОсновная теорема о рекуррентных соотношениях Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда\n$$T(n)= \\begin{cases}\\Theta\\left(n^{c}\\right), \u0026 \\text { если } c \u003e \\log_{b} a \\\\ \\Theta\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ \\Theta\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log_{b} a\\end{cases}$$ Доказательство Пусть мы уже доказали утверждение теоремы для чисел вида $n=b^{k}$. Рассмотрим такую функцию $k(n)$, что для любого $n$ верно $b^{k(n)-1} \u003c n \\leqslant b^{k(n)}$. При уменьшении $n$ количество веток в дереве рекурсии и их размер могли только уменьшиться, поэтому\n$$ T(n) = O \\left(T\\left(b^{k(n)}\\right)\\right)= \\begin{cases}O\\left(b^{k(n) c}\\right)=O\\left(n^{c}\\right), \u0026 \\text { если } c\u003e\\log _{b} a \\\\ O\\left(b^{k(n) c} \\log \\left(b^{k(n)}\\right)\\right)=O\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ O\\left(b^{k(n) \\log _{b} a}\\right)=O\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log _{b} a\\end{cases} $$(мы пользуемся тем, что $b^{k(n)} = b \\cdot b^{k(n)-1} \u003c b n = O(n)$ ). Доказательство же точной асимптотической оценки $(\\Theta)$ для произвольного $n$ требует ещё некоторого количества технических выкладок, которые мы опустим. Желающие могут прочитать их в главе 4.6 в Кормене1.\nТеперь считаем, что $n=b^{k}$ для некоторого $k$. Раскроем рекуррентность:\n$$ \\begin{gathered} T(n)=a \\cdot T\\left(\\frac{n}{b}\\right)+\\Theta\\left(n^{c}\\right)=\\Theta\\left(n^{c}+a \\cdot\\left(\\frac{n}{b}\\right)^{c}\\right)+a^{2} T\\left(\\frac{n}{b^{2}}\\right)= \\\\ =\\cdots=\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k-1}\\right)\\right)+a^{k} T(1)= \\\\ =\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)\\right) \\end{gathered} $$(последнее слагаемое $a^{k} T(1)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(n^{c} \\cdot\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)$, так как $\\left.T(1)=\\Theta(1), n=b^{k}\\right)$. Получаем геометрическую прогрессию со знаменателем $q=\\frac{a}{b^{c}}$.\nЕсли $c\u003e\\log _{b} a$, то $q \u003c 1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot \\frac{1-q^{k+1}}{1-q}\\right)=\\Theta\\left(n^{c} \\cdot \\frac{1}{1-q}\\right)=\\Theta\\left(n^{c}\\right) $$Если $c=\\log _{b} a$, то $q=1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot(k+1)\\right)=\\Theta\\left(n^{c} \\log _{b} n\\right)=\\Theta\\left(n^{c} \\log n\\right) $$Наконец, если $c \u003c \\log _{b} a$, то $q\u003e1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot\\left(q^{k}+\\frac{q^{k}-1}{q-1}\\right)\\right)=\\Theta\\left(n^{c} q^{k}\\right)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(a^{\\log _{b} n}\\right)=\\Theta\\left(n^{\\log _{b} a}\\right) $$ Introduction to Algorithms (Fourth Edition) ↩︎\nВ алгоритме Карацубы $a=3, b=2, c=1$, это соответствует третьему случаю теоремы, который и даёт $T(n)=\\Theta\\left(n^{\\log _{2} 3}\\right)$.\nОцените следующие алгоритмы Пример 1 1 2 3 4 5 6 7 8 9 10 11 12 void foo(int* array, int n) { int sum = 0; int product = 1; for (int i = 0; i \u003c n; i++) { sum += array[i]; } for (int i = 0; i \u003c n; i++) { product += array[i]; } std::cout \u003c\u003c sum \u003c\u003c \", \" \u003c\u003c product; } Пример 2 1 2 3 4 5 6 7 void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } Пример 3 1 2 3 4 5 6 7 void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = i + 1; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } Пример 4 1 2 3 4 5 6 7 8 9 void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { if (arrA[i] \u003c arrB[j]) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } } Пример 5 1 2 3 4 5 6 7 8 9 void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { for (int j = 0; j \u003c 100000; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } } Пример 6 1 2 3 4 5 6 7 8 void reverse(int* arr, int n) { for (int i = 0; i \u003c n / 2; i++) { int other = n - i - 1; int temp = arr[i]; arr[i] = arr[other]; arr[other] = temp } } Пример 7 1 2 3 4 5 6 7 8 9 bool isPrime(int n) { for (int x = 2; x * x \u003c= n; x++) { if (n % x == 0) { return false; } } return true; } Пример 8 1 2 3 4 5 6 7 8 9 int factorial(int n) { if (n \u003c 0) { return -1; } else if (n == 0) { return 1; } else { return n * factorial(n - 1); } } Пример 9 1 2 3 4 5 int fib(int n) { if (n \u003c= 0) return 0; else if (n == 1) return 1; else fib(n - 1) + fib(n - 2); }",
    "description": "Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nОсновная теорема о рекуррентных соотношениях Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда",
    "tags": [],
    "title": "3. Рекурентные соотношения",
    "uri": "/basics/recurrence_relation/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.\nПоиск элемента в массиве по значению также имеет сложность $\\Theta(n)$, так как нужно проверить все элементы массива. Если массив специально упорядочен (например, элементы массива расположены в возрастающем порядке), то поиск можно делать быстрее (скоро мы изучим алгоритм двоичного поиска).\nНемного о работе с массивом в C++:\n1 2 3 4 5 6 7 int a[n]; // объявить массив длины n, нумерация ячеек от 0 до n-1 a[i]; // обращение к i-му элементу массива a[i] = x; // присвоить x в i-ю ячейку массива int b[n][m]; // объявить двумерный массив, нумерация ячеек двумя индексами, // от 0 до n-1 и от 0 до m-1 b[i][j]; // обращение к j-му элементу i-й строки массива b[i][j] = y; // присвоение Связный список Связный список (linked list) состоит из узлов, каждый из которых содержит данные и ссылки на соседние узлы. В двусвязном списке поддерживаются ссылки на следующий и предыдущий узел, в односвязном списке - только на следующий. Также поддерживаются ссылки на начало и конец списка (их часто называют головой и хвостом). Для того, чтобы посетить все узлы, можно начать с головы и переходить по ссылке на следующий узел, пока он существует.\nПреимущество списка перед массивом - возможность вставлять и удалять элементы за $O(1)$.\nНедостаток списка - невозможность быстрого доступа к произвольному элементу. Так, доступ к $i$-му элементу можно получить, лишь $i$ раз пройдя по ссылке вперёд, начиная из головы списка (то есть за $\\Theta(i)$ ).\nУдобно делать голову и хвост фиктивными элементами и не хранить в них никаких данных, тогда функции вставки и удаления элементов пишутся проще.\nПримерная реализация двусвязного списка на C++:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 struct Node { // один узел списка Node *next, *prev; // указатели на следующий и предыдущий узлы int x; // данные, хранящиеся в узле }; struct List { Node *head, *tail; // указатели на начало и конец списка List() { // инициализируем пустой список - создаём фиктивные head и tail // и связываем их друг с другом head = new Node(); tail = new Node(); head-\u003enext = tail; tail-\u003eprev = head; } void pushBack(int x) { // вставить x в конец списка Node *v = new Node(); v-\u003ex = x, v-\u003eprev = tail-\u003eprev, v-\u003enext = tail; v-\u003eprev-\u003enext = v, tail-\u003eprev = v; } void insert(Node *v, int x) { // вставить x после v Nove *w = new Node(); w-\u003ex = x, w-\u003enext = v-\u003enext, w-\u003eprev = v; v-\u003enext = w, w-\u003enext-\u003eprev = w; } void erase(Node *v) { // удалить узел v v-\u003eprev-\u003enext = v-\u003enext; v-\u003enext-\u003eprev = v-\u003eprev; delete v; } Node *find(int x) { // найти x в списке for (Node *v = head-\u003enext; v != tail; v = v-\u003enext) { if (v-\u003ex == x) { return v; } } return nullptr; } }; Можно хранить узлы списка в массиве, тогда вместо указателей можно использовать числа - номера ячеек массива. Но тогда нужно либо заранее знать количество элементов в списке, либо использовать динамический массив (который мы скоро изучим).\nМожно также использовать встроенный в C++ двусвязный список - std::list. Односвязный список использует меньше дополнительной памяти, но не позволяет перемещаться по списку в сторону начала. Также из него сложнее удалять элементы - не получится удалить элемент, имея ссылку только на него. Нужно как-то получить доступ к предыдущему элементу, чтобы пересчитать ссылку из него на следующий.\nДинамический массив Пусть мы хотим научиться вставлять новые элементы в конец массива. Можно попытаться сразу создать массив достаточно большого размера, и в отдельной переменной поддер-\nживать его реальную длину. Но далеко не всегда максимальное количество элементов известно заранее.\nПоступим следующим образом: если мы хотим вставить новый элемент, а место в массиве закончилось, то создадим новый массив вдвое большего размера, и скопируем данные туда.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int *a; // используемая память int size; // размер памяти int n; // реальный размер массива void pushBack(int x) { // вставить х в конец массива if (n == size) { int *b = new int[size * 2]; for (int i = 0; i \u003c size; ++i) { b[i] = a[i]; } delete[] a; a = b; size *= 2; } a[n] = x; n += 1; } void popBack() { // удалить последний элемент n -= 1; } В C++ можно и нужно использовать встроенную реализацию динамического массива std::vector.\nКаждая конкретная операция вставки элемента может работать долго $-\\Theta(n)$, где $n$ - длина массива. Зато можно показать, что среднее время работы операции вставки $O(1)$. Докажем этот факт при помощи метода амортизационного анализа, который нам ещё не раз пригодится.\nСвойство 4.3.1 Среднее время работы операции вставки - $O(1)$.\nЭто утверждение равносильно тому, что суммарное время работы $m$ операций вставки - $O(m)$.\nДоказательство Операция вставки работает не за $O(1)$ только тогда, когда происходит удвоение размера используемой памяти. Заметим, что если такая операция увеличила размер с size до $2 \\cdot$ size, то хотя бы size $/ 2$ предыдущих операций вставки работали за $O(1)$ (может быть, и больше, если происходили также удаления элементов). Тогда суммарное время работы этих size $/ 2+1$ операций есть $O$ (size), поэтому среднее время их работы есть $O(1)$.\nСтек, очередь, дек Стек (stack) позволяет поддерживать список элементов, вставлять элемент в конец списка, а также удалять элемент из конца списка. Часто также говорят, что он организован по принципу LIFO (last in - first out).\nОчередь (queue) организована по принципу FIFO (first in - first out). Она позволяет вставлять элемент в конец списка, а также удалять элемент из начала списка.\nДек (deque - double ended queue), или двусторонняя очередь, отличается от обычной очереди тем, что позволяет добавлять и удалять элементы как в начало, так и в конец списка.\nВсе эти структуры можно реализовать с помощью связного списка. Для стека и очереди хватит односвязного списка: в стеке достаточно поддерживать ссылку на предыдущий элемент, а в очереди - на следующий. Для реализации дека понадобится двусвязный список.\nТакже все три структуры можно реализовать с помощью динамического массива. В случае дека нужно научиться добавлять элементы в начало, для чего можно зациклить массив. То же самое можно сделать в реализации очереди, чтобы переиспользовать освободившееся место в начале массива. Приведём примерную реализацию дека на динамическом массиве:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 int *a; // используемая память int size; // размер памяти int b, e; // дек использует ячейки в диапазоне [b,e), или, // если b \u003e= e, в диапазонах [b,size) и [0,e) int getSize() { // узнать количество элементов в деке if (b \u003c e) { return e - b; } else { return e - b + size; } } void pushBack(int x) { // вставить x в конец дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[e] = x; e = (e + 1) % size; } void popBack() { // удалить элемент из конца дека e = (e - 1 + size) % size; } void pushFront(int x) { // вставить x в начало дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[b] = x; b = (b - 1 + size) % size; } void popFront() { // удалить элемент из начала дека b = (b + 1) % size; } В C++ существуют встроенные реализации этих структур - std::stack, std::queue, std::deque.",
    "description": "Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.",
    "tags": [],
    "title": "4. Базовые структуры данных",
    "uri": "/basics/basic_data_structures/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.\nТеперь можно рекурсивно повторить те же рассуждения - посмотреть на середину нового подоотрезка массива, и либо в середине найдётся $x$, либо мы поймём, в какой половине нового подотрезка нужно его искать. Будем повторять эти действия, пока не найдём $x$, либо не дойдём до подотрезка нулевой длины. Поскольку на каждом шаге длина отрезка уменьшается вдвое, это произойдёт через $O(\\log n)$ шагов.\nТакже можно оценить время работы нашего алгоритма с помощью основной теоремы о рекуррентных соотношениях: время работы алгоритма в худшем случае (когда $x$ так и не нашёлся, либо нашёлся в самом конце) удовлетворяет рекуррентному соотношению $T(n)=T\\left(\\frac{n}{2}\\right)+\\Theta(1)$. Это соответствует $a=1, b=2, c=0$ в формулировке теоремы. $c=0=\\log _{2} 1=\\log _{b} a$, тогда по теореме время работы алгоритма в худшем случае есть $\\Theta(\\log n)$.\nПримерная реализация алгоритма двоичного поиска:\n1 2 3 4 5 6 7 8 9 10 11 12 binarySearch(a, n, x): # ищет x в массиве а длины n, возвращает индекс, # по которому лежит x, или -1, если х в массиве нет l = 0, r = n - 1 # границы интересующего нас подотрезка массива while l \u003c= r: # пока подотрезок непуст m = (l + r) / 2 if a[m] == x: return m if a[m] \u003c x: l = m + 1 else: r = m - 1 return -1 # x так и не нашёлся Левое вхождение Возможно, $x$ встречается в массиве несколько раз (так как массив отсортирован, эти вхождения образуют подотрезок массива). Научимся находить количество вхождений $x$ в массив (то есть длину этого подотрезка).\nДостаточно найти индексы самого левого и самого правого вхождений $x$ в массив, тогда количество вхождений - это разность этих индексов плюс один. Начнём с левого вхождения.\nБудем снова поддерживать границы $l$ и $r$, но уже со следующим условием: пусть в любой момент времени выполняется $a[l]\u003c x$ и $a[r] \\geqslant x$. Удобно мысленно добавить к массиву фиктивные элементы $a[-1]=-\\infty$ и $a[n]=\\infty$, тогда не нужно отдельно обрабатывать случаи, когда все элементы меньше $x$, или все элементы больше или равны $x$.\nСам алгоритм становится только проще: смотрим на середину отрезка $(l, r)$, и, в зависимости от значения элемента массива с этим индексом, сдвигаем $l$ или $r$ так, чтобы требуемые от них условия не нарушились. Когда $l$ и $r$ указывают на соседние элементы (то есть $l=r-1$ ), всё ещё верно, что $a[l] \u003c x, a[r] \\geqslant x$. Тогда если $a[r]=x$, то $r$ индекс левого вхождения $x$, иначе $x$ в массиве не встречается.\nПримерная реализация:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 lowerBound(a, n, x): # возвращает минимальное i такое, что a[i] \u003e= x l = -1, r = n # l и r изначально указывают на фиктивные элементы while r - l \u003e 1: m = (l + r) / 2 if a[m] \u003c x: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r getLeftEntry(a, n, x): # возвращает номер левого вхождения или -1 i = lowerBound(a, n, x) if i == n or a[i] != x: return -1 return i Правое вхождение ищется примерно так же: нужно требовать от границ поиска $a[l] \\leqslant x$ и $a[r]\u003ex$, тогда в конце $l$ - правое вхождение, если $a[l]=x$, иначе $x$ в массиве не встречается. Заметим, что в реализации самого поиска нужно поправить всего одну строчку: a [m] \u003c x заменить на $\\mathrm{a}[\\mathrm{m}]$ \u003c= x .\nВ C++ есть встроенные функции std::lower_bound и std::upper_bound. Первая возвращает итератор, указывающий на первый элемент, больше или равный $x$, вторая на первый элемент, строго больший $x$. Пример использования:\n1 2 3 4 5 6 7 8 i = lower_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e= x; n, если все элементы а меньше х i = upper_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e x; n, если все элементы а меньше или равны х i = upper_bound(a, a + n, x) - a - 1; // максимальное i такое, что a[i] \u003c= x; -1, если все элементы а больше х cnt = upper_bound(a, a + n, x) - lower_bound(a, a + n, x); // количество вхождений х в а Двоичный поиск по функции Можно рассмотреть и ещё более общую версию алгоритма. Пусть есть некоторая функция $f(i)$ такая, что $f(i)=0$ для всех $i$, меньших некоторого порога $t$, и $f(i)=1$ для всех $i \\geqslant t$. Тогда этот порог можно найти с помощью алгоритма двоичного поиска.\nИдея та же, что и при поиске левого вхождения: сначала возьмём такие границы поиска $l=L, r=R$, что $f(L)=0, f(R)=1$. После этого на каждом шаге будем сдвигать одну из границ $l, r$, в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. В тот момент, когда $l$ и $r$ отличаются на единицу, $r$ - искомый порог. Если считать, что значение функции в точке вычисляется за $O(1)$, то время работы алгоритма $-O(\\log (R-L))$, где $L, R-$ начальные границы поиска.\nЗаметим, что алгоритмы поиска левого и правого вхождения являются частными случаями этого алгоритма: в случае левого вхождения можно взять функцию $f$ такую,\nчто $f(i)=1$ тогда и только тогда, когда $a[i] \\geqslant x$; в случае правого вхождения $-f(i)=1$ тогда и только тогда, когда $a[i]\u003ex$ (и нас интересует максимальное $i$ такое, что $f(i)=0$, то есть в конце алгоритма нужно вернуть $l$, а не $r$ ).\nДвоичный поиск по функции с вещественным аргументом Двоичный поиск можно делать и по функции, принимающей значения во всех вещественных точках, а не только в целых. Решим для примера такую задачу: дана монотонно возрастающая непрерывная функция $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, и точки $L, R$ такие, что $f(L)\u003c0$, $f(R) \\geqslant 0$. Найдём точку $x$ такую, что $f(x)=0$. (R)\nЕсли такие $L, R$ не даны, но известно, что они существуют, то их можно найти алгоритмом экспоненциального поиска: начнём с $R=1$ и будем удваивать $R$, пока $f(R)\u003c0$. Аналогично, начнём с $L=-1$ и будем удваивать $L$, пока $f(L) \\geqslant 0$. При этом мы затратим $O\\left(\\log \\left|R^{\\prime}\\right|+\\log \\left|L^{\\prime}\\right|\\right)$ действий, где $R^{\\prime} \\geqslant 0, L^{\\prime} \\leqslant 0-$ любые такие, что $f\\left(L^{\\prime}\\right)\u003c0, f\\left(R^{\\prime}\\right) \\geqslant 0$.\nПоскольку $f$ принимает вещественные значения, мы не всегда можем найти значение такого $x$ абсолютно точно. Тем не менее, мы можем найти такое $x$ с погрешностью $\\varepsilon$ : найдём такое $x$, что существует $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0,\\left|x-x^{\\prime}\\right|\u003c\\varepsilon$.\nАлгоритм практически не меняется: начинаем с границ $l=L, r=R$, на каждом шаге сдвигаем одну из границ в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. Единственное отличие: мы завершаем алгоритм в тот момент, когда $r-l\u003c\\varepsilon$. Поскольку в этот момент всё ещё $f(l)\u003c0, f(r) \\geqslant 0$, а функция $f$ непрерывна, на отрезке $[l, r]$ найдётся $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0$. Поскольку длина отрезка меньше $\\varepsilon$, любая из границ $l, r$ подойдёт в качестве ответа. Время работы алгоритма (при условии, что значение $f$ вычисляется за $O(1)$ ) $O\\left(\\log \\left(\\frac{R-L}{\\varepsilon}\\right)\\right)$.\nЕсли вычисления проводятся при помощи стандартных вещественнозначных типов данных (например, double в C++), то из-за накопления погрешности при пересчёте границ условие $r-l\u003c\\varepsilon$ может никогда не выполниться. Чтобы избежать этого, вместо while напишем цикл, делающий столько итераций, сколько нужно, чтобы условие точно выполнилось. Пример реализации:\n1 2 3 4 5 6 7 8 9 10 findRoot(L, R, eps): l = L, r = R k = log((r - l) / eps) # через k итераций условие r - l \u003c eps выполнится for i = 0..(k - 1): m = (l + r) / 2 if f(m) \u003c 0: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r Метод двух указателей Иногда много двоичных поисков на одних и тех же данных можно соптимизировать с помощью так называемого метода двух указателей. Сделаем это на примере следующей задачи: дан массив $a$ длины $n$, состоящий из неотрицательных целых чисел. Нужно найти максимальный по длине подотрезок массива, сумма на котором не превосходит $M$.\nУдобнее работать не с отрезками, а с полуинтервалами: паре $l, r$ будем сопоставлять полуинтервал $a[l, r)$, то есть элементы массива с индексами $l, l+1, \\ldots, r-1$ (это практически всегда позволяет писать меньше плюс-минус единиц в индексах массивов, что делает код короче). Самое простое решение - перебрать все возможные полуинтервалы:\n1 2 3 4 5 6 7 maxLen = 0, ansL = 0, ansR = 0 # здесь будем хранить ответ for l = 0..(n - 1): # перебираем левую границу sum = 0 # будем считать сумму на текущем полуинтервале for r = (l + 1)..n: sum += a[r - 1] if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r Получилось решение за $\\Theta\\left(n^{2}\\right)$. Заметим, что при фиксированной левой границе сумма на полуинтервале не уменьшается при увеличении правой границы. Значит сколькото первых правых границ подойдут, а все последующие уже не подойдут. При этом нас интересует максимальная подходящая правая граница. Её можно найти двоичным поиском, таким образом соптимизировав решение до $\\Theta(n \\log n)$ :\nДля того, чтобы быстро находить сумму на произвольном полуинтервале, заранее предподсчитаем префиксные суммы - массив $p$ такой, что $p[i]=\\sum_{j=0}^{i-1} a_{j}$. Тогда сумма на полуинтервале $a[l, r)$ равна $\\sum_{i=l}^{r-1} a_{i}=p[r]-p[l]$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 maxLen = 0, ansL=0, ansR = 0 int p[n + 1] # предподсчитываем префиксные суммы p[0] = 0 for i = 1..n: p[i] = p[i - 1] + a[i - 1] for l = 0..(n - 1): sl = l, sr = n + 1 # границы поиска, sum a[l..sl) \u003c= M, sum a[l..sr) \u003e M while sr - sl \u003e 1: sm = (sl + sr) / 2 if p[sm] - p[l] \u003c= M: sl = sm else: sr = sm if sl - l \u003e maxLen: # sl - максимальная подходящая правая граница maxLen = sl - l, ansL = l, ansR = sl Перейдём теперь собственно к методу двух указателей. Пусть $r$ - максимальная подходящая правая граница для левой границы $l-1$. Тогда $M \\geqslant \\sum_{i=l-1}^{r-1} a_{i} \\geqslant \\sum_{i=l}^{r-1} a_{i}$, то есть граница $r$ подойдёт и для $l$. Будем вместо двоичного поиска искать границу $r$ так же, как и в самой первой версии алгоритма: просто перебирая все границы подряд. Но начнём перебор не с $l+1$, а с правой границы, найденной на предыдущем шаге. Поскольку теперь правая граница только увеличивается в ходе алгоритма, суммарно за всё время выполнения алгоритма она сдвинется не более, чем $n$ раз. Тогда общее время работы алгоритма $-\\Theta(n)$.\n1 2 3 4 5 6 7 8 9 maxLen = 0, ansL=0, ansR = 0 r = 0, sum = 0 for l = 0..(n - 1): # перебираем левую границу while r \u003c n and sum + a[r] \u003c= M: sum += a[r] r += 1 if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r sum -= a[l]",
    "description": "Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.",
    "tags": [],
    "title": "5. Двоичный поиск",
    "uri": "/basics/binary_search/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Квадратичные сортировки Существует множество различных алгоритмов, сортирующих массив длины $n$ за $\\Theta\\left(n^{2}\\right)$. Мы поговорим лишь о двух из них.\nСортировка выбором Сортировка выбором (selection sort) на $i$-м шаге находит $i$-й по возрастанию элемент и ставит его на $i$-ю позицию. Поскольку первые $i-1$ элементов в этот момент уже стоят на своих позициях, достаточно просто найти минимальный элемент в подотрезке $[i, n)$.\n1 2 3 4 5 6 for i = 0..(n - 1): minPos = i for j = (i + 1)..(n - 1): if a[minPos] \u003e a[j]: minPos = j swap(a[i], a[minPos]) Сортировка вставками На $i$-м шаге сортировки вставками (insertion sort) первые $i$ элементов массива (образующие префикс длины $i$ ) расположены в отсортированном порядке. $i$-й шаг состоит в том, что $i$-й элемент массива вставляется в нужную позицию остортированного префикса.\n1 2 3 for i = 1..(n - 1): for (j = i; j \u003e 0 and a[j] \u003c a[j - 1]; --j): swap(a[j], a[j - 1]) Время работы В обеих сортировках два вложенных цикла в худшем случае дают время работы $\\Theta\\left(n^{2}\\right)$. Сортировка выбором полезна тем, что делает $\\Theta(n)$ операций swap. Это свойство пригождается, когда сортируются тяжёлые объекты, и каждая операция swap занимает много времени.\nИнверсией называют такую пару элементов на позициях $i \u003c j$, что $a_{i} \u003e a_{j}$. Последовательность элементов является отсортированной тогда и только тогда, когда в ней нет инверсий. Время работы сортировки вставками можно оценить как $\\Theta(n+\\operatorname{Inv}(a))$, где $\\operatorname{Inv}(a)$ - количество инверсий в массиве $a$, так как на каждом шаге внутреннего цикла количество инверсий в массиве уменьшается ровно на один. Значит, на отсортированном (или почти отсортированном) массиве время работы сортировки вставками составит $O(n)$.\nСтабильность Сортировка называется стабильной, если она оставляет равные элементы в исходном порядке. Обычно такое свойство сортировки нужно, когда помимо данных, по которым производится сортировка, в элементах массива хранятся какие-то дополнительные данные. Например, если дан список участников соревнований в алфавитном порядке, и хочется отсортировать их по убыванию набранных баллов так, чтобы участники с равным числом баллов всё ещё шли в алфавитном порядке.\nСортировка выбором стабильной не является (например, массив $[5,5,3,1]$ после первого шага изменится на $[1,5,3,5]$, при этом порядок пятёрок поменялся). Сортировка вставками стабильна, так как меняет местами только соседние элементы, образующие инверсию, поэтому ни в какой момент времени не поменяет порядок равных элементов.\nЛюбую сортировку можно сделать стабильной, если вместо исходных элементов сортировать пары (элемент, его номер в исходном массиве), и при сравнении пар сначала сравнивать элементы, а при равенстве номера.\nПрименение на практике Сейчас мы перейдём в алгоритмам сортировки, работающим за $\\Theta(n \\log n)$. Квадратичные сортировки, благодаря малой константе в оценке времени работы, работают быстрее более сложных алгоритмов на коротких массивах. На практике часто применяется гибридный подход: алгоритм сортировки, использующий метод “разделяй и властвуй”, работает, пока массив не поделится на части достаточно малого размера, после чего на них запускается алгоритм квадратичной сортировки.\nСортировка слиянием (Merge sort) Сортировка слиянием (Von Neumann, 1945) использует метод “разделяй и властвуй” следующим образом: массив делится на две части, каждая из них сортируется рекурсивно, после чего две отсортированных части сливаются при помощи метода двух указателей.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mergeSort(a, l, r): # coртирует a[l, r) if r - l \u003c= 1: return m = (l + r) / 2 mergeSort(a, l, m) mergeSort(a, m, r) merge(a, l, m, r) # merge использует вспомогательный массив buf достаточно большого размера merge(a, l, m, r): # сливает два отсортированных отрезка а[l, m) и а[m, r) for (i = l, j = m, k = 0; i \u003c m or j \u003c r; ): if i == m or (j \u003c r and a[i] \u003e a[j]): buf[k] = a[j] k += 1, j += 1 else: buf[k] = a[i] k += 1, i += 1 for i = 0..(r - l - 1): a[l + i] = buf[i] Время работы сортировки слиянием можно оценить с помощью рекуррентного соотношения $T(n)=2 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n)$. По основной теореме о рекуррентных соотношениях получаем $T(n)=\\Theta(n \\log n)$.\nСортировка слиянием стабильна (поскольку функция merge не меняет относительный порядок равных элементов).\nНедостатком сортировки слиянием является то, что она требует $\\Theta(n)$ дополнительной памяти (вспомогательный массив в merge).\nБыстрая сортировка (Quicksort) Быстрая сортировка (Hoare, 1959) также использует метод “разделяй и властвуй”, но немного по-другому. Возьмём какой-нибудь элемент массива - $x$. Поделим массив на три части так, что в первой все элементы меньше $x$, во второй равны $x$, в третьей больше $x$ (это можно сделать за линейное от длины массива время, например, с помощью трёх вспомогательных массивов). Остаётся рекурсивно отсортировать первую и третью части.\n1 2 3 4 5 6 quickSort(a): if len(a) \u003c= 1: return x = randomElement(a) # x - случайный элемент а a --\u003e l (\u003c x), m (= x), r (\u003e x) # делим а на три части return quickSort(l) + m + quickSort(r) На практике, чтобы алгоритм работал быстрее и использовал меньше дополнительной памяти, используют более хитрый способ деления массива на части:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # partition выбирает x - случайный элемент a[l, r], # переставляет местами элементы a[l, r] и возвращает m (l \u003c= m \u003c r) такое, что # все элементы a[l, m] меньше или равны x, все элементы a[m + 1, r] больше или равны x partition(a, l, r) -\u003e int: p = random(l, r), x = a[p] swap(a[p], a[l]) # теперь x стоит на l-й позиции i = l, j = r while i \u003c= j: while a[i] \u003c x: i += 1 while a[j] \u003e x: j -= 1 if i \u003e= j: break swap(a[i], a[j]) i += 1, j -= 1 return j quickSort(a, l, r): # coртирует a[l, r] if l == r: return m = partition(a, l, r) quickSort(a, l, m) quickSort(a, m + 1, r) Свойства Функция partition работает корректно, то есть $i$ и $j$ не выходят за границы $l, r$, функция возвращает такое $m$, что $l \\leqslant m \u003c r$, все элементы $a[l, m]$ меньше или равны $x$, все элементы $a[m+1, r]$ больше или равны $x$. Доказательство Заметим, что в любой момент времени все элементы $a[l, i)$ меньше или равны $x$, все элементы $a(j, r]$ больше или равны $x$.\nНа первой итерации внешнего цикла в момент проверки условия на $13$-ой строке верно, что $i=l \\leqslant j$. Значит либо мы сразу выйдем из внешнего цикла (если $i=j=l \u003c r$ ), либо выполнятся $15-16$ строки, после чего всегда будет верно, что $a[l] \\leqslant x, a[r] \\geqslant x, j \u003c r$. При этом 15-16 строки выполнились только при $i \u003c j$, тогда даже после их выполнения $i$, $j$ не вышли за границы $l$, $r$.\nОтдельно отметим, что после первой итерации внешнего цикла точно выполняется неравенство $j \u003c r$.\nНа всех последующих итерациях внешнего цикла после выполнения $9-12$ строк будут верны неравенства $j \\geqslant l, i \\leqslant r$ (так как $a[l] \\leqslant x, a[r] \\geqslant x$ ). При этом если условие на\n13-й строке не выполняется (то есть $i \u003c j$ ), то даже после выполнения $15-16$ строк $i, j$ не выйдут за границы $l, r$.\nНаконец, по итогам выполнения функции $l \\leqslant m=j \u003c r$, все элементы $a[m+1, r]=$ $a(j, r]$ больше или равны $x$, все элементы $a[l, m]=a[l, j]$ меньше или равны $x$ (так как это верно для элементов $a[l, i), j \\leqslant i$, причём $j=i$ только если мы вышли из внешнего цикла на $14$-ой строке, что возможно, только если $a[j]=x$ ).\nЗаметим, что такая реализация является нестабильной сортировкой.\nОценка времени работы В худшем случае массив каждый раз будет делиться очень неравномерно, и почти все элементы будут попадать в одну из частей. Время работы в худшем случае можно оценить с помощью рекуррентного соотношения $T(n)=T(n-1)+\\Theta(n)$, раскрыв которое, получаем $T(n)=\\sum_{i=1}^{n} \\Theta(i)=\\Theta\\left(n^{2}\\right)$.\nВ лучшем же случае массив каждый раз будет делиться на две примерно равные части. Получаем рекуррентное соотношение $T(n)=2 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n)$, тогда по основной теореме о рекуррентных соотношениях получаем $T(n)=\\Theta(n \\log n)$.\nОказывается, время работы алгоритма в среднем намного ближе к лучшему случаю, чем к худшему. Для того, чтобы это доказать, нам понадобится терминология из теории вероятностей. Определение Математическое ожидание случайной величины $X$, принимающей значение $x_{1}$ с вероятностью $p_{1}, x_{2}$ с вероятностью $p_{2}, \\ldots, x_{n}$ с вероятностью $p_{n}$ $\\left(p_{1}+\\cdots+p_{n}=1\\right)$ есть\n$$ \\mathbb{E} X=\\sum_{i=1}^{n} p_{i} x_{i} . $$ Теорема Математическое ожидание времени работы алгоритма быстрой сортировки есть $O(n \\log n)$. Доказательство Мы докажем теорему только в случае, когда все элементы массива попарно различны. Кроме того, будем рассматривать версию алгоритма, делящую массив на три части (элементы меньше $x$; элементы, равные $x$; элементы больше $x$ ).\nДля удобства будем использовать индексы элементов не в исходном, а в уже отсортированном массиве: пусть отсортированный массив имеет вид $z_{1}, z_{2}, \\ldots, z_{n}$, исходный массив $a$ - это какая-то перестановка элементов $z_{i}$.\nВремя работы алгоритма быстрой сортировки пропорционально количеству выполненных сравнений. Обозначим количество выполненных сравнений за $T(n)$, достаточно оценить его математическое ожидание.\n$z_{i}$ и $z_{j}$ могли сравниваться, только если на каком-то шаге алгоритма один из них был выбран в качестве $x$. Заметим, что такой элемент не участвует в последующих рекурсивных вызовах, поэтому каждую пару элементов алгоритм сравнит не более, чем один раз.\nПусть $\\Gamma$ - множество всех возможных сценариев выполнения алгоритма, $p(A)$ вероятность того, что произошёл сценарий $A \\in \\Gamma$. Для каждой пары $i, j$ введём величину $\\chi(A, i, j)$, равную единице, если $z_{i}$ и $z_{j}$ сравнивались в сценарии $A$, и нулю, если не сравнивались. Математическое ожидание количества выполненных алгоритмом сравнений равняется\n$$ \\mathbb{E} T(n)=\\sum_{A \\in \\Gamma}\\left(p(A) \\sum_{1 \\leqslant i \u003c j \\leqslant n} \\chi(A, i, j)\\right)=\\sum_{1 \\leqslant i \u003c j \\leqslant n} \\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j) . $$Остаётся для каждой пары $1 \\leqslant i \u003c j \\leqslant n$ посчитать $\\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j)$, то есть вероятность, с которой $z_{i}$ и $z_{j}$ сравнивались между собой.\nПосмотрим на момент, в который $z_{i}$ и $z_{j}$ при делении массива попали в разные части. Заметим, что массив, который делился на части в этот момент, после сортировки будет являться подотрезком отсортированного массива $z$, тогда вместе с $z_{i}$ и $z_{j}$ он содержит весь подотрезок $z[i, j]$.\nПоскольку $z_{i}$ и $z_{j}$ при делении попали в разные части, в качестве $x$ точно был выбран один из элементов $z[i, j]$. При этом $z_{i}$ и $z_{j}$ сравнивались между собой только если в качестве $x$ был выбран один из них. Поскольку $x$ выбирается среди всех элементов равновероятно, вероятность того, что между $z_{i}$ и $z_{j}$ произошло сравнение, равняется $\\frac{2}{j-i+1}$. Получаем\n$$ \\begin{aligned} \\mathbb{E} T(n)= \u0026 \\sum_{1 \\leqslant i \u003c j \\leqslant n} \\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j)=\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac{2}{j-i+1}\\xlongequal{\\left(k=j-i\\right)} \\\\ \u0026 =\\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k+1} \u003c 2 n \\sum_{k=1}^{n} \\frac{1}{k}=O(n \\log n) \\end{aligned} $$Последний переход можно понять, например, следующим способом:\n$$ \\begin{gathered} \\sum_{k=1}^{n} \\frac{1}{k}=\\frac{1}{1}+\\left(\\frac{1}{2}+\\frac{1}{3}\\right)+\\left(\\frac{1}{4}+\\frac{1}{5}+\\frac{1}{6}+\\frac{1}{7}\\right)+\\cdots\u003c \\\\ \u003c \\\\ \\frac{1}{1}+\\left(\\frac{1}{2}+\\frac{1}{2}\\right)+\\left(\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}\\right)+\\cdots \\leqslant\\left\\lfloor\\log _{2} n\\right\\rfloor+1 . \\end{gathered} $$ Отсюда следует и более сильное утверждение: время работы алгоритма есть $O(n \\log n)$ с вероятностью, близкой в единице. Это следует из следующего утверждения, известного как неравенство Маркова: для неотрицательной случайной величины $X$ с математическим ожиданием $\\mathbb{E}(X)$ вероятность того, что $X\u003ek \\cdot \\mathbb{E}(X)$, не превосходит $\\frac{1}{k}$. Это верно, так как в противном случае математическое ожидание $X$ оказалось бы больше $\\frac{1}{k} \\cdot k \\cdot \\mathbb{E}(X)=\\mathbb{E}(X)$. В нашем случае, например, для $k=100$ получаем, что с вероятностью $99 \\%$ время работы не превосходит $O(100 \\cdot n \\log n)=O(n \\log n)$.\nВзятие случайного элемента - достаточно медленная операция, поэтому на практике вместо случайного элемента часто берут какой-то конкретный, например самый левый, самый правый, или средний; также часто используют средний по значению из этих трёх. Для подобных версий алгоритма можно построить массив, на котором сортировка будет работать за $\\Theta\\left(n^{2}\\right)$. С этим борются разными способами, например, когда глубина рекурсии превышает $\\log n$, переключаются на какой-нибудь другой алгоритм сортировки.\nНа практике алгоритм быстрой сортировки оказывается одним из самых быстрых и часто используемых. Встроенная в C++ сортировка - std::sort, использует алгоритм Introsort, который начинает сортировать массив алгоритмом быстрой сортировки, на большой глубине рекурсии переключается на heapsort (который мы скоро изучим), а массивы совсем небольшой длины сортирует сортировкой вставками.\nПоиск $k$-й порядковой статистики $k$-я порядковая статистика на массиве из $n$ элементов - это $k$-й по возрастанию элемент. Например, при $k=1$ это минимум, при $k=n$ - максимум. Медиана - это элемент, который оказался бы в середине массива, если бы его отсортировали. Если длина массива чётна, то в нём есть две медианы на позициях $\\left\\lfloor\\frac{n+1}{2}\\right\\rfloor$ и $\\left\\lceil\\frac{n+1}{2}\\right\\rceil$. Для определённости под медианой будем иметь в виду $\\left\\lfloor\\frac{n+1}{2}\\right\\rfloor$-ю порядковую статистику.\nМинимум и максимум в массиве очень легко ищется за $O(n)$ одним проходом по всем элементам массива. $k$-й по возрастанию элемент так просто уже не найти.\nМожно отсортировать массив за $O(n \\log n)$, тогда $k$-я порядковая статистика окажется на $k$-й позиции (если нумеровать элементы массива, начиная с единицы). Однако существуют и более быстрые алгоритмы, находящие $k$-ю порядковую статистику для произвольного $k$ за $O(n)$.\nВернёмся к алгоритму быстрой сортировки. Когда мы поделили массив на две части, можно понять, в какой из этих частей находится $k$-й по возрастанию элемент: если размер левой части хотя бы $k$, то он находится в ней, иначе он находится в правой части. Тогда, если нас интересует не весь отсортированный массив, а только $k$-й по возрастанию элемент, можно сделать рекурсивный запуск только от той части, в которой он лежит.\n1 2 3 4 5 6 7 8 kthElement(a, l, r, k): # находит k-ю порядковую статистику в a[l, r] if l == r: return a[l] m = partition(a, l, r) if m - l + 1 \u003e= k: return kthElement(a, l, m, k) else: return kthElement(a, m + 1, r, k - (m - l + 1)) В худшем случае такой алгоритм будет работать по-прежнему за $\\Theta\\left(n^{2}\\right)$. Однако оказывается, что оценка среднего времени работы после такой оптимизации улучшается с $O(n \\log n)$ до $O(n)$.\nТеорема Математическое ожидание времени работы алгоритма kthElement есть $O(n)$. Доказательство Мы докажем теорему в случае, когда все элементы массива попарно различны.\nБудем говорить, что алгоритм находится в $j$-й фазе, если размер текущего отрезка массива не больше $\\left(\\frac{3}{4}\\right)^{j} n$, но строго больше $\\left(\\frac{3}{4}\\right)^{j+1} n$. Оценим время работы алгоритма в каждой фазе отдельно.\nНазовём элемент центральным, если хотя бы четверть элементов в текущем отрезке массива меньше его, и хотя бы четверть больше. Если в качестве разделителя $x$ был выбран центральный элемент, то размер отрезка, от которого будет сделан рекурсивный запуск, будет не больше $\\frac{3}{4}$ от размера текущего отрезка, то есть текущая фаза алгоритма точно закончится. При этом вероятность выбрать центральный элемент равна $\\frac{1}{2}$ (так как ровно половина элементов отрезка являются центральными). Тогда математическое ожидание количества рекурсивных запусков, сделанных в течение $j$-й фазы, не превосходит\n$$ 1+\\frac{1}{2}\\left(1+\\frac{1}{2}(1+\\cdots)\\right)=1+\\frac{1}{2}+\\frac{1}{4}+\\cdots=2 . $$При этом каждая итерация алгоритма на $j$-й фазе совершает $O\\left(\\left(\\frac{3}{4}\\right)^{j} n\\right)$ действий. Математическое ожидание времени работы алгоритма равняется сумме математических ожиданий времён работы каждой фазы, которая не превосходит\n$$ \\sum_{j} O\\left(\\left(\\frac{3}{4}\\right)^{j} n\\right) \\cdot 2=O\\left(n \\cdot \\sum_{j}\\left(\\frac{3}{4}\\right)^{j}\\right)=O\\left(n \\cdot \\frac{1}{1-3 / 4}\\right)=O(n) $$ В C++ есть встроенная реализация этого алгоритма - std::nth_element.\nАлгоритм можно модифицировать так, чтобы он работал за $O(n)$ в худшем случае. Это делается так: поделим массив на $n / 5$ групп по 5 элементов, в каждой за $O(1)$ найдём медиану. Теперь рекурсивным запуском алгоритма найдём медиану среди этих медиан, и уже её будем использовать в качестве разделителя. Тогда в половине групп хотя бы 3 элемента окажутся меньше разделителя, а в другой половине хотя бы 3 элемента окажутся больше разделителя. Значит каждая из частей, на которые поделился массив, будет иметь размер хотя бы $3 n / 10$. Получаем в худшем случае рекуррентное соотношение $T(n)=\\Theta(n)+T(n / 5)+T(7 n / 10)$. Можно показать, что в этом случае верно $T(n)=\\Theta(n)$.\nОценка снизу на время работы сортировки сравнениями Алгоритм сортировки сравнениями может копировать сортируемые объекты и сравнивать их друг с другом, но никак не использует внутреннюю структуру объектов. Все сортировки, изученные нами до этого момента, являются сортировками сравнения. Можно показать, что никакая сортировка сравнениями не может в общем случае работать быстрее, чем за $\\Theta(n \\log n)$.\nТеорема Любой алгоритм сортировки сравнениями имеет время работы $\\Omega(n \\log n)$ в худшем случае. Доказательство Алгоритм сортировки сравнениями должен уметь корректно сортировать любую перестановку чисел от 1 до $n$. Пусть алгоритм на каждой перестановке делает не более $k$ сравнений. Заметим, что если зафиксировать результаты всех сравнений в ходе работы алгоритма, то он будет выдавать всегда одну и ту же перестановку данного на вход массива.\nЭто не совсем верно для алгоритмов, использующих случайные числа (например, для алгоритма быстрой сортировки), поскольку сама последовательность сравнений может зависеть от того, какие случайные числа выпали. Однако это верно, если зафиксировать последовательность случайных чисел, которую получает алгоритм. Поскольку алгоритм должен корректно работать на любой данной ему последовательности случайных чисел, дальнейшие рассуждения остаются верны.\nПоскольку алгоритм делает не более $k$ сравнений, и равенств не бывает (поскольку мы сортируем перестановки), существует не более $2^{k}$ различных перестановок данного на вход массива, которые он может выдать. Поскольку алгоритм корректно сортирует произвольную перестановку, получаем $2^{k} \\geqslant n!$. Тогда\n$$ k \\geqslant \\log (n!) \\geqslant \\log \\left(\\left(\\frac{n}{2}\\right)^{n / 2}\\right)=\\frac{n}{2} \\log \\frac{n}{2}=\\Omega(n \\log n) $$ Тем не менее, если обладать какой-то дополнительной информацией о свойствах сортируемых объектов, иногда можно воспользоваться этими свойствами, чтобы отсортировать объекты быстрее, чем за $\\Theta(n \\log n)$.\nСортировка подсчётом Если известно, что все числа во входном массиве целые, неотрицательные и меньше некоторого $k$, то их можно отсортировать за $\\Theta(n+k)$ (при этом понадобится $\\Theta(k)$ вспомогательной памяти). Для этого посчитаем, сколько раз встретилось каждое число от 1 до $k$, после чего просто выпишем каждое число в ответ столько раз, сколько он встречалось в исходном массиве.\n1 2 3 4 5 6 7 8 9 10 11 int c[k] countingSort(a, n): for i = 0..(k - 1): c[i] = 0 for i = 0..(n - 1): c[a[i]] += 1 p = 0 for i = 0..(k - 1): for j = 0..(c[i] - 1): a[p] = i p += 1 Ясно, что таким же образом можно сортировать целые числа, лежащие в диапазоне $[L, R)$, за $\\Theta(n+(R-L))$.\nЕсли воспользоваться ещё одним вспомогательным массивом, сортировку можно сделать стабильной:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int c[k] countingSort(a, n): for i = 0..(k - 1): c[i] = 0 for i = 0..(n - 1): c[a[i]] += 1 for i = 1..(k - 1): c[i] += c[i - 1] # теперь с[i] - количество элементов массива, не превосходящих i int b[n] for i = (n - 1)..0: c[a[i]] -= 1 b[c[a[i]]] = a[i] for i = 0..(n - 1): a[i] = b[i] Стабильная сортировка подсчётом пригодится нам в следующем алгоритме сортировки.\nПоразрядная сортировка (Radix sort) Пусть дан массив из $n$ чисел, записанных в $k$-ичной системе счисления и имеющих не более $d$ разрядов каждое. Отсортируем числа сортировкой подсчётом $d$ раз - сначала по младшему разряду, потом по следующему, и так далее, в конце - по старшему разряду. При этом будем пользоваться стабильной версией сортировки подсчётом.\nПосле первого шага числа будут отсортированы по 0-му разряду, после второго по 1-му разряду, а при равенстве цифр в 1-м разряде - по 0-му. В конце числа будут отсортированы по $(d-1)$-му разряду, при равенстве цифр в $(d-1)$-м разряде - по ( $d-2)$-му,…, при равенстве цифр во всех разрядах, кроме 0-го - по 0-му. Значит числа просто окажутся отсортированы в порядке возрастания.\n1 2 3 radixSort(a, n, d): for i = 0..(d - 1): countingSort(a, n, i) # стабильная сортировка подсчётом по i-му разряду Каждый шаг алгоритма работает за $\\Theta(n+k)$, тогда время работы всего алгоритма $\\Theta(d(n+k))$. Поразрядная сортировка является стабильной.\nС помощью поразрядной сортировки можно сортировать любые объекты, которые можно лексикографически упорядочить. Так, можно лексикографически отсортировать $n$ строк длины $d$ каждая, в записи которых используется $k$ различных символов, за $\\Theta(d(n+k))$.\nПусть нам даны $n$ неотрицательных целых чисел, меньших $m$. Если мы переведём их в $k$-ичную систему счисления, то сможем отсортировать их поразрядной сортировкой за $\\Theta\\left(\\left(1+\\log _{k} m\\right)(n+k)\\right)$ (используя $\\Theta\\left(n\\left(1+\\log _{k} m\\right)+k\\right)$ дополнительной памяти). При $n=k$ получаем время работы $\\left.\\Theta\\left(n+n \\log _{n} m\\right)\\right)=\\Theta\\left(n+n \\frac{\\log m}{\\log n}\\right)$.",
    "description": "Квадратичные сортировки Существует множество различных алгоритмов, сортирующих массив длины $n$ за $\\Theta\\left(n^{2}\\right)$. Мы поговорим лишь о двух из них.\nСортировка выбором Сортировка выбором (selection sort) на $i$-м шаге находит $i$-й по возрастанию элемент и ставит его на $i$-ю позицию. Поскольку первые $i-1$ элементов в этот момент уже стоят на своих позициях, достаточно просто найти минимальный элемент в подотрезке $[i, n)$.\n1 2 3 4 5 6 for i = 0..(n - 1): minPos = i for j = (i + 1)..(n - 1): if a[minPos] \u003e a[j]: minPos = j swap(a[i], a[minPos])",
    "tags": [],
    "title": "6. Сортировки",
    "uri": "/basics/sorts/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Пусть мы хотим поддерживать множество элементов и быстро выполнять на нём следующие операции: добавлять и удалять элементы, а также находить минимум.\nМожно попытаться сделать это с помощью динамического массива или списка. Тогда мы сможем добавлять и удалять элементы за $O(1)$ (чтобы быстро удалить элемент из массива, поменяем его местами с последним элементом, после чего уменьшим длину массива на один). Однако быстро находить минимум не получится: можно пытаться поддерживать указатель на минимальный элемент, но после каждого удаления минимума новый минимум получится найти лишь перебором всех оставшихся элементов за $\\Theta(n)$ (где $n$ - число элементов в множестве). Можно было бы поддерживать массив в отсортированном порядке (тогда минимум всегда будет в начале), но тогда не получится быстро добавлять или удалять произвольный элемент.\nДвоичная куча (пирамида), binary heap - структура данных, которая позволяет добавлять и удалять элементы за $O(\\log n)$, а также в любой момент иметь доступ к минимальному элементу в множестве за $O(1)$.\nОпределение Двоичная куча из $n$ элементов - это массив $a$ с индексами от 1 до $n$, образующий двоичное дерево: для любого $1 \u003c i \\leqslant n$ родитель $i$-го элемента - это элемент с номером $\\left\\lfloor\\frac{i}{2}\\right\\rfloor ;$ соответственно, для любого $1 \\leqslant i \\leqslant n$ дети $i$-го элемента имеют номера $2 i, 2 i+1$ (если элементы с такими номерами существуют).\nПри этом выполняется свойство кучи: значение любого элемента не меньше значения его родителя: $a[i] \\geqslant a\\left[\\left\\lfloor\\frac{i}{2}\\right]\\right]$. Такую кучу также могут называть неубывающая пирамида. Аналогично, если $a[i] \\leqslant a\\left[\\left\\lfloor\\frac{i}{2}\\right]\\right]$ - то невозрастающая пирамида.\nЗаметим, что отсортированный массив всегда является кучей. Но фиксированный набор элементов обычно можно упорядочить и многими другими способами так, что тоже получится куча. Будем пользоваться терминами из теории графов. Элементы дерева (кучи) будем называть вершинами. Корень дерева - единственная вершина без родителя (в нашем случае это $a[1]$ ). Потомки вершины - это она сама, её дети, а также все их потомки; у листа (вершины без детей) потомков (кроме него самого) нет. Предок вершины $v$ - любая такая вершина $u$, что $v$ - потомок $u$. Поддерево вершины $v$ состоит из всех её потомков; $v$ - корень своего поддерева.\nИногда мы будем называть кучей не массив с индексами от 1 до $n$, а просто дерево, в котором свойство кучи выполняется для всех пар родитель-ребёнок. В этом смысле любое поддерево кучи тоже является кучей.\nГлубина $x$ - количество вершин на пути от корня дерева до $x$ (все вершины являются потомками корня). Высота вершины $x$ - максимальное количество вершин на пути от $x$ до какого-либо её потомка. Высота дерева - максимум из высот вершин, то есть высота корня.\nГлубина $i$-й вершины равна $\\lfloor\\log i\\rfloor+1$, так как $i$ нужно поделить на два нацело $\\lfloor\\log i\\rfloor$ раз, чтобы получить 1.\nВысота корня - это максимум из глубин всех его потомков. Максимальная глубина у $n$-й вершины $-\\lfloor\\log n\\rfloor+1$. Значит, высота кучи из $n$ элементов равна $\\lfloor\\log n\\rfloor+1$.\nБазовые операции Научимся выполнять три простых операции: getMin() - поиск минимального элемента в куче, insert(x) - добавление нового элемента $x$ в кучу, и extractMin() - удаление минимума из кучи. Последние две операции будут пользоваться вспомогательными операциями $\\operatorname{siftUp(i)~и~siftDown(i).~}$\nПоиск минимума Из определения кучи сразу же следует, что минимальный элемент всегда будет находиться в корне. Тогда просто вернём $a[1]$, время работы $-O(1)$.\n1 2 getMin(): return a[1] Добавление нового элемента Пусть свойство кучи “практически” выполняется: для некоторого $i$ известно, что $a[i]=x$ можно увеличить так, что массив $a$ станет кучей. Такую почти кучу можно “починить”, не меняя множество лежащих в ней значений, следующим образом: просто будем “поднимать” $x$ вверх, пока $x$ меньше значения в родителе.\n1 2 3 4 siftUp(i): while i \u003e 1 and a[i] \u003c a[i / 2]: swap(a[i], a[i / 2]) i /= 2 Свойства Пусть известно, что $a[i]=x$ можно увеличить так, что массив $a$ станет кучей. Тогда после выполнения $\\operatorname{siftUp}(i)$ массив $a$ станет кучей. Доказательство Докажем утверждение индукцией по $i$. База. Если $i=1$, то $a$ уже является кучей (если корень можно увеличить так, что он станет не больше детей, то он и сейчас не больше детей). siftUp(1) не меняет массив $a$.\nПереход. Заметим сначала, что если $a$ уже является кучей, то $\\operatorname{siftUp}(i)$ ничего не сделает, поэтому утверждение предложения верно.\nЕсли $a$ не является кучей, то единственная пара родитель-ребёнок, для которой не выполняется свойство кучи - это $x=a[i]",
    "description": "Пусть мы хотим поддерживать множество элементов и быстро выполнять на нём следующие операции: добавлять и удалять элементы, а также находить минимум.\nМожно попытаться сделать это с помощью динамического массива или списка. Тогда мы сможем добавлять и удалять элементы за $O(1)$ (чтобы быстро удалить элемент из массива, поменяем его местами с последним элементом, после чего уменьшим длину массива на один). Однако быстро находить минимум не получится: можно пытаться поддерживать указатель на минимальный элемент, но после каждого удаления минимума новый минимум получится найти лишь перебором всех оставшихся элементов за $\\Theta(n)$ (где $n$ - число элементов в множестве). Можно было бы поддерживать массив в отсортированном порядке (тогда минимум всегда будет в начале), но тогда не получится быстро добавлять или удалять произвольный элемент.",
    "tags": [],
    "title": "7. Двоичная куча",
    "uri": "/basics/heap/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Хеш-таблица Пусть мы хотим поддерживать множество $A$ элементов - ключей, то есть уметь добавлять ключ в $A$, удалять ключ из $A$, а также искать ключ в $A$. Мы будем считать, что все ключи берутся из множества $U=\\{0,1, \\ldots,|U|-1\\}$.\nВ общем случае ключами могут быть какие угодно объекты, которым можно каким-либо образом сопоставить числа (например, строки).\nЕсли $|U|$ не очень велико, можно просто создать массив размера $|U|$ и хранить каждый ключ в ячейке с номером, равным этому ключу. Чтобы понимать, каких ключей в $A$ нет, в соответствующих ячейках будем хранить специальное значение, не равное ни одному из ключей, например, -1 . Тогда все операции можно осуществлять за $O(1)$.\nНедостаток этого подхода в том, что он он использует $|U|$ памяти, и поэтому работает, только если множество $U$ не слишком велико. Кроме того, если в любой момент времени $|A| \\leqslant n$, и $n$ намного меньше $|U|$, то большая часть массива никак не будет использоваться, что непрактично.\nХеш-таблица позволяет решить ту же задачу, используя $\\Theta(n)$ памяти, и осуществляя все операции в среднем за $O(1)$. Идея состоит в том, чтобы использовать массив размера $m$, где $m$ намного меньше $|U|$, и ключ $x$ хранить в ячейке с номером $h(x)$, пользуясь хеш-функцией $h: U \\rightarrow M=\\{0,1, \\ldots, m-1\\}$.\nМожет случиться так, что $x \\neq y$, но $h(x)=h(y)$. Такую ситуацию мы будем называть коллизией. Хеш-функцию стараются выбрать так, чтобы минимизировать число коллизий. Тем не менее, поскольку $m\u003c|U|$, всегда найдётся пара ключей из $U$ с одинаковым значением хеш-функции, то есть какой бы хорошей ни была хеш-функция, коллизии иногда будут происходить.\nМетод цепочек Самый простой способ разрешения коллизий - в каждой ячейке массива хранить “цепочку” - список всех ключей, попавших в эту ячейку. Этот список можно реализовывать как с помощью связного списка, так и с помощью динамического массива, или даже ещё одной внутренней хеш-таблицы (чуть позже мы увидим пример, когда именно такой способ оказывается полезен).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 vector\u003cint\u003e T[m] # хеш-таблица find(x): # возвращает True, если x лежит в хеш-таблице p = h(x) for i = 0..(T[p].size() - 1): if T[p][i] == x: return True return False insert(x): if not find(x): T[h(x)].push_back(x) delete(x): p = h(x) for i = 0..(T[p].size() - 1): if T[p][i] == x: swap(T[p][i], T[p].back()) T[p].pop_back() break Все операции работают за время, пропорциональное длине списка в соответствующей ячейке, то есть за $O(|T[h(x)]|+1)$. Если ключи распределены по таблице равномерно, то есть в каждом списке оказалось примерно $\\frac{n}{m}$ элементов, то операции будут работать в среднем за $O\\left(\\frac{n}{m}+1\\right)$, то есть за $O(1)$ при $m=\\Omega(n)$ (мы докажем подобное утверждение более формально, когда будем рассматривать технику универсального хеширования).\nКак добиться равномерного распределения? Подобрать “хорошую” хеш-функцию. Иногда это можно сделать, пользуясь информацией о том, с какими ключами придётся работать.\nТак, если известно, что ключи выбираются из множества $U$ случайно равновероятно, и $|U|$ делится на $m$, то подойдёт хеш-функция $h(x)=x \\bmod m$ : каждый ключ попадёт в каждую ячейку таблицы с равной вероятностью.\nНа практике часто используют хеш-функцию $h(x)=x \\bmod m$, даже если о ключах, с которыми придётся работать, ничего не известно. При этом $m$ обычно стараются выбирать простым.\nОткрытая адресация Поговорим о ещё одном способе разрешения коллизий. В хеш-таблицах с открытой адресацией в каждой ячейке хранится не более одного ключа, а при поиске ключа ячейки проверяются в некотором порядке, пока не найдётся этот ключ или пустая ячейка. Этот порядок, конечно же, будет зависеть от того, какой ключ мы ищем. Поскольку в каждой ячейке хранится не более одного ключа, в таблице размера $m$ не может храниться больше $m$ ключей.\nБолее формально, теперь мы будем работать с хеш-функцией от двух аргументов $h: U \\times\\{0, \\ldots, m-1\\} \\rightarrow\\{0, \\ldots, m-1\\}$, и при поиске $x$ перебирать ячейки в порядке $h(x, 0), h(x, 1), \\ldots, h(x, m-1)$. При этом мы будем требовать, чтобы эта последовательность (будем называть её последовательностью проб) была перестановкой чисел от 0 до $m-1$ (чтобы не проверять одну ячейку несколько раз, и чтобы рано или поздно проверить каждую ячейку).\nПоследовательность проб чаще всего строят одним из следующих трёх способов (здесь $h^{\\prime}, h_{1}, h_{2}: U \\rightarrow\\{0, \\ldots, m-1\\}$ - вспомогательные хеш-функции):\nЛинейное пробирование. $h(x, i)=\\left(h^{\\prime}(x)+c \\cdot i\\right) \\bmod m$. Обычно используют $c=1$. Квадратичное пробирование. $h(x, i)=\\left(h^{\\prime}(x)+c_{1} \\cdot i+c_{2} \\cdot i^{2}\\right) \\bmod m$. Двойное хеширование. $h(x, i)=\\left(h_{1}(x)+i \\cdot h_{2}(x)\\right) \\bmod m$. При этом $c$ в первом способе и $h_{2}(x)$ в третьем должны быть взаимно просты с $m$ (чтобы последовательность пробегала все ячейки). Во втором способе по тем же причинам тоже подойдут не все $c_{1}, c_{2}$.\nЛинейное пробирование самое простое и имеет наименьшую константу во времени работы, но страдает от кластеризации: блоки из лежащих в таблице подряд ключей со временем становятся всё больше и больше (потому что всё больше и больше становится вероятность попасть в такой блок).\nДвойное хеширование отличается тем, что может дать $\\Theta\\left(m^{2}\\right)$ различных последовательностей (в первых двух способах вся последовательность проб однозначно определяется по $h^{\\prime}(x)$, поэтому всего возможно $\\Theta(m)$ различных последовательностей), поэтому менее подвержено кластеризации. Квадратичное пробирование - некий компромисс между линейным пробированием и двойным хешированием.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int T[m] # Хеш-таблица # считаем, что ключи - неотрицательные целые числа, # в пустых ячейках лежит -1 getIndex(x): # возвращает индекс ячейки, в которой лежит x, # или индекс первой встретившейся пустой ячейки for i = 0..(m-1): p = h(x, i) if T[p] == x or T[p] == -1: return p error \"Hash table is full\" find(x): # возвращает True, если x лежит в хеш-таблице return T[getIndex(x)] == x insert(x): p = getIndex(x) if T[p] != -1: return # x уже есть в таблице T[p] = x Удалить ключ из хеш-таблицы теперь не так просто. Если просто пометить ячейку, где лежал ключ, как свободную, то поиск других ключей может перестать работать корректно (потому что при вставке другого ключа мы могли проходить через эту ячейку при поиске свободного места). Можно при удалении специальным образом помечать ячейку и пропускать её при дальнейшей работе с таблицей, но такую ячейку никогда нельзя будет переиспользовать.\n1 2 3 4 delete(x): p = getIndex(x) if T[p] != -1: T[p] = -2 # getIndex всегда будет пропускать такую ячейку $$ 1+\\alpha(1+\\alpha(1+\\alpha(1+\\cdots))) \\leqslant \\frac{1}{1-\\alpha} $$Мы эвристически оценили время работы операций в среднем как $O\\left(\\frac{1}{1-\\alpha}\\right)$. На практике хеш-таблицы с открытой адресацией действительно работают быстро, пока $\\alpha$ достаточно далеко от единицы. Когда $\\alpha$ приближается к единице (скажем, когда $\\alpha\u003e\\frac{2}{3}$ ), можно построить новую таблицу вдвое большего размера, и перенести все элементы в неё (удалённые элементы при этом, конечно, переносить не надо).\nАссоциативный массив В хеш-таблице можно хранить не просто ключи, а пары (ключ, значение). Получится ассоциативный массив - массив с произвольными индексами.\nВ С++ есть встроенные реализации хеш-таблицы и ассоциативного массива - это std::unordered_set, std::unordered_map.\nОтметим, что множество ключей хеш-таблицы (и реализации ассоциативного массива через хеш-таблицу) никак не упорядочено.\nВ С++ есть и реализации упорядоченного множества и ассоциативного массива с упорядоченными ключами - std::set, std::map. Эти структуры позволяют совершать больше различных операций (например, находить следующий по значению ключ). Однако они устроены сильно сложнее (мы поговорим об их устройстве позже), а операции работают медленнее: даже поиск ключа требует в худшем случае $\\Theta(\\log n)$ времени.\nСортировка Киркпатрика-Рейша С помощью хеш-таблицы можно соптимизировать поразрядную сортировку, улучшив время работы до $O(n \\log \\log C)$ на массиве из $n$ целых чисел от 0 до $C-1$.\nАлгоритм (Kirkpatrick, Reisch, 1984) выглядит следующим образом: пусть $C=2^{2^{k}}$ (если надо, округлим $C$ вверх до ближайшего числа такого вида, при этом $\\log \\log C$ увеличится не более, чем на один). Если $C \\leqslant n$, просто отсортируем числа подсчётом за $O(n)$, иначе представим каждое число в $\\sqrt{C}$-ичной системе счисления; при этом каждое число будет состоять из двух $2^{k-1}$-битных цифр ( $\\sqrt{C}=2^{2^{k-1}}$ ). Осталось отсортировать числа по старшей цифре, а при равенстве по младшей рекурсивными вызовами.\nХеш-таблицы (точнее, ассоциативные массивы) нужны, чтобы сгруппировать числа в блоки с равной старшей цифрой.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 sort(vector\u003cint\u003e a, int k): n = a.size() if n \u003e= 2 ** (2 ** k): countingSort(a) return vector\u003cint\u003e l # сюда запишем все встретившиеся старшие цифры unordered_map\u003cint, vector\u003cint\u003e \u003e t # ассоциативный массив: в t[x] будем хранить список всех таких у, # что a[i] = x * 2 ** (2 ** (k - 1)) + y для некоторого i for i = 0..(n - 1): x = a[i] / (2 ** (2 ** (k - 1))) # x - старшие 2 ** (k - 1) 6ит a[i] y = a[i] % (2 ** (2 ** (k - 1))) # y - младшиие 2 ** (k - 1) бит а[i] if t.find(x) == t.end(): # x встретилось впервые l.push_back(x) t[x].push_back(y) sort(l, k - 1) # отсортировали старшие цифры a.clear() for (x in l): # перебираем старшие цифры в порядке возрастания r = t[x] i = maxElementIndex(r) # найдём индекс максимального элемента в r rmax = r[i] swap(r[i], r.back()) r.pop_back() sort(r, k - 1) # отсортировали все младшие цифры, кроме максимальной for (y in r): a.push_back(x * 2 ** (2 ** (k - 1)) + y) a.push_back(x * 2 ** (2 ** (k - 1)) + rmax) Получаем рекуррентное соотношение\n$$ T(n)=\\Theta(n)+T(|l|)+\\sum_{x \\in l} T(|t[x]|-1) $$Заметим, что\n$$ |l|+\\sum_{x \\in l}(|t[x]|-1)=|l|-|l|+\\sum_{x \\in l}|t[x]|=n, $$то есть на каждом уровне рекурсии суммарный размер подзадач равен $n$. При этом глубина рекурсии не превышает $k$, значит суммарно на всех уровнях совершено $O(k n)=$ $O(n \\log \\log C)$ действий. Отметим, что это оценка времени работы в среднем, поскольку мы используем хеш-таблицы.\nНа практике из-за большой константы во времени работы хеш-таблиц алгоритм как правило оказывается не быстрее обычных алгоритмов сортировки.\nНа похожей идее основано дерево ван Эмде Боаса (van Emde Boas, 1975), позволяющее делать те же операции, что и двоичная куча, на целых числах от 0 до $C-1$ в среднем за $O(\\log \\log C)$. На практике оно практически не используется по тем же причинам\nУниверсальное хеширование Можно ли раз и навсегда выбрать для хеш-таблицы фиксированную хеш-функцию $h: U \\rightarrow M$, которая будет хорошо работать на любых входных данных? Нет: поскольку $|U|\u003e|M|$, всегда найдутся $\\lceil|U| /|M|\\rceil$ ключей с одинаковым значением хеш-функции (причём отношение $|U| /|M|$, как правило, достаточно велико). Тогда, если в запросах будет встречаться много таких ключей (например, если злоумышленник, знающий хеш-функцию, будет специально посылать такие запросы), эти запросы будут обрабатываться долго.\nЭту проблему можно решить следующим образом: до начала работы хеш-таблицы выберем хеш-функцию случайно из некоторого семейства. Если правильно подобрать семейство, запросы по-прежнему будут обрабатываться в среднем быстро, вне зависимости от того, какие ключи подаются на вход.\nОпределение Семейство $\\mathcal{H}$ хеш-функций, действующих из множества $U$ во множество $M=\\{0,1, \\ldots, m-1\\}$, называется универсальным, если для любой пары различных ключей $k, l \\in U$ количество таких хеш-функций $h \\in \\mathcal{H}$, что $h(k)=h(l)$, не превосходит $\\frac{|\\mathcal{H}|}{m}$.\nТеорема Универсальное хеширование Пусть хеш-функция $h \\in \\mathcal{H}, h: U \\rightarrow\\{0,1, \\ldots, m-1\\}$ была случайно выбрана из универсального семейства $\\mathcal{H}$, и использована при работе хеш-таблицы размера $m$. Пусть в хеш-таблицу уже были добавлены $n$ ключей $l_{1}, \\ldots, l_{n}$, коллизии разрешались методом цепочек. Тогда для любого ключа $k \\in U$ математическое ожидание длины списка в ячейке с индексом $h(k)$ не превосходит $1+\\frac{n}{m}$. Доказательство Для каждой пары ключей $a, b \\in U$ введём величину $\\chi(h, a, b)$, равную единице, если $h(a)=h(b)$, и нулю иначе. $\\chi(h, a, a)=1$ для любой $h$. При этом для $a \\neq b$ математическое ожидание $\\chi(h, a, b)$ не превосходит $\\frac{1}{m}$ по определению универсального семейства:\n$$ \\mathbb{E}(\\chi(h, a, b))=\\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\chi(h, a, b) \\leqslant \\frac{1}{|\\mathcal{H}|} \\cdot \\frac{|\\mathcal{H}|}{m}=\\frac{1}{m} $$Обозначим $L=\\left\\{l_{1}, \\ldots, l_{n}\\right\\}$. Математическое ожидание длины списка в ячейке с индексом $h(k)$ равняется\n$$ \\mathbb{E}\\left(\\sum_{l \\in L} \\chi(h, k, l)\\right)=\\sum_{l \\in L} \\mathbb{E}(\\chi(h, k, l)) \\leqslant 1+\\sum_{l \\in L \\backslash\\{k\\}} \\mathbb{E}(\\chi(h, k, l)) \\leqslant 1+\\frac{n}{m} $$ Следствие Пусть хеш-таблица размера $m$ использует технику универсального хеширования и метод цепочек для разрешения коллизий. Математическое ожидание суммарного времени работы $k$ операций $\\operatorname{insert}$, $\\operatorname{find}$, $\\operatorname{delete}$, среди которых $O(m)$ операций $\\operatorname{insert}$, есть $\\Theta(k)$. Доказательство В любой момент времени в таблице лежат не более $O(m)$ элементов. Время работы любой операции не превосходит времени вычисления хеш-функции (которое мы считаем константным) и времени прохода по списку в ячейке с индексом, равным значению хеш-функции. По предыдущей теореме, математическое ожидание времени выполнения каждой операции не превосходит $O\\left(1+\\frac{O(m)}{m}\\right)=O(1)$. Тогда математическое ожидание суммарного времени работы всех операций не превосходит $O(k)$, то есть равняется $\\Theta(k)$.\nЕщё раз отметим, что теорема и следствие выполняются для любых последовательностей запросов; математическое ожидание берётся не по входным данным, а по случайному выбору хеш-функции.\nПостроение универсального семейства хеш-функций Построим универсальное семейство хеш-функций для чисел, влезающих в машинное слово (считаем, что арифметические операции над числами выполняются за $O(1)$ ).\nТеорема Пусть $p$ - такое простое, что $U \\subset\\{0,1, \\ldots, p-1\\}$; пусть $m \u003c p$. Для любых целых $1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p$ определим хеш-функцию\n$$ h_{a, b}(k)=((a k+b) \\bmod p) \\bmod m $$Тогда семейство хеш-функций\n$$ \\mathcal{H}_{p, m}=\\left\\{h_{a, b}: 1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p\\right\\} $$является универсальным.\nДоказательство Пусть $k \\neq l$ - два различных ключа. Поймём, для каких $a, b$ верно $h_{a, b}(k)=h_{a, b}(l)$.\nЗаметим сначала, что если $r=(a k+b) \\bmod p, s=(a l+b) \\bmod p$, то $r \\neq s$, так как $r-s \\equiv a(k-l) \\not \\equiv 0(\\bmod p)$.\nБолее того, разным парам $(a, b)$ соответствуют разные пары $(r, s)$, так как по $(r, s)$ можно восстановить $(a, b): a \\equiv(r-s)(k-l)^{-1}(\\bmod p), b \\equiv(r-a k)(\\bmod p)$.\nРазличных пар $(a, b)$ с $1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p$ всего $(p-1) \\cdot p$. Различных пар $(r, s)$ с $0 \\leqslant r \\neq s \u003c p$ всего $p^{2}-p$, то есть столько же. Значит, мы установили взаимно однозначное соответствие между множествами пар ( $a, b$ ) и пар $(r, s)$.\nОстаётся заметить, что $h_{a, b}(k)=h_{a, b}(l)$ тогда и только тогда, когда $r \\equiv s(\\bmod m)$. Для фиксированного $0 \\leqslant r \u003c p$ количество $0 \\leqslant s \u003c p$ таких, что $s \\neq r$, но $r \\equiv s(\\bmod m)$, не превосходит $\\left\\lceil\\frac{p}{m}\\right\\rceil-1 \\leqslant \\frac{p+m-1}{m}-1=\\frac{p-1}{m}$.\nТогда количество хеш-функций $h_{a, b} \\in \\mathcal{H}_{p, m}$, для которых $h_{a, b}(k)=h_{a, b}(l)$ равняется количеству пар $(r, s)$ таких, что $0 \\leqslant r \\neq s \u003c p, r \\equiv s(\\bmod m)$, которое не превосходит $p \\cdot \\frac{p-1}{m}=\\frac{\\left|\\mathcal{H}_{p, m}\\right|}{m}$.\nСовершенное хеширование Если множество используемых ключей известно заранее (например, в языках программирования множество зарезервированных слов фиксировано), то можно построить хеш-таблицу, операции в которой будут работать за $O(1)$ в худшем случае. При этом можно добиться того, чтобы хеш-таблица имела размер $O(n)$, где $n$ - число используемых ключей.\nСнова считаем, что все ключи - целые неотрицательные числа, меньшие некоторого простого $p$. Построим хеш-таблицу с $m=n$ ячейками, при этом хеш-функцию аккуратно выберем из универсального семейства $\\mathcal{H}_{p, m}$. В каждой ячейке этой таблицы вместо списка мы заведём хеш-таблицу второго уровня, причём, если в ячейку с индексом $j$ попало $n_{j}$ ключей, то внутреннюю хеш-таблицу в этой ячейке сделаем размера $m_{j}=n_{j}^{2}$. Хешфункцию для внутренней таблицы мы аккуратно выберем из универсального семейства $\\mathcal{H}_{p, m_{j}}$.\nОказывается, можно подобрать такие хеш-функции, что во внутренних таблицах не будет коллизий, а суммарный размер всех таблиц будет оцениваться как $O(n)$.\nВ доказательстве оценок мы будем пользоваться следующим фактом (под $\\mathbb{P}(A)$ имеется в виду вероятность того, что произошло событие $A$ ):\nНеравенство Маркова Пусть $X$ - неотрицательная случайная величина с конечным математическим ожиданием. Тогда для любого $a\u003e0$ верно\n$$ \\mathbb{P}(X \\geqslant a) \\leqslant \\frac{\\mathbb{E} X}{a} $$ Доказательство Поскольку $X$ неотрицательно, $\\mathbb{E} X \\geqslant 0 \\cdot \\mathbb{P}(X \u003c a)+a \\cdot \\mathbb{P}(X \\geqslant a)$, откуда следует требуемое неравенство.\nТеорема Пусть в хеш-таблице размера $m=n^{2}$ хранятся $n$ ключей, причём хешфункция $h(\\cdot)$ была случайно выбрана из $\\mathcal{H}_{p, m}$. Тогда с вероятностью более чем $\\frac{1}{2}$ коллизий нет (все ключи хранятся в разных ячейках). Доказательство Для каждой пары различных ключей введём величину $\\chi(h, a, b)$, равную единице, если $h(a)=h(b)$, и нулю иначе. Как и в теореме Универсального хеширования 8.6.1, $\\mathbb{E}(\\chi(h, a, b)) \\leqslant \\frac{1}{m}=\\frac{1}{n^{2}}$. Рассмотрим случайную величину $X$ - количество коллизий. Тогда\n$$ \\mathbb{E} X=\\mathbb{E}\\left(\\sum_{a \\neq b}(\\chi(h, a, b))\\right)=\\sum_{a \\neq b} \\mathbb{E}(\\chi(h, a, b)) \\leqslant \\frac{n(n-1)}{2} \\cdot \\frac{1}{n^{2}}\u003c\\frac{1}{2} $$Поскольку $X$ принимает только целые неотрицательные значения, по неравенству Маркова\n$$ \\mathbb{P}(X\u003e0)=\\mathbb{P}(X \\geqslant 1) \\leqslant \\mathbb{E} X\u003c\\frac{1}{2} $$ Из этой теоремы сразу же следует, что если мы можем позволить себе использовать хеш-таблицу размера $n^{2}$, то, сделав несколько попыток, мы подберём такую хеш-функцию, что все ключи попадут в разные ячейки таблицы (вероятность того, что мы не найдём такую хеш-функцию за $s$ попыток, меньше $2^{-s}$; в среднем понадобится не более двух попыток). Наша двухуровневая схема нужна, чтобы уменьшить количество используемой памяти до $O(n)$.\nТе же рассуждения показывают, что, сделав несколько попыток, мы подберём хешфункцию без коллизий для каждой внутренней хеш-таблицы. Осталось понять, почему суммарный размер таблиц можно сделать линейным от числа ключей.\nТеорема Пусть в хеш-таблице размера $m=n$ хранятся $n$ ключей, причём хешфункция $h(\\cdot)$ была случайно выбрана из $\\mathcal{H}_{p, m}$. Пусть в $j$-ю ячейку попало $n_{j}$ ключей, которые были помещены во внутреннюю хеш-таблицу размера $m_{j}=n_{j}^{2}$. Тогда математическое ожидание суммарного размера внутренних хеш-таблиц меньше $2 n$. Доказательство Заметим сначала, что\n$$ \\begin{aligned} \u0026 \\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right)=\\mathbb{E}\\left(\\sum_{j=0}^{m-1} n_{j}^{2}\\right)=\\mathbb{E}\\left(\\sum_{j=0}^{m-1}\\left(n_{j}+2 \\cdot \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right)\\right)= \\\\ = \u0026 \\mathbb{E}\\left(\\sum_{j=0}^{m-1} n_{j}\\right)+2 \\cdot \\mathbb{E}\\left(\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right)=n+2 \\cdot \\mathbb{E}\\left(\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right) . \\end{aligned} $$Теперь заметим, что $\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}$ - это суммарное количество случившихся коллизий. Из доказательства предыдущей теоремы мы знаем, что математическое ожидание количества коллизий не превосходит $\\frac{n(n-1)}{2} \\cdot \\frac{1}{m}=\\frac{n-1}{2}$, так как $m=n$. Получаем\n$$ \\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right) \\leqslant n+2 \\cdot \\frac{n-1}{2}=2 n-1\u003c2 n . $$ Следствие В предположениях теоремы суммарный размер внутренних хеш-таблиц окажется больше или равен $4 n$ с вероятностью меньше $\\frac{1}{2}$. Доказательство Воспользуемся неравенством Маркова:\n$$ \\mathbb{P}\\left(\\sum_{j=0}^{m-1} m_{j} \\geqslant 4 n\\right) \\leqslant \\frac{\\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right)}{4 n}\u003c\\frac{2 n}{4 n}=\\frac{1}{2} $$ Снова получаем, что за $s$ попыток с вероятностью хотя бы $1-2^{-s}$ (и не больше чем за две попытки в среднем) мы подберём такую хеш-функцию для внешней хеш-таблицы, что суммарный размер всех внутренних хеш-таблиц будет оцениваться как $O(n)$ (будет меньше $4 n$ ).",
    "description": "Хеш-таблица Пусть мы хотим поддерживать множество $A$ элементов - ключей, то есть уметь добавлять ключ в $A$, удалять ключ из $A$, а также искать ключ в $A$. Мы будем считать, что все ключи берутся из множества $U=\\{0,1, \\ldots,|U|-1\\}$.\nВ общем случае ключами могут быть какие угодно объекты, которым можно каким-либо образом сопоставить числа (например, строки).\nЕсли $|U|$ не очень велико, можно просто создать массив размера $|U|$ и хранить каждый ключ в ячейке с номером, равным этому ключу. Чтобы понимать, каких ключей в $A$ нет, в соответствующих ячейках будем хранить специальное значение, не равное ни одному из ключей, например, -1 . Тогда все операции можно осуществлять за $O(1)$.",
    "tags": [],
    "title": "8. Хеширование",
    "uri": "/basics/hashing/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Категории",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Теги",
    "uri": "/tags/index.html"
  }
]
