var relearn_searchindex = [
  {
    "breadcrumb": "",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Basics",
    "uri": "/basics/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "1. Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.\nБаза. Утверждение верно для $n=0,1$.\nПереход. Пусть $n\u003e1$, тогда $F_{n}=F_{n-1}+F_{n-2} \\leqslant 2^{n-1}+2^{n-2}\u003c2^{n}$.\nС другой стороны, $F_{n} \\geqslant 2 \\cdot F_{n-2} \\geqslant 2 \\cdot 2^{\\lfloor(n-2) / 2\\rfloor}=2^{\\lfloor n / 2\\rfloor}$.\nЭкспоненциальный алгоритм Следующий рекурсивный алгоритм вычисляет $n$-ое число Фибоначчи, точно следуя определению:\nfib(n): if n \u003c= 1: return 1 return fib(n - 1) + fib(n - 2) Каково время работы этого алгоритма? Оценим $T(n)$ - суммарное количество вызовов fib , происходящих при выполнении $\\mathrm{fib}(\\mathrm{n})$ : если $n \\leqslant 1$, то $T(n)=1$; иначе\n$$ T(n)=1+T(n-1)+T(n-2) . $$Несложно доказать по индукции, что $F_{n} \\leqslant T(n)\u003c2 F_{n}$. В каждом вызове fib совершается ограниченное число (скажем, не больше пяти) операций, поэтому время работы алгоритма примерно пропорционально $F_{n}$ (чуть позже у нас появится формальное определение этого “примерно”).\nИз леммы 1.1.1 следует, например, что, $F_{300} \\geqslant 2^{150}\u003e10^{45}$. На компьютере, выполняющем $10^{9}$ операций в секунду, fib(300) будет выполняться больше $10^{36}$ секунд.\nПолиномиальный алгоритм Посмотрим на дерево рекурсивных вызовов алгоритма fib (рис. 1.1). Видно, что алгоритм много раз вычисляет одно и то же. Давайте сохранять результаты промежуточных вычислений в массив: Рис. 1.1: Дерево рекурсивных вызовов fib\nfibFast(n): if n \u003c= 1: return 1 int f[n + 1] # создаём массив с индексами 0..n f[0] = f[1] = 1 for i = 2..n: f[i] = f[i - 1] + f[i - 2] return f[n] На каждой итерации цикла совершается одно сложение, всего итераций примерно $n$, поэтому количество сложений, выполняемых в ходе нового алгоритма, примерно пропорционально $n$. Есть ещё одна тонкость - из леммы 1.1.1 следует, что двоичная запись $F_{n}$ имеет длину порядка $n$. Чуть позже мы увидим, что сложение двух $n$-битовых чисел требует порядка $n$ элементарных операций, значит общее время работы нового алгоритма примерно пропорционально $n^{2}$. С помощью fibFast можно уже за разумное время вычислить не только $F_{300}$, но и $F_{100000}$.\n1.2 О-символика Хорошо себя зарекомендовал и стал общепринятым следующий подход - оценивать время работы алгоритма некоторой функцией от входных параметров, при этом пренебрегая ограниченными множителями. Это позволяет эффективно сравнивать алгоритмы между собой, при этом не нужно заниматься точным подсчётом количества элементарных операций. Примерно так мы и рассуждали об алгоритмах вычисления чисел Фибоначчи. Введём несколько обозначений, которые помогут проводить подобные рассуждения более кратко и точно.\nВремя работы алгоритма - это, конечно, всегда неотрицательная функция. Тем не менее, мы даём следующие определения для функций, принимающих произвольные вещественные значения, поскольку такие функции часто возникают в процессе рассуждений.\nОпределение 1.2.1 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=O(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$. $f=\\Omega(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$. $f=\\Theta(g)$, если существуют такие $C_{1}\u003e0, C_{2}\u003e0, N\u003e0$, что для любого $n\u003eN$ : $C_{1} \\cdot|g(n)| \\leqslant|f(n)| \\leqslant C_{2} \\cdot|g(n)|$. Запись $f=O(g)$ можно понимать как \" $|f| \\leqslant|g|$ с точностью до константы\". Аналогично, $\\Omega(\\cdot)$ можно считать аналогом $\\geqslant$, а $\\Theta(\\cdot)$ - аналогом $=$. Ещё один способ понимать запись $f=O(g)$ - отношение $\\frac{|f(n)|}{|g(n)|}$ ограничено сверху некоторой константой.\nМногие естественные свойства операторов сравнения $\\leqslant,=, \\geqslant$ выполняются и для их асимптотических аналогов. Сформулируем некоторые из этих свойств:\nСвойства 1.2.1 $f=\\Theta(g)$ тогда и только тогда, когда $f=O(g)$ и $f=\\Omega(g)$. $f=O(g)$ тогда и только тогда, когда $g=\\Omega(f)$. $f=\\Theta(g)$ тогда и только тогда, когда $g=\\Theta(f)$. Доказательство Докажем для примера п. 2.\nПусть $C\u003e0$, тогда $|f(n)| \\leqslant C \\cdot|g(n)|$ равносильно $|g(n)| \\geqslant \\frac{1}{C} \\cdot|f(n)|$.\nТаким образом, $f=O(g)$ с константой $C$ тогда и только тогда, когда $g=\\Omega(f)$ с константой $\\frac{1}{C}$.\nСуществуют также асимптотические аналоги $\u003c$ и $\u003e: o(\\cdot)$ и $\\omega(\\cdot)$. Определение 1.2.2 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=o(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$.\n$f=\\omega(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$.\nЗапись $f=o(g)$ можно понимать как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к нулю с увеличением $n$ “, а запись $f=\\omega(g)$ - как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к бесконечности с увеличением $n$ “.\nСвойства 1.2.2 $f=o(g)$ тогда и только тогда, когда $g=\\omega(f)$. Если $f=O(g)$ и $g=O(h)$, то $f=O(h)$. То же верно для $\\Omega, \\Theta, o, \\omega$. Если $f=O(g), g=o(h)$, то $f=o(h)$. Если $f=\\Omega(g), g=\\omega(h)$, то $f=\\omega(h)$. Если $f=O(h)$ и $g=O(h)$, то $f+g=O(h)$. То же верно для $o$. Если $f, g \\geqslant 0$, то то же верно и для $\\Omega, \\Theta, \\omega$. Если $f=o(g)$, то $f+g=\\Theta(g)$. Если $f, g \\geqslant 0, f=O(g)$, то $f+g=\\Theta(g)$. Для любого $C \\neq 0$ верно $C \\cdot f=\\Theta(f)$. Заметим, что у последних трёх свойств нет аналогов для обычных операторов сравнения.\nДоказательство Докажем для примера п. 3 (вариант с $O$ и $о$).\nНужно показать, что для любого $C\u003e0$ найдётся $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|h(n)|$. Зафиксируем $C\u003e0$.\n$f=O(g)$, поэтому найдутся $C_{1}\u003e0, N_{1}\u003e0$, что для любого $n\u003eN_{1}:|f(n)| \\leqslant C_{1} \\cdot|g(n)|$. $g=o(h)$, поэтому найдётся такое $N_{2}\u003e0$, что для любого $n\u003eN_{2}:|g(n)| \\leqslant \\frac{C}{C_{1}} \\cdot|h(n)|$.\nТогда для любого $n\u003e\\max \\left(N_{1}, N_{2}\\right):|f(n)| \\leqslant C_{1} \\cdot|g(n)| \\leqslant C_{1} \\cdot \\frac{C}{C_{1}} \\cdot|h(n)|=C \\cdot|h(n)|$.\nВернемся к алгоритмам вычисления чисел Фибоначчи. Пользуясь новыми обозначениями, мы показали, что время работы fibFast(n) можно оценить как $O\\left(n^{2}\\right)$. При этом время работы $\\mathrm{fib}(\\mathrm{n})$ можно оценить (сверху) как $O\\left(2^{n} \\cdot n\\right)$, и (снизу) как $\\Omega\\left(2^{\\lfloor n / 2\\rfloor}\\right)$.\n1.3 Многочлены, экспоненты и логарифмы Очень часто время работы алгоритма удаётся оценить функцией, являющейся комбинацией каких-то из трёх базовых типов: многочленов, экспонент и логарифмов. Так, время работы fibFast мы оценили многочленом $n^{2}$, а время работы fib - произведением экспоненты на многочлен: $2^{n} \\cdot n$. В связи с этим полезно изучить асимптотические свойства этих функций и научиться сравнивать их между собой.\nЛемма 1.3.1 Для любых $l\u003ek$ верно $n^{k}=o\\left(n^{l}\\right)$. Доказательство Для любого $C\u003e0$, при $n \\geqslant\\left(\\frac{1}{C}\\right)^{\\frac{1}{l-k}}$ верно\n$$ n^{k} \\leqslant C \\cdot \\frac{1}{C} \\cdot n^{k} \\leqslant C \\cdot n^{l-k} \\cdot n^{k}=C \\cdot n^{l} $$ Определение 1.3.1 Многочлен - это функция, которую можно записать в виде\n$$ f(n)=a_{0}+a_{1} n+a_{2} n^{2}+\\cdots+a_{d} n^{d} $$для некоторого $d \\geqslant 0$, так, что $a_{d} \\neq 0$. Это $d$ называют степенью многочлена и обозначают как $\\operatorname{deg}(f)$.\nСледствие 1.3.2 Пусть $f(n)$ - многочлен степени $d$. Тогда $f(n)=\\Theta\\left(n^{d}\\right)$. Доказательство Пусть $f(n) = a_{0} + a_{1} n + a_{2} n^{2} + \\cdots + a_{d} n^{d}$. Из леммы 1.3.1 и п. 6 предложения 1.2.2 следует, что $a_{j} n^{j} = o\\left(n^{d}\\right)$ для любого $0 \\leqslant j",
    "description": "1. Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.",
    "tags": [],
    "title": "1. Анализ сложности алгоритмов",
    "uri": "/basics/complexity/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\nВ произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.\n2.1 Сложение Используем всем знакомый со школы способ сложения чисел в столбик:\nadd(a, b, n): # a и b - двоичные записи чисел int c[n + 1] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: c[i] = a[i] + b[i] if c[i] \u003e= 2: c[i + 1] += 1, c[i] -= 2 return c Время работы алгоритма - $O(n)$, существенно быстрее нельзя, потому что столько времени занимает уже считывание входных данных или вывод ответа.\nАналогичный алгоритм можно написать для чисел, записанных в $b$-ичной системе счисления. Если сумма трёх $b$-ичных чисел помещается в 32(64)-битный тип данных, то алгоритм всё ещё будет корректен. При этом $n$-битное число будет иметь примерно $\\frac{n}{\\log b}$ цифр в $b$-ичной записи, то есть алгоритм будет работать за $O\\left(\\frac{n}{\\log b}\\right)=O(n)$, так как $\\frac{1}{\\log b}$ - это константа. Тем не менее, это может дать ускорение в несколько десятков раз, что безусловно бывает полезно на практике.\n2.2 Умножение Вспомним теперь и школьное умножение чисел в столбик (заметим лишь, что ответ имеет длину не больше $2 n$, поскольку $\\left.\\left(2^{n}-1\\right) \\cdot\\left(2^{n}-1\\right)\u003c2^{2 n}\\right)$ :\nmultiply(a, b, n): # a и b - двоичные записи чисел int c[2 * n] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: for j = 0..n - 1: c[i + j] += a[i] * b[j] for i = 0..2 * n - 2: if c[i] \u003e= 2: c[i + 1] += c[i] / 2 c[i] %= 2 return c Из-за двух вложенных циклов время работы этого алгоритма - уже $O\\left(n^{2}\\right)$. Приведём альтернативный рекурсивный алгоритм умножения двух чисел, пользующийся следующим правилом:\n$$ a \\cdot b= \\begin{cases}2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { если } b \\text { чётно, } \\\\ a+2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { иначе. }\\end{cases} $$multiply(a, b): # a и b - двоичные записи чисел if b == 0: return 0 c = multiply(a, b / 2) # деление нацело if b % 2 == 0: return 2 * c else: return 2 * c + a В этой схематичной записи под делением на два имеется ввиду битовый сдвиг вправо (то есть взятие двоичной записи без младшего бита), под умножением на два - битовый сдвиг влево (добавление нуля в начало битовой записи). Остаток по модулю два - это младший бит числа.\nАлгоритм произведёт $O(n)$ рекурсивных вызовов, поскольку при каждом вызове длина битовой записи $b$ уменьшается на один. В каждом вызове функции происходят битовый сдвиг влево, битовый сдвиг вправо, и, возможно сложение - всего $O(n)$ элементарных операций. Таким образом, общее время работы снова $O\\left(n^{2}\\right)$.\nЗаметим, что если длины битовых записей $a$ и $b$ равны $n$ и $m$, то время работы обоих алгоритмов можно оценить как $O(n m)$.\n2.3 Деление Пусть теперь мы хотим поделить $a$ на $b$, то есть найти такие $q, r$, что $a=q b+r$ и $0 \\leqslant r",
    "description": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\nВ произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.",
    "tags": [],
    "title": "2. Элементарная арифметика",
    "uri": "/basics/elementary_arithmetic/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "3.1 Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nТеорема 3.1.1 - Основная теорема о рекуррентных соотношениях.\" Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда\n$$T(n)= \\begin{cases}\\Theta\\left(n^{c}\\right), \u0026 \\text { если } c \u003e \\log_{b} a \\\\ \\Theta\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ \\Theta\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log_{b} a\\end{cases}$$ Доказательство Пусть мы уже доказали утверждение теоремы для чисел вида $n=b^{k}$. Рассмотрим такую функцию $k(n)$, что для любого $n$ верно $b^{k(n)-1} \u003c n \\leqslant b^{k(n)}$. При уменьшении $n$ количество веток в дереве рекурсии и их размер могли только уменьшиться, поэтому\n$$ T(n) = O \\left(T\\left(b^{k(n)}\\right)\\right)= \\begin{cases}O\\left(b^{k(n) c}\\right)=O\\left(n^{c}\\right), \u0026 \\text { если } c\u003e\\log _{b} a \\\\ O\\left(b^{k(n) c} \\log \\left(b^{k(n)}\\right)\\right)=O\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ O\\left(b^{k(n) \\log _{b} a}\\right)=O\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log _{b} a\\end{cases} $$(мы пользуемся тем, что $b^{k(n)} = b \\cdot b^{k(n)-1} \u003c b n = O(n)$ ). Доказательство же точной асимптотической оценки $(\\Theta)$ для произвольного $n$ требует ещё некоторого количества технических выкладок, которые мы опустим. Желающие могут прочитать их в главе 4.6 в Кормене1.\nТеперь считаем, что $n=b^{k}$ для некоторого $k$. Раскроем рекуррентность:\n$$ \\begin{gathered} T(n)=a \\cdot T\\left(\\frac{n}{b}\\right)+\\Theta\\left(n^{c}\\right)=\\Theta\\left(n^{c}+a \\cdot\\left(\\frac{n}{b}\\right)^{c}\\right)+a^{2} T\\left(\\frac{n}{b^{2}}\\right)= \\\\ =\\cdots=\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k-1}\\right)\\right)+a^{k} T(1)= \\\\ =\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)\\right) \\end{gathered} $$(последнее слагаемое $a^{k} T(1)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(n^{c} \\cdot\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)$, так как $\\left.T(1)=\\Theta(1), n=b^{k}\\right)$. Получаем геометрическую прогрессию со знаменателем $q=\\frac{a}{b^{c}}$.\nЕсли $c\u003e\\log _{b} a$, то $q \u003c 1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot \\frac{1-q^{k+1}}{1-q}\\right)=\\Theta\\left(n^{c} \\cdot \\frac{1}{1-q}\\right)=\\Theta\\left(n^{c}\\right) $$Если $c=\\log _{b} a$, то $q=1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot(k+1)\\right)=\\Theta\\left(n^{c} \\log _{b} n\\right)=\\Theta\\left(n^{c} \\log n\\right) $$Наконец, если $c \u003c \\log _{b} a$, то $q\u003e1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot\\left(q^{k}+\\frac{q^{k}-1}{q-1}\\right)\\right)=\\Theta\\left(n^{c} q^{k}\\right)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(a^{\\log _{b} n}\\right)=\\Theta\\left(n^{\\log _{b} a}\\right) $$ Introduction to Algorithms (Fourth Edition) ↩︎\nВ алгоритме Карацубы $a=3, b=2, c=1$, это соответствует третьему случаю теоремы, который и даёт $T(n)=\\Theta\\left(n^{\\log _{2} 3}\\right)$.\nОцените следующие алгоритмы Пример void foo(int* array, int n) { int sum = 0; int product = 1; for (int i = 0; i \u003c n; i++) { sum += array[i]; } for (int i = 0; i \u003c n; i++) { product += array[i]; } std::cout \u003c\u003c sum \u003c\u003c \", \" \u003c\u003c product; } void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = i + 1; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { if (arrA[i] \u003c arrB[j]) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } } void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { for (int j = 0; j \u003c 100000; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } }",
    "description": "3.1 Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nТеорема 3.1.1 - Основная теорема о рекуррентных соотношениях.\" Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда",
    "tags": [],
    "title": "3. Рекурентные соотношения",
    "uri": "/basics/recurrence_relation/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "4.1 Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.\nПоиск элемента в массиве по значению также имеет сложность $\\Theta(n)$, так как нужно проверить все элементы массива. Если массив специально упорядочен (например, элементы массива расположены в возрастающем порядке), то поиск можно делать быстрее (скоро мы изучим алгоритм двоичного поиска).\nНемного о работе с массивом в C++:\nint a[n]; // объявить массив длины n, нумерация ячеек от 0 до n-1 a[i]; // обращение к i-му элементу массива a[i] = x; // присвоить x в i-ю ячейку массива int b[n][m]; // объявить двумерный массив, нумерация ячеек двумя индексами, // от 0 до n-1 и от 0 до m-1 b[i][j]; // обращение к j-му элементу i-й строки массива b[i][j] = y; // присвоение 4.2 Связный список Связный список (linked list) состоит из узлов, каждый из которых содержит данные и ссылки на соседние узлы. В двусвязном списке поддерживаются ссылки на следующий и предыдущий узел, в односвязном списке - только на следующий.\nТакже поддерживаются ссылки на начало и конец списка (их часто называют головой и хвостом). Для того, чтобы посетить все узлы, можно начать с головы и переходить по ссылке на следующий узел, пока он существует.\nПреимущество списка перед массивом - возможность вставлять и удалять элементы за $O(1)$.\nНедостаток списка - невозможность быстрого доступа к произвольному элементу. Так, доступ к $i$-му элементу можно получить, лишь $i$ раз пройдя по ссылке вперёд, начиная из головы списка (то есть за $\\Theta(i)$ ).\nУдобно делать голову и хвост фиктивными элементами и не хранить в них никаких данных, тогда функции вставки и удаления элементов пишутся проще.\nПримерная реализация двусвязного списка на C++:\nstruct Node { // один узел списка Node *next, *prev; // указатели на следующий и предыдущий узлы int x; // данные, хранящиеся в узле }; struct List { Node *head, *tail; // указатели на начало и конец списка List() { // инициализируем пустой список - создаём фиктивные head и tail // и связываем их друг с другом head = new Node(); tail = new Node(); head-\u003enext = tail; tail-\u003eprev = head; } void pushBack(int x) { // вставить x в конец списка Node *v = new Node(); v-\u003ex = x, v-\u003eprev = tail-\u003eprev, v-\u003enext = tail; v-\u003eprev-\u003enext = v, tail-\u003eprev = v; } void insert(Node *v, int x) { // вставить x после v Nove *w = new Node(); w-\u003ex = x, w-\u003enext = v-\u003enext, w-\u003eprev = v; v-\u003enext = w, w-\u003enext-\u003eprev = w; } void erase(Node *v) { // удалить узел v v-\u003eprev-\u003enext = v-\u003enext; v-\u003enext-\u003eprev = v-\u003eprev; delete v; } Node *find(int x) { // найти x в списке for (Node *v = head-\u003enext; v != tail; v = v-\u003enext) { if (v-\u003ex == x) { return v; } } return nullptr; } }; Можно хранить узлы списка в массиве, тогда вместо указателей можно использовать числа - номера ячеек массива. Но тогда нужно либо заранее знать количество элементов в списке, либо использовать динамический массив (который мы скоро изучим).\nМожно также использовать встроенный в C++ двусвязный список - std::list. Односвязный список использует меньше дополнительной памяти, но не позволяет перемещаться по списку в сторону начала. Также из него сложнее удалять элементы - не получится удалить элемент, имея ссылку только на него. Нужно как-то получить доступ к предыдущему элементу, чтобы пересчитать ссылку из него на следующий.\n4.3 Динамический массив Пусть мы хотим научиться вставлять новые элементы в конец массива. Можно попытаться сразу создать массив достаточно большого размера, и в отдельной переменной поддер-\nживать его реальную длину. Но далеко не всегда максимальное количество элементов известно заранее.\nПоступим следующим образом: если мы хотим вставить новый элемент, а место в массиве закончилось, то создадим новый массив вдвое большего размера, и скопируем данные туда.\nint *a; // используемая память int size; // размер памяти int n; // реальный размер массива void pushBack(int x) { // вставить х в конец массива if (n == size) { int *b = new int[size * 2]; for (int i = 0; i \u003c size; ++i) { b[i] = a[i]; } delete[] a; a = b; size *= 2; } a[n] = x; n += 1; } void popBack() { // удалить последний элемент n -= 1; } В C++ можно и нужно использовать встроенную реализацию динамического массива std::vector.\nКаждая конкретная операция вставки элемента может работать долго $-\\Theta(n)$, где $n$ - длина массива. Зато можно показать, что среднее время работы операции вставки $O(1)$. Докажем этот факт при помощи метода амортизационного анализа, который нам ещё не раз пригодится.\nСвойство 4.3.1 Среднее время работы операции вставки - $O(1)$.\nЭто утверждение равносильно тому, что суммарное время работы $m$ операций вставки - $O(m)$.\nДоказательство Операция вставки работает не за $O(1)$ только тогда, когда происходит удвоение размера используемой памяти. Заметим, что если такая операция увеличила размер с size до $2 \\cdot$ size, то хотя бы size $/ 2$ предыдущих операций вставки работали за $O(1)$ (может быть, и больше, если происходили также удаления элементов). Тогда суммарное время работы этих size $/ 2+1$ операций есть $O$ (size), поэтому среднее время их работы есть $O(1)$.\n4.4 Стек, очередь, дек Стек (stack) позволяет поддерживать список элементов, вставлять элемент в конец списка, а также удалять элемент из конца списка. Часто также говорят, что он организован по принципу LIFO (last in - first out).\nОчередь (queue) организована по принципу FIFO (first in - first out). Она позволяет вставлять элемент в конец списка, а также удалять элемент из начала списка.\nДек (deque - double ended queue), или двусторонняя очередь, отличается от обычной очереди тем, что позволяет добавлять и удалять элементы как в начало, так и в конец списка.\nВсе эти структуры можно реализовать с помощью связного списка. Для стека и очереди хватит односвязного списка: в стеке достаточно поддерживать ссылку на предыдущий элемент, а в очереди - на следующий. Для реализации дека понадобится двусвязный список.\nТакже все три структуры можно реализовать с помощью динамического массива. В случае дека нужно научиться добавлять элементы в начало, для чего можно зациклить массив. То же самое можно сделать в реализации очереди, чтобы переиспользовать освободившееся место в начале массива. Приведём примерную реализацию дека на динамическом массиве:\nint *a; // используемая память int size; // размер памяти int b, e; // дек использует ячейки в диапазоне [b,e), или, // если b \u003e= e, в диапазонах [b,size) и [0,e) int getSize() { // узнать количество элементов в деке if (b \u003c e) { return e - b; } else { return e - b + size; } } void pushBack(int x) { // вставить x в конец дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[e] = x; e = (e + 1) % size; } void popBack() { // удалить элемент из конца дека e = (e - 1 + size) % size; } void pushFront(int x) { // вставить x в начало дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[b] = x; b = (b - 1 + size) % size; } void popFront() { // удалить элемент из начала дека b = (b + 1) % size; } В C++ существуют встроенные реализации этих структур - std::stack, std::queue, std::deque.",
    "description": "4.1 Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.",
    "tags": [],
    "title": "4. Базовые структуры данных",
    "uri": "/basics/basic_data_structures/index.html"
  },
  {
    "breadcrumb": "Basics",
    "content": "5.1 Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.\nТеперь можно рекурсивно повторить те же рассуждения - посмотреть на середину нового подоотрезка массива, и либо в середине найдётся $x$, либо мы поймём, в какой половине нового подотрезка нужно его искать. Будем повторять эти действия, пока не найдём $x$, либо не дойдём до подотрезка нулевой длины. Поскольку на каждом шаге длина отрезка уменьшается вдвое, это произойдёт через $O(\\log n)$ шагов.\nТакже можно оценить время работы нашего алгоритма с помощью теоремы 3.1.1: время работы алгоритма в худшем случае (когда $x$ так и не нашёлся, либо нашёлся в самом конце) удовлетворяет рекуррентному соотношению $T(n)=T\\left(\\frac{n}{2}\\right)+\\Theta(1)$. Это соответствует $a=1, b=2, c=0$ в формулировке теоремы. $c=0=\\log _{2} 1=\\log _{b} a$, тогда по теореме время работы алгоритма в худшем случае есть $\\Theta(\\log n)$.\nПримерная реализация алгоритма двоичного поиска:\nbinarySearch(a, n, x): # ищет x в массиве а длины n, возвращает индекс, # по которому лежит x, или -1, если х в массиве нет l = 0, r = n - 1 # границы интересующего нас подотрезка массива while l \u003c= r: # пока подотрезок непуст m = (l + r) / 2 if a[m] == x: return m if a[m] \u003c x: l = m + 1 else: r = m - 1 return -1 # x так и не нашёлся 5.2 Левое вхождение Возможно, $x$ встречается в массиве несколько раз (так как массив отсортирован, эти вхождения образуют подотрезок массива). Научимся находить количество вхождений $x$ в массив (то есть длину этого подотрезка).\nДостаточно найти индексы самого левого и самого правого вхождений $x$ в массив, тогда количество вхождений - это разность этих индексов плюс один. Начнём с левого вхождения.\nБудем снова поддерживать границы $l$ и $r$, но уже со следующим условием: пусть в любой момент времени выполняется $a[l]\u003c x$ и $a[r] \\geqslant x$. Удобно мысленно добавить\nк массиву фиктивные элементы $a[-1]=-\\infty$ и $a[n]=\\infty$, тогда не нужно отдельно обрабатывать случаи, когда все элементы меньше $x$, или все элементы больше или равны $x$.\nСам алгоритм становится только проще: смотрим на середину отрезка $(l, r)$, и, в зависимости от значения элемента массива с этим индексом, сдвигаем $l$ или $r$ так, чтобы требуемые от них условия не нарушились. Когда $l$ и $r$ указывают на соседние элементы (то есть $l=r-1$ ), всё ещё верно, что $a[l] \u003c x, a[r] \\geqslant x$. Тогда если $a[r]=x$, то $r$ индекс левого вхождения $x$, иначе $x$ в массиве не встречается.\nПримерная реализация:\nlowerBound(a, n, x): # возвращает минимальное i такое, что a[i] \u003e= x l = -1, r = n # l и r изначально указывают на фиктивные элементы while r - l \u003e 1: m = (l + r) / 2 if a[m] \u003c x: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r getLeftEntry(a, n, x): # возвращает номер левого вхождения или -1 i = lowerBound(a, n, x) if i == n or a[i] != x: return -1 return i Правое вхождение ищется примерно так же: нужно требовать от границ поиска $a[l] \\leqslant x$ и $a[r]\u003ex$, тогда в конце $l$ - правое вхождение, если $a[l]=x$, иначе $x$ в массиве не встречается. Заметим, что в реализации самого поиска нужно поправить всего одну строчку: a [m] \u003c x заменить на $\\mathrm{a}[\\mathrm{m}]$ \u003c= x .\nВ C++ есть встроенные функции std::lower_bound и std::upper_bound. Первая возвращает итератор, указывающий на первый элемент, больше или равный $x$, вторая на первый элемент, строго больший $x$. Пример использования:\ni = lower_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e= x; n, если все элементы а меньше х i = upper_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e x; n, если все элементы а меньше или равны х i = upper_bound(a, a + n, x) - a - 1; // максимальное i такое, что a[i] \u003c= x; -1, если все элементы а больше х cnt = upper_bound(a, a + n, x) - lower_bound(a, a + n, x); // количество вхождений х в а 5.3 Двоичный поиск по функции Можно рассмотреть и ещё более общую версию алгоритма. Пусть есть некоторая функция $f(i)$ такая, что $f(i)=0$ для всех $i$, меньших некоторого порога $t$, и $f(i)=1$ для всех $i \\geqslant t$. Тогда этот порог можно найти с помощью алгоритма двоичного поиска.\nИдея та же, что и при поиске левого вхождения: сначала возьмём такие границы поиска $l=L, r=R$, что $f(L)=0, f(R)=1$. После этого на каждом шаге будем сдвигать одну из границ $l, r$, в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. В тот момент, когда $l$ и $r$ отличаются на единицу, $r$ - искомый порог. Если считать, что значение функции в точке вычисляется за $O(1)$, то время работы алгоритма $-O(\\log (R-L))$, где $L, R-$ начальные границы поиска.\nЗаметим, что алгоритмы поиска левого и правого вхождения являются частными случаями этого алгоритма: в случае левого вхождения можно взять функцию $f$ такую,\nчто $f(i)=1$ тогда и только тогда, когда $a[i] \\geqslant x$; в случае правого вхождения $-f(i)=1$ тогда и только тогда, когда $a[i]\u003ex$ (и нас интересует максимальное $i$ такое, что $f(i)=0$, то есть в конце алгоритма нужно вернуть $l$, а не $r$ ).\n5.4 Двоичный поиск по функции с вещественным аргументом Двоичный поиск можно делать и по функции, принимающей значения во всех вещественных точках, а не только в целых. Решим для примера такую задачу: дана монотонно возрастающая непрерывная функция $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, и точки $L, R$ такие, что $f(L)\u003c0$, $f(R) \\geqslant 0$. Найдём точку $x$ такую, что $f(x)=0$. (R)\nЕсли такие $L, R$ не даны, но известно, что они существуют, то их можно найти алгоритмом экспоненциального поиска: начнём с $R=1$ и будем удваивать $R$, пока $f(R)\u003c0$. Аналогично, начнём с $L=-1$ и будем удваивать $L$, пока $f(L) \\geqslant 0$. При этом мы затратим $O\\left(\\log \\left|R^{\\prime}\\right|+\\log \\left|L^{\\prime}\\right|\\right)$ действий, где $R^{\\prime} \\geqslant 0, L^{\\prime} \\leqslant 0-$ любые такие, что $f\\left(L^{\\prime}\\right)\u003c0, f\\left(R^{\\prime}\\right) \\geqslant 0$.\nПоскольку $f$ принимает вещественные значения, мы не всегда можем найти значение такого $x$ абсолютно точно. Тем не менее, мы можем найти такое $x$ с погрешностью $\\varepsilon$ : найдём такое $x$, что существует $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0,\\left|x-x^{\\prime}\\right|\u003c\\varepsilon$.\nАлгоритм практически не меняется: начинаем с границ $l=L, r=R$, на каждом шаге сдвигаем одну из границ в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. Единственное отличие: мы завершаем алгоритм в тот момент, когда $r-l\u003c\\varepsilon$. Поскольку в этот момент всё ещё $f(l)\u003c0, f(r) \\geqslant 0$, а функция $f$ непрерывна, на отрезке $[l, r]$ найдётся $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0$. Поскольку длина отрезка меньше $\\varepsilon$, любая из границ $l, r$ подойдёт в качестве ответа. Время работы алгоритма (при условии, что значение $f$ вычисляется за $O(1)$ ) $O\\left(\\log \\left(\\frac{R-L}{\\varepsilon}\\right)\\right)$.\nЕсли вычисления проводятся при помощи стандартных вещественнозначных типов данных (например, double в C++), то из-за накопления погрешности при пересчёте границ условие $r-l\u003c\\varepsilon$ может никогда не выполниться. Чтобы избежать этого, вместо while напишем цикл, делающий столько итераций, сколько нужно, чтобы условие точно выполнилось. Пример реализации:\nfindRoot(L, R, eps): l = L, r = R k = log((r - l) / eps) # через k итераций условие r - l \u003c eps выполнится for i = 0..(k - 1): m = (l + r) / 2 if f(m) \u003c 0: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r 5.5 Метод двух указателей Иногда много двоичных поисков на одних и тех же данных можно соптимизировать с помощью так называемого метода двух указателей. Сделаем это на примере следующей задачи: дан массив $a$ длины $n$, состоящий из неотрицательных целых чисел. Нужно найти максимальный по длине подотрезок массива, сумма на котором не превосходит $M$.\nУдобнее работать не с отрезками, а с полуинтервалами: паре $l, r$ будем сопоставлять полуинтервал $a[l, r)$, то есть элементы массива с индексами $l, l+1, \\ldots, r-1$ (это практически всегда позволяет писать меньше плюс-минус единиц в индексах массивов, что делает код короче). Самое простое решение - перебрать все возможные полуинтервалы:\nmaxLen = 0, ansL = 0, ansR = 0 # здесь будем хранить ответ for l = 0..(n - 1): # перебираем левую границу sum = 0 # будем считать сумму на текущем полуинтервале for r = (l + 1)..n: sum += a[r - 1] if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r Получилось решение за $\\Theta\\left(n^{2}\\right)$. Заметим, что при фиксированной левой границе сумма на полуинтервале не уменьшается при увеличении правой границы. Значит сколькото первых правых границ подойдут, а все последующие уже не подойдут. При этом нас интересует максимальная подходящая правая граница. Её можно найти двоичным поиском, таким образом соптимизировав решение до $\\Theta(n \\log n)$ :\nДля того, чтобы быстро находить сумму на произвольном полуинтервале, заранее предподсчитаем префиксные суммы - массив $p$ такой, что $p[i]=\\sum_{j=0}^{i-1} a_{j}$. Тогда сумма на полуинтервале $a[l, r)$ равна $\\sum_{i=l}^{r-1} a_{i}=p[r]-p[l]$.\nmaxLen = 0, ansL=0, ansR = 0 int p[n + 1] # предподсчитываем префиксные суммы p[0] = 0 for i = 1..n: p[i] = p[i - 1] + a[i - 1] for l = 0..(n - 1): sl = l, sr = n + 1 # границы поиска, sum a[l..sl) \u003c= M, sum a[l..sr) \u003e M while sr - sl \u003e 1: sm = (sl + sr) / 2 if p[sm] - p[l] \u003c= M: sl = sm else: sr = sm if sl - l \u003e maxLen: # sl - максимальная подходящая правая граница maxLen = sl - l, ansL = l, ansR = sl Перейдём теперь собственно к методу двух указателей. Пусть $r$ - максимальная подходящая правая граница для левой границы $l-1$. Тогда $M \\geqslant \\sum_{i=l-1}^{r-1} a_{i} \\geqslant \\sum_{i=l}^{r-1} a_{i}$, то есть граница $r$ подойдёт и для $l$. Будем вместо двоичного поиска искать границу $r$ так же, как и в самой первой версии алгоритма: просто перебирая все границы подряд. Но начнём перебор не с $l+1$, а с правой границы, найденной на предыдущем шаге. Поскольку теперь правая граница только увеличивается в ходе алгоритма, суммарно за всё время выполнения алгоритма она сдвинется не более, чем $n$ раз. Тогда общее время работы алгоритма $-\\Theta(n)$.\nmaxLen = 0, ansL=0, ansR = 0 r = 0, sum = 0 for l = 0..(n - 1): # перебираем левую границу while r \u003c n and sum + a[r] \u003c= M: sum += a[r] r += 1 if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r sum -= a[l]",
    "description": "5.1 Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.",
    "tags": [],
    "title": "5. Двоичный поиск",
    "uri": "/basics/binary_search/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Категории",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Теги",
    "uri": "/tags/index.html"
  }
]
