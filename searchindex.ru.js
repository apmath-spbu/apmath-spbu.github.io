var relearn_searchindex = [
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.\nБаза. Утверждение верно для $n=0,1$.\nПереход. Пусть $n\u003e1$, тогда $F_{n}=F_{n-1}+F_{n-2} \\leqslant 2^{n-1}+2^{n-2}\u003c2^{n}$.\nС другой стороны, $F_{n} \\geqslant 2 \\cdot F_{n-2} \\geqslant 2 \\cdot 2^{\\lfloor(n-2) / 2\\rfloor}=2^{\\lfloor n / 2\\rfloor}$.\nЭкспоненциальный алгоритм Следующий рекурсивный алгоритм вычисляет $n$-ое число Фибоначчи, точно следуя определению:\n1 2 3 4 fib(n): if n \u003c= 1: return 1 return fib(n - 1) + fib(n - 2) Каково время работы этого алгоритма? Оценим $T(n)$ - суммарное количество вызовов fib , происходящих при выполнении $\\mathrm{fib}(\\mathrm{n})$ : если $n \\leqslant 1$, то $T(n)=1$; иначе\n$$ T(n)=1+T(n-1)+T(n-2) . $$Несложно доказать по индукции, что $F_{n} \\leqslant T(n)\u003c2 F_{n}$. В каждом вызове fib совершается ограниченное число (скажем, не больше пяти) операций, поэтому время работы алгоритма примерно пропорционально $F_{n}$ (чуть позже у нас появится формальное определение этого “примерно”).\nИз леммы 1.1.1 следует, например, что, $F_{300} \\geqslant 2^{150}\u003e10^{45}$. На компьютере, выполняющем $10^{9}$ операций в секунду, fib(300) будет выполняться больше $10^{36}$ секунд.\nПолиномиальный алгоритм Посмотрим на дерево рекурсивных вызовов алгоритма fib. Видно, что алгоритм много раз вычисляет одно и то же. Давайте сохранять результаты промежуточных вычислений в массив: 1 2 3 4 5 6 7 8 fibFast(n): if n \u003c= 1: return 1 int f[n + 1] # создаём массив с индексами 0..n f[0] = f[1] = 1 for i = 2..n: f[i] = f[i - 1] + f[i - 2] return f[n] На каждой итерации цикла совершается одно сложение, всего итераций примерно $n$, поэтому количество сложений, выполняемых в ходе нового алгоритма, примерно пропорционально $n$. Есть ещё одна тонкость - из леммы 1.1.1 следует, что двоичная запись $F_{n}$ имеет длину порядка $n$. Чуть позже мы увидим, что сложение двух $n$-битовых чисел требует порядка $n$ элементарных операций, значит общее время работы нового алгоритма примерно пропорционально $n^{2}$. С помощью fibFast можно уже за разумное время вычислить не только $F_{300}$, но и $F_{100000}$.\n1.2 О-символика Хорошо себя зарекомендовал и стал общепринятым следующий подход - оценивать время работы алгоритма некоторой функцией от входных параметров, при этом пренебрегая ограниченными множителями. Это позволяет эффективно сравнивать алгоритмы между собой, при этом не нужно заниматься точным подсчётом количества элементарных операций. Примерно так мы и рассуждали об алгоритмах вычисления чисел Фибоначчи. Введём несколько обозначений, которые помогут проводить подобные рассуждения более кратко и точно.\n✍️ Время работы алгоритма - это, конечно, всегда неотрицательная функция. Тем не менее, мы даём следующие определения для функций, принимающих произвольные вещественные значения, поскольку такие функции часто возникают в процессе рассуждений.\nОпределение 1.2.1 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=O(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$. $f=\\Omega(g)$, если существуют такие $C\u003e0, N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$. $f=\\Theta(g)$, если существуют такие $C_{1}\u003e0, C_{2}\u003e0, N\u003e0$, что для любого $n\u003eN$ : $C_{1} \\cdot|g(n)| \\leqslant|f(n)| \\leqslant C_{2} \\cdot|g(n)|$. ✍️ Запись $f=O(g)$ можно понимать как \" $|f| \\leqslant|g|$ с точностью до константы\". Аналогично, $\\Omega(\\cdot)$ можно считать аналогом $\\geqslant$, а $\\Theta(\\cdot)$ - аналогом $=$. Ещё один способ понимать запись $f=O(g)$ - отношение $\\frac{|f(n)|}{|g(n)|}$ ограничено сверху некоторой константой.\nМногие естественные свойства операторов сравнения $\\leqslant,=, \\geqslant$ выполняются и для их асимптотических аналогов. Сформулируем некоторые из этих свойств:\nСвойства 1.2.1 $f=\\Theta(g)$ тогда и только тогда, когда $f=O(g)$ и $f=\\Omega(g)$. $f=O(g)$ тогда и только тогда, когда $g=\\Omega(f)$. $f=\\Theta(g)$ тогда и только тогда, когда $g=\\Theta(f)$. Доказательство Докажем для примера п. 2.\nПусть $C\u003e0$, тогда $|f(n)| \\leqslant C \\cdot|g(n)|$ равносильно $|g(n)| \\geqslant \\frac{1}{C} \\cdot|f(n)|$.\nТаким образом, $f=O(g)$ с константой $C$ тогда и только тогда, когда $g=\\Omega(f)$ с константой $\\frac{1}{C}$.\nСуществуют также асимптотические аналоги $\u003c$ и $\u003e: o(\\cdot)$ и $\\omega(\\cdot)$. Определение 1.2.2 Рассмотрим функции $f, g: \\mathbb{N} \\rightarrow \\mathbb{R}$.\n$f=o(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|g(n)|$.\n$f=\\omega(g)$, если для любого $C\u003e0$ найдётся такое $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\geqslant C \\cdot|g(n)|$.\n✍️ Запись $f=o(g)$ можно понимать как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к нулю с увеличением $n$ “, а запись $f=\\omega(g)$ - как “отношение $\\frac{|f(n)|}{|g(n)|}$ стремится к бесконечности с увеличением $n$ “.\nСвойства 1.2.2 $f=o(g)$ тогда и только тогда, когда $g=\\omega(f)$. Если $f=O(g)$ и $g=O(h)$, то $f=O(h)$. То же верно для $\\Omega, \\Theta, o, \\omega$. Если $f=O(g), g=o(h)$, то $f=o(h)$. Если $f=\\Omega(g), g=\\omega(h)$, то $f=\\omega(h)$. Если $f=O(h)$ и $g=O(h)$, то $f+g=O(h)$. То же верно для $o$. Если $f, g \\geqslant 0$, то то же верно и для $\\Omega, \\Theta, \\omega$. Если $f=o(g)$, то $f+g=\\Theta(g)$. Если $f, g \\geqslant 0, f=O(g)$, то $f+g=\\Theta(g)$. Для любого $C \\neq 0$ верно $C \\cdot f=\\Theta(f)$. ✍️ Заметим, что у последних трёх свойств нет аналогов для обычных операторов сравнения.\nДоказательство Докажем для примера п. 3 (вариант с $O$ и $о$).\nНужно показать, что для любого $C\u003e0$ найдётся $N\u003e0$, что для любого $n\u003eN$ : $|f(n)| \\leqslant C \\cdot|h(n)|$. Зафиксируем $C\u003e0$.\n$f=O(g)$, поэтому найдутся $C_{1}\u003e0, N_{1}\u003e0$, что для любого $n\u003eN_{1}:|f(n)| \\leqslant C_{1} \\cdot|g(n)|$. $g=o(h)$, поэтому найдётся такое $N_{2}\u003e0$, что для любого $n\u003eN_{2}:|g(n)| \\leqslant \\frac{C}{C_{1}} \\cdot|h(n)|$.\nТогда для любого $n\u003e\\max \\left(N_{1}, N_{2}\\right):|f(n)| \\leqslant C_{1} \\cdot|g(n)| \\leqslant C_{1} \\cdot \\frac{C}{C_{1}} \\cdot|h(n)|=C \\cdot|h(n)|$.\nВернемся к алгоритмам вычисления чисел Фибоначчи. Пользуясь новыми обозначениями, мы показали, что время работы fibFast(n) можно оценить как $O\\left(n^{2}\\right)$. При этом время работы $\\mathrm{fib}(\\mathrm{n})$ можно оценить (сверху) как $O\\left(2^{n} \\cdot n\\right)$, и (снизу) как $\\Omega\\left(2^{\\lfloor n / 2\\rfloor}\\right)$.\n1.3 Многочлены, экспоненты и логарифмы Очень часто время работы алгоритма удаётся оценить функцией, являющейся комбинацией каких-то из трёх базовых типов: многочленов, экспонент и логарифмов. Так, время работы fibFast мы оценили многочленом $n^{2}$, а время работы fib - произведением экспоненты на многочлен: $2^{n} \\cdot n$. В связи с этим полезно изучить асимптотические свойства этих функций и научиться сравнивать их между собой.\nЛемма 1.3.1 Для любых $l\u003ek$ верно $n^{k}=o\\left(n^{l}\\right)$. Доказательство Для любого $C\u003e0$, при $n \\geqslant\\left(\\frac{1}{C}\\right)^{\\frac{1}{l-k}}$ верно\n$$ n^{k} \\leqslant C \\cdot \\frac{1}{C} \\cdot n^{k} \\leqslant C \\cdot n^{l-k} \\cdot n^{k}=C \\cdot n^{l} $$ Определение 1.3.1 Многочлен - это функция, которую можно записать в виде\n$$ f(n)=a_{0}+a_{1} n+a_{2} n^{2}+\\cdots+a_{d} n^{d} $$для некоторого $d \\geqslant 0$, так, что $a_{d} \\neq 0$. Это $d$ называют степенью многочлена и обозначают как $\\operatorname{deg}(f)$.\nСледствие 1.3.2 Пусть $f(n)$ - многочлен степени $d$. Тогда $f(n)=\\Theta\\left(n^{d}\\right)$. Доказательство Пусть $f(n) = a_{0} + a_{1} n + a_{2} n^{2} + \\cdots + a_{d} n^{d}$. Из леммы 1.3.1 и п. 6 предложения 1.2.2 следует, что $a_{j} n^{j} = o\\left(n^{d}\\right)$ для любого $0 \\leqslant j",
    "description": "Вычисление чисел Фибоначчи Последовательность чисел Фибоначчи\n$$ 1,1,2,3,5,8,13,21,34,55,89, \\ldots $$определяется следующим образом:\nОпределение 1.1.1 $F_{n}$ - $n$-ое число Фибоначчи, где $F_{0}=F_{1}=1, F_{n}=F_{n-1}+F_{n-2}, n\u003e1 .$\nЧисла Фибоначчи растут экспоненциально быстро:\nЛемма 1.1.1 $2^{\\lfloor n / 2\\rfloor} \\leqslant F_{n} \\leqslant 2^{n}$. Доказательство Докажем утверждение по индукции.",
    "tags": [],
    "title": "1. Анализ сложности алгоритмов",
    "uri": "/basics/complexity/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\n✍️ В произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.\n2.1 Сложение Используем всем знакомый со школы способ сложения чисел в столбик:\n1 2 3 4 5 6 7 add(a, b, n): # a и b - двоичные записи чисел int c[n + 1] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: c[i] = a[i] + b[i] if c[i] \u003e= 2: c[i + 1] += 1, c[i] -= 2 return c Время работы алгоритма - $O(n)$, существенно быстрее нельзя, потому что столько времени занимает уже считывание входных данных или вывод ответа.\n✍️ Аналогичный алгоритм можно написать для чисел, записанных в $b$-ичной системе счисления. Если сумма трёх $b$-ичных чисел помещается в 32(64)-битный тип данных, то алгоритм всё ещё будет корректен. При этом $n$-битное число будет иметь примерно $\\frac{n}{\\log b}$ цифр в $b$-ичной записи, то есть алгоритм будет работать за $O\\left(\\frac{n}{\\log b}\\right)=O(n)$, так как $\\frac{1}{\\log b}$ - это константа. Тем не менее, это может дать ускорение в несколько десятков раз, что безусловно бывает полезно на практике.\n2.2 Умножение Вспомним теперь и школьное умножение чисел в столбик (заметим лишь, что ответ имеет длину не больше $2 n$, поскольку $\\left.\\left(2^{n}-1\\right) \\cdot\\left(2^{n}-1\\right)\u003c2^{2 n}\\right)$ :\n1 2 3 4 5 6 7 8 9 10 multiply(a, b, n): # a и b - двоичные записи чисел int c[2 * n] = 0 # создаём и заполняем нулями массив, куда запишем ответ for i = 0..n - 1: for j = 0..n - 1: c[i + j] += a[i] * b[j] for i = 0..2 * n - 2: if c[i] \u003e= 2: c[i + 1] += c[i] / 2 c[i] %= 2 return c Из-за двух вложенных циклов время работы этого алгоритма - уже $O\\left(n^{2}\\right)$. Приведём альтернативный рекурсивный алгоритм умножения двух чисел, пользующийся следующим правилом:\n$$ a \\cdot b= \\begin{cases}2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { если } b \\text { чётно, } \\\\ a+2 \\cdot\\left(a \\cdot\\left\\lfloor\\frac{b}{2}\\right\\rfloor\\right), \u0026 \\text { иначе. }\\end{cases} $$ 1 2 3 4 5 6 7 8 multiply(a, b): # a и b - двоичные записи чисел if b == 0: return 0 c = multiply(a, b / 2) # деление нацело if b % 2 == 0: return 2 * c else: return 2 * c + a В этой схематичной записи под делением на два имеется ввиду битовый сдвиг вправо (то есть взятие двоичной записи без младшего бита), под умножением на два - битовый сдвиг влево (добавление нуля в начало битовой записи). Остаток по модулю два - это младший бит числа.\nАлгоритм произведёт $O(n)$ рекурсивных вызовов, поскольку при каждом вызове длина битовой записи $b$ уменьшается на один. В каждом вызове функции происходят битовый сдвиг влево, битовый сдвиг вправо, и, возможно сложение - всего $O(n)$ элементарных операций. Таким образом, общее время работы снова $O\\left(n^{2}\\right)$.\nЗаметим, что если длины битовых записей $a$ и $b$ равны $n$ и $m$, то время работы обоих алгоритмов можно оценить как $O(n m)$.\n2.3 Деление Пусть теперь мы хотим поделить $a$ на $b$, то есть найти такие $q, r$, что $a=q b+r$ и $0 \\leqslant r \u003c b$. Здесь работает похожая идея: обозначим за $q^{\\prime}, r^{\\prime}$ результат деления $\\left\\lfloor\\frac{a}{2}\\right\\rfloor$ на $b$, тогда:\n$$ (q, r)= \\begin{cases}\\left(2 \\cdot q^{\\prime}+\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor, 2 \\cdot r^{\\prime}-\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor \\cdot b\\right), \u0026 \\text { если } a \\text { чётно, } \\\\ \\left(2 \\cdot q^{\\prime}+\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor, 2 \\cdot r^{\\prime}+1-\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor \\cdot b\\right), \u0026 \\text { иначе. }\\end{cases} $$При этом $\\left\\lfloor\\frac{2 \\cdot r^{\\prime}}{b}\\right\\rfloor,\\left\\lfloor\\frac{2 \\cdot r^{\\prime}+1}{b}\\right\\rfloor \\leqslant 1$. Получаем следующий рекурсивный алгоритм:\n1 2 3 4 5 6 7 8 9 10 divide(a, b): # a и b - двоичные записй чисел if a == 0: return 0, 0 # q = r = 0 q, r = divide(a / 2, b) # деление нацело q = 2 * q, r = 2 * r if a % 2 == 1: r += 1 if r \u003e= b: q += 1, r -= b return q, r Снова имеем $O(n)$ рекурсивных вызовов, в каждом из которых происходит константное число битовых сдвигов и сложений (вычитаний), поэтому время работы снова оценивается как $O\\left(n^{2}\\right)$.\nАльтернативный способ - школьное деление в столбик:\n1 2 3 4 5 6 7 8 9 10 divide(a, b): # a и b - двоичные записи чисел n = len(a), m = len(b) q=0 for i = (n - m)..0: # умножение на 2 ** k - битовый сдвиг числа на k влево c = b * 2 ** i if a \u003e= c: a = a - c q = q + 2 **i return q, a Время работы, как и в предыдущем случае, оценивается как $O\\left(n^{2}\\right)$. Есть и более точная в некоторых случаях оценка: если длины битовых записей $a$ и $b$ равны $n$ и $m$, то количество итераций цикла не превосходит $n-m+1$, поэтому получаем оценку $O(n(n-m+1))$.\n2.4 Алгоритм Карацубы Можно ли перемножать числа быстрее? Оказывается, что да! Следующий алгоритм был придуман советским математиком Анатолием Карацубой в 1962 году.\nБудем для удобства считать, что длина битовой записи чисел $n$ - степень двойки (если это не так, добавим ведущих нулей, при этом длина чисел увеличится не более чем вдвое, что не повлияет на асимптотическую оценку).\nРазобьём каждое из чисел на две равных половины: $a=2^{\\frac{n}{2}} a_{l}+a_{r}, b=2^{\\frac{n}{2}} b_{l}+b_{r}$. Заметим, что\n$$ a b=\\left(2^{\\frac{n}{2}} a_{l}+a_{r}\\right) \\cdot\\left(2^{\\frac{n}{2}} b_{l}+b_{r}\\right)=2^{n} a_{l} b_{l}+2^{\\frac{n}{2}}\\left(a_{l} b_{r}+a_{r} b_{l}\\right)+a_{r} b_{r} $$Напишем рекурсивный алгоритм, пользующийся этим равенством: найдём четыре произведения вдвое более коротких чисел ( $a_{l} b_{l}, a_{l} b_{r}, a_{r} b_{l}, a_{r} b_{r}$ ) рекурсивно, после чего вычислим с их помощью $a b$. Для этого нам понадобится сделать константное число сложений и битовых сдвигов (и то, и другое делается за $\\Theta(n)$ ). Обозначим за $T(n)$ время работы алгоритма на $n$-битовых числах, тогда мы получаем следующее рекуррентное соотношение:\n$$ \\begin{equation*} T(n)=4 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n) \\tag{2.1} \\end{equation*} $$Совсем скоро мы докажем теорему, из которой следует, что $T(n)=\\Theta\\left(n^{2}\\right)$. Пока мы не получили никакого выигрыша по сравнению с предыдущими алгоритмами умножения. Но оказывается, что этот алгоритм можно ускорить, заметив, что\n$$ a_{l} b_{r}+a_{r} b_{l}=\\left(a_{l}+b_{l}\\right) \\cdot\\left(a_{r}+b_{r}\\right)-a_{l} b_{l}-a_{r} b_{r} . $$Значит, достаточно рекурсивно вычислить три произведения, и время работы алгоритма будет удовлетворять уже\n$$ \\begin{equation*} T(n)=3 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n) \\tag{2.2} \\end{equation*} $$Время работы такого алгоритма по той же теореме можно оценить уже как $\\Theta\\left(n^{\\log _{2} 3}\\right)=$ $\\Theta\\left(n^{1.585 \\ldots}\\right)$.\n✍️ Строго говоря, длина чисел $a_{l}+b_{l}, a_{r}+b_{r}$ может оказаться равна $\\frac{n}{2}+1$. Чтобы честно получить рекуррентное соотношение 2.2, можно за линейное время свести умножение $\\left(\\frac{n}{2}+1\\right)$-битных чисел к умножению $\\frac{n}{2}$-битных. На самом деле можно показать, что соотношение $T(n)=3 \\cdot T\\left(\\frac{n}{2}+1\\right)+\\Theta(n)$ даёт такую же асимптотическую оценку.\n1 2 3 4 5 6 7 8 9 10 11 multiply(a, b): # a и b - двоичные записи чисел n = max(len(a), len(b)) if n == 1: return [a[0] * b[0]] # число из одного бита a --\u003e al, ar # делим число а на две равные части b --\u003e bl, br # делим число b на две равные части x = multiply(al, bl) y = multiply(ar, br) z = multiply(al + ar, bl + br) # умножение на 2 ** k - битовый сдвиг числа на k влево return x * 2 ** n + (z - x - y) * 2 ** (n / 2) + y На практике, дойдя в рекурсии до $16(32)$-битных чисел, уже стоит воспользоваться стандартной операцией умножения.\nСуществуют и более быстрые методы умножения чисел (например, метод, основанный на быстром преобразовании Фурье), но о них мы поговорим позже.\nПример Рассмотрим произведение $1234 * 4321$.\nЗадача делится на 3 подзадачи:\n$a_{l} b_{l} = 12 \\cdot 43$ $a_{r} b_{r} = 34 \\cdot 21$ $e = \\left(a_{l}+b_{l}\\right) \\cdot\\left(a_{r}+b_{r}\\right)-a_{l} b_{l}-a_{r} b_{r} = \\left(12 + 34\\right) \\cdot \\left(43 + 21\\right) - a_{l} b_{l} - a_{r} b_{r}$ Первая подзадача ($a_{l} b_{l} = 12 \\cdot 43$) в свою очередь также резделяется на 3 подпроблемы:\n$a_{l}^\\prime b_{l}^\\prime = 1 \\cdot 4 = 4$ $a_{r}^\\prime b_{r}^\\prime = 2 \\cdot 3 = 6$ $\\left(a_{l}^\\prime+b_{l}^\\prime\\right) \\cdot\\left(a_{r}^\\prime+b_{r}^\\prime\\right)-a_{l}^\\prime b_{l}^\\prime-a_{r} b_{r}^\\prime = \\left(1 + 2\\right) \\cdot \\left(4 + 3\\right) - 4 - 6 = 11$ Ответ: $4 \\cdot 10^2 + 11 \\cdot 10 + 6 = 516$ Вторая подзадача ($a_{r} b_{r} = 34 \\cdot 21$):\n$a_{l}^{\\prime\\prime} b_{l}^{\\prime\\prime} = 3 \\cdot 2 = 6$ $a_{r}^{\\prime\\prime} b_{r}^{\\prime\\prime} = 4 \\cdot 1 = 4$ $\\left(a_{l}^{\\prime\\prime}+b_{l}^{\\prime\\prime}\\right) \\cdot\\left(a_{r}^{\\prime\\prime}+b_{r}^{\\prime\\prime}\\right)-a_{l}^{\\prime\\prime} b_{l}^{\\prime\\prime}-a_{r} b_{r}^{\\prime\\prime} = \\left(3 + 4\\right) \\cdot \\left(2 + 1\\right) - 6 - 4 = 11$ Ответ: $6 \\cdot 10^2 + 11 \\cdot 10 + 4 = 714$ Третья подзадача ($e = 46 \\cdot 64 - 516 - 714$). Аналогично вычисляем $46 \\cdot 64$:\n$4 \\cdot 6 = 24$ $6 \\cdot 4 = 24$ $\\left(4 + 6 \\right) \\cdot (6 + 4) - 24 - 24 = 52$ Ответ: $24 \\cdot 10^2 + 52 \\cdot 10 + 24 - 516 - 714 = 1714$ Финальный ответ: $1234 \\cdot 4321 = 516 \\cdot 10^4 + 1714 \\cdot 10^2 + 714 = 5,332,114$",
    "description": "Современные компьютеры умеют за одну элементарную операцию складывать два 32(или 64)-битных числа. Часто (например, в криптографии) приходится работать с куда более длинными числами. Поговорим о том, как проводить с ними элементарные арифметические операции.\nДля удобства будем считать, что на вход алгоритмам даются натуральные числа в двоичной записи. Мы будем работать с числами, имеющими двоичную запись длины $n$ (возможно, с ведущими нулями).\n✍️ В произвольном случае можно применить соответствующий алгоритм для $n$ равного максимуму из длин чисел, а в конце определить длину записи результата применения операции (удалить ведущие нули), что потребует $O(n)$ операций и не повлияет на оценку сложности алгоритма.",
    "tags": [],
    "title": "2. Элементарная арифметика",
    "uri": "/basics/elementary_arithmetic/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nОсновная теорема о рекуррентных соотношениях Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда\n$$T(n)= \\begin{cases}\\Theta\\left(n^{c}\\right), \u0026 \\text { если } c \u003e \\log_{b} a \\\\ \\Theta\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ \\Theta\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log_{b} a\\end{cases}$$ Доказательство Пусть мы уже доказали утверждение теоремы для чисел вида $n=b^{k}$. Рассмотрим такую функцию $k(n)$, что для любого $n$ верно $b^{k(n)-1} \u003c n \\leqslant b^{k(n)}$. При уменьшении $n$ количество веток в дереве рекурсии и их размер могли только уменьшиться, поэтому\n$$ T(n) = O \\left(T\\left(b^{k(n)}\\right)\\right)= \\begin{cases}O\\left(b^{k(n) c}\\right)=O\\left(n^{c}\\right), \u0026 \\text { если } c\u003e\\log _{b} a \\\\ O\\left(b^{k(n) c} \\log \\left(b^{k(n)}\\right)\\right)=O\\left(n^{c} \\log n\\right), \u0026 \\text { если } c=\\log _{b} a \\\\ O\\left(b^{k(n) \\log _{b} a}\\right)=O\\left(n^{\\log _{b} a}\\right), \u0026 \\text { если } c \u003c \\log _{b} a\\end{cases} $$(мы пользуемся тем, что $b^{k(n)} = b \\cdot b^{k(n)-1} \u003c b n = O(n)$ ). Доказательство же точной асимптотической оценки $(\\Theta)$ для произвольного $n$ требует ещё некоторого количества технических выкладок, которые мы опустим. Желающие могут прочитать их в главе 4.6 в Кормене1.\nТеперь считаем, что $n=b^{k}$ для некоторого $k$. Раскроем рекуррентность:\n$$ \\begin{gathered} T(n)=a \\cdot T\\left(\\frac{n}{b}\\right)+\\Theta\\left(n^{c}\\right)=\\Theta\\left(n^{c}+a \\cdot\\left(\\frac{n}{b}\\right)^{c}\\right)+a^{2} T\\left(\\frac{n}{b^{2}}\\right)= \\\\ =\\cdots=\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k-1}\\right)\\right)+a^{k} T(1)= \\\\ =\\Theta\\left(n^{c} \\cdot\\left(1+\\frac{a}{b^{c}}+\\left(\\frac{a}{b^{c}}\\right)^{2}+\\cdots+\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)\\right) \\end{gathered} $$(последнее слагаемое $a^{k} T(1)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(n^{c} \\cdot\\left(\\frac{a}{b^{c}}\\right)^{k}\\right)$, так как $\\left.T(1)=\\Theta(1), n=b^{k}\\right)$. Получаем геометрическую прогрессию со знаменателем $q=\\frac{a}{b^{c}}$.\nЕсли $c\u003e\\log _{b} a$, то $q \u003c 1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot \\frac{1-q^{k+1}}{1-q}\\right)=\\Theta\\left(n^{c} \\cdot \\frac{1}{1-q}\\right)=\\Theta\\left(n^{c}\\right) $$Если $c=\\log _{b} a$, то $q=1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot(k+1)\\right)=\\Theta\\left(n^{c} \\log _{b} n\\right)=\\Theta\\left(n^{c} \\log n\\right) $$Наконец, если $c \u003c \\log _{b} a$, то $q\u003e1$, тогда\n$$ T(n)=\\Theta\\left(n^{c} \\cdot\\left(q^{k}+\\frac{q^{k}-1}{q-1}\\right)\\right)=\\Theta\\left(n^{c} q^{k}\\right)=\\Theta\\left(a^{k}\\right)=\\Theta\\left(a^{\\log _{b} n}\\right)=\\Theta\\left(n^{\\log _{b} a}\\right) $$ Introduction to Algorithms (Fourth Edition) ↩︎\nВ алгоритме Карацубы $a=3, b=2, c=1$, это соответствует третьему случаю теоремы, который и даёт $T(n)=\\Theta\\left(n^{\\log _{2} 3}\\right)$.\nОцените следующие алгоритмы Пример 1 1 2 3 4 5 6 7 8 9 10 11 12 void foo(int* array, int n) { int sum = 0; int product = 1; for (int i = 0; i \u003c n; i++) { sum += array[i]; } for (int i = 0; i \u003c n; i++) { product += array[i]; } std::cout \u003c\u003c sum \u003c\u003c \", \" \u003c\u003c product; } Пример 2 1 2 3 4 5 6 7 void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } Пример 3 1 2 3 4 5 6 7 void printPairs(int* arr, int n) { for (int i = 0; i \u003c n; i++) { for (int j = i + 1; j \u003c n; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } Пример 4 1 2 3 4 5 6 7 8 9 void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { if (arrA[i] \u003c arrB[j]) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } } Пример 5 1 2 3 4 5 6 7 8 9 void printUnorderedPairs(int* arrA, int n, int* arrB, int m) { for (int i = 0; i \u003c n; i++) { for (int j = 0; j \u003c m; j++) { for (int j = 0; j \u003c 100000; j++) { std::cout \u003c\u003c arr[i] \u003c\u003c \", \" \u003c\u003c arr[j] \u003c\u003c std::endl; } } } } Пример 6 1 2 3 4 5 6 7 8 void reverse(int* arr, int n) { for (int i = 0; i \u003c n / 2; i++) { int other = n - i - 1; int temp = arr[i]; arr[i] = arr[other]; arr[other] = temp } } Пример 7 1 2 3 4 5 6 7 8 9 bool isPrime(int n) { for (int x = 2; x * x \u003c= n; x++) { if (n % x == 0) { return false; } } return true; } Пример 8 1 2 3 4 5 6 7 8 9 int factorial(int n) { if (n \u003c 0) { return -1; } else if (n == 0) { return 1; } else { return n * factorial(n - 1); } } Пример 9 1 2 3 4 5 int fib(int n) { if (n \u003c= 0) return 0; else if (n == 1) return 1; else fib(n - 1) + fib(n - 2); }",
    "description": "Основная теорема о рекуррентных соотношениях Алгоритм Карацубы - пример применения метода “разделяй и властвуй” (“divide-and-conquer”), с которым мы ещё не раз встретимся.\nИдея этого метода состоит в том, чтобы свести решение задачи к решению нескольких подзадач в несколько раз меньшего размера, после чего восстановить решение исходной задачи с помощью решений подзадач. Сейчас мы докажем достаточно общую теорему, применимую к оценке многих алгоритмов, использующих метод “разделяй и властвуй”.\nОсновная теорема о рекуррентных соотношениях Пусть $T(n)=a \\cdot T\\left(\\left\\lceil\\frac{n}{b}\\right\\rceil\\right)+\\Theta\\left(n^{c}\\right)$, где $a\u003e0, b\u003e1, c \\geqslant 0$. Тогда",
    "tags": [],
    "title": "3. Рекурентные соотношения",
    "uri": "/basics/recurrence_relation/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.\nПоиск элемента в массиве по значению также имеет сложность $\\Theta(n)$, так как нужно проверить все элементы массива. Если массив специально упорядочен (например, элементы массива расположены в возрастающем порядке), то поиск можно делать быстрее (скоро мы изучим алгоритм двоичного поиска).\nНемного о работе с массивом в C++:\n1 2 3 4 5 6 7 int a[n]; // объявить массив длины n, нумерация ячеек от 0 до n-1 a[i]; // обращение к i-му элементу массива a[i] = x; // присвоить x в i-ю ячейку массива int b[n][m]; // объявить двумерный массив, нумерация ячеек двумя индексами, // от 0 до n-1 и от 0 до m-1 b[i][j]; // обращение к j-му элементу i-й строки массива b[i][j] = y; // присвоение Связный список Связный список (linked list) состоит из узлов, каждый из которых содержит данные и ссылки на соседние узлы. В двусвязном списке поддерживаются ссылки на следующий и предыдущий узел, в односвязном списке - только на следующий. Также поддерживаются ссылки на начало и конец списка (их часто называют головой и хвостом). Для того, чтобы посетить все узлы, можно начать с головы и переходить по ссылке на следующий узел, пока он существует.\nПреимущество списка перед массивом - возможность вставлять и удалять элементы за $O(1)$.\nНедостаток списка - невозможность быстрого доступа к произвольному элементу. Так, доступ к $i$-му элементу можно получить, лишь $i$ раз пройдя по ссылке вперёд, начиная из головы списка (то есть за $\\Theta(i)$ ).\nУдобно делать голову и хвост фиктивными элементами и не хранить в них никаких данных, тогда функции вставки и удаления элементов пишутся проще.\nПримерная реализация двусвязного списка на C++:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 struct Node { // один узел списка Node *next, *prev; // указатели на следующий и предыдущий узлы int x; // данные, хранящиеся в узле }; struct List { Node *head, *tail; // указатели на начало и конец списка List() { // инициализируем пустой список - создаём фиктивные head и tail // и связываем их друг с другом head = new Node(); tail = new Node(); head-\u003enext = tail; tail-\u003eprev = head; } void pushBack(int x) { // вставить x в конец списка Node *v = new Node(); v-\u003ex = x, v-\u003eprev = tail-\u003eprev, v-\u003enext = tail; v-\u003eprev-\u003enext = v, tail-\u003eprev = v; } void insert(Node *v, int x) { // вставить x после v Nove *w = new Node(); w-\u003ex = x, w-\u003enext = v-\u003enext, w-\u003eprev = v; v-\u003enext = w, w-\u003enext-\u003eprev = w; } void erase(Node *v) { // удалить узел v v-\u003eprev-\u003enext = v-\u003enext; v-\u003enext-\u003eprev = v-\u003eprev; delete v; } Node *find(int x) { // найти x в списке for (Node *v = head-\u003enext; v != tail; v = v-\u003enext) { if (v-\u003ex == x) { return v; } } return nullptr; } }; Можно хранить узлы списка в массиве, тогда вместо указателей можно использовать числа - номера ячеек массива. Но тогда нужно либо заранее знать количество элементов в списке, либо использовать динамический массив (который мы скоро изучим).\nМожно также использовать встроенный в C++ двусвязный список - std::list. Односвязный список использует меньше дополнительной памяти, но не позволяет перемещаться по списку в сторону начала. Также из него сложнее удалять элементы - не получится удалить элемент, имея ссылку только на него. Нужно как-то получить доступ к предыдущему элементу, чтобы пересчитать ссылку из него на следующий.\nДинамический массив Пусть мы хотим научиться вставлять новые элементы в конец массива. Можно попытаться сразу создать массив достаточно большого размера, и в отдельной переменной поддер-\nживать его реальную длину. Но далеко не всегда максимальное количество элементов известно заранее.\nПоступим следующим образом: если мы хотим вставить новый элемент, а место в массиве закончилось, то создадим новый массив вдвое большего размера, и скопируем данные туда.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int *a; // используемая память int size; // размер памяти int n; // реальный размер массива void pushBack(int x) { // вставить х в конец массива if (n == size) { int *b = new int[size * 2]; for (int i = 0; i \u003c size; ++i) { b[i] = a[i]; } delete[] a; a = b; size *= 2; } a[n] = x; n += 1; } void popBack() { // удалить последний элемент n -= 1; } В C++ можно и нужно использовать встроенную реализацию динамического массива std::vector.\nКаждая конкретная операция вставки элемента может работать долго $-\\Theta(n)$, где $n$ - длина массива. Зато можно показать, что среднее время работы операции вставки $O(1)$. Докажем этот факт при помощи метода амортизационного анализа, который нам ещё не раз пригодится.\nСвойство 4.3.1 Среднее время работы операции вставки - $O(1)$.\n✍️ Это утверждение равносильно тому, что суммарное время работы $m$ операций вставки - $O(m)$.\nДоказательство Операция вставки работает не за $O(1)$ только тогда, когда происходит удвоение размера используемой памяти. Заметим, что если такая операция увеличила размер с size до $2 \\cdot$ size, то хотя бы size $/ 2$ предыдущих операций вставки работали за $O(1)$ (может быть, и больше, если происходили также удаления элементов). Тогда суммарное время работы этих size $/ 2+1$ операций есть $O$ (size), поэтому среднее время их работы есть $O(1)$.\nСтек, очередь, дек Стек (stack) позволяет поддерживать список элементов, вставлять элемент в конец списка, а также удалять элемент из конца списка. Часто также говорят, что он организован по принципу LIFO (last in - first out).\nОчередь (queue) организована по принципу FIFO (first in - first out). Она позволяет вставлять элемент в конец списка, а также удалять элемент из начала списка.\nДек (deque - double ended queue), или двусторонняя очередь, отличается от обычной очереди тем, что позволяет добавлять и удалять элементы как в начало, так и в конец списка.\nВсе эти структуры можно реализовать с помощью связного списка. Для стека и очереди хватит односвязного списка: в стеке достаточно поддерживать ссылку на предыдущий элемент, а в очереди - на следующий. Для реализации дека понадобится двусвязный список.\nТакже все три структуры можно реализовать с помощью динамического массива. В случае дека нужно научиться добавлять элементы в начало, для чего можно зациклить массив. То же самое можно сделать в реализации очереди, чтобы переиспользовать освободившееся место в начале массива. Приведём примерную реализацию дека на динамическом массиве:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 int *a; // используемая память int size; // размер памяти int b, e; // дек использует ячейки в диапазоне [b,e), или, // если b \u003e= e, в диапазонах [b,size) и [0,e) int getSize() { // узнать количество элементов в деке if (b \u003c e) { return e - b; } else { return e - b + size; } } void pushBack(int x) { // вставить x в конец дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[e] = x; e = (e + 1) % size; } void popBack() { // удалить элемент из конца дека e = (e - 1 + size) % size; } void pushFront(int x) { // вставить x в начало дека if (getSize() == size) { ... // выделяем вдвое больше памяти } a[b] = x; b = (b - 1 + size) % size; } void popFront() { // удалить элемент из начала дека b = (b + 1) % size; } В C++ существуют встроенные реализации этих структур - std::stack, std::queue, std::deque.",
    "description": "Массив Массив (array) - структура данных, позволяющая хранить набор значений в ячейках, пронумерованных индексами (или набором индексов в случае многомерного массива) из некоторого отрезка целых чисел. Встроен в большинство современных языков программирования.\nМассив позволяет за константное $(O(1))$ время получать доступ к элементу по индексу, а также изменять этот элемент. При этом массив имеет фиксированный размер, поэтому не поддерживает операций вставки и удаления элементов. Если хочется вставить или удалить элемент, можно создать новый массив нужного размера и скопировать информацию в него, но это потребует $\\Theta(n)$ операций, где $n$ - длина массива.",
    "tags": [],
    "title": "4. Базовые структуры данных",
    "uri": "/basics/basic_data_structures/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.\nТеперь можно рекурсивно повторить те же рассуждения - посмотреть на середину нового подоотрезка массива, и либо в середине найдётся $x$, либо мы поймём, в какой половине нового подотрезка нужно его искать. Будем повторять эти действия, пока не найдём $x$, либо не дойдём до подотрезка нулевой длины. Поскольку на каждом шаге длина отрезка уменьшается вдвое, это произойдёт через $O(\\log n)$ шагов.\nТакже можно оценить время работы нашего алгоритма с помощью основной теоремы о рекуррентных соотношениях: время работы алгоритма в худшем случае (когда $x$ так и не нашёлся, либо нашёлся в самом конце) удовлетворяет рекуррентному соотношению $T(n)=T\\left(\\frac{n}{2}\\right)+\\Theta(1)$. Это соответствует $a=1, b=2, c=0$ в формулировке теоремы. $c=0=\\log _{2} 1=\\log _{b} a$, тогда по теореме время работы алгоритма в худшем случае есть $\\Theta(\\log n)$.\nПримерная реализация алгоритма двоичного поиска:\n1 2 3 4 5 6 7 8 9 10 11 12 binarySearch(a, n, x): # ищет x в массиве а длины n, возвращает индекс, # по которому лежит x, или -1, если х в массиве нет l = 0, r = n - 1 # границы интересующего нас подотрезка массива while l \u003c= r: # пока подотрезок непуст m = (l + r) / 2 if a[m] == x: return m if a[m] \u003c x: l = m + 1 else: r = m - 1 return -1 # x так и не нашёлся Левое вхождение Возможно, $x$ встречается в массиве несколько раз (так как массив отсортирован, эти вхождения образуют подотрезок массива). Научимся находить количество вхождений $x$ в массив (то есть длину этого подотрезка).\nДостаточно найти индексы самого левого и самого правого вхождений $x$ в массив, тогда количество вхождений - это разность этих индексов плюс один. Начнём с левого вхождения.\nБудем снова поддерживать границы $l$ и $r$, но уже со следующим условием: пусть в любой момент времени выполняется $a[l]\u003c x$ и $a[r] \\geqslant x$. Удобно мысленно добавить к массиву фиктивные элементы $a[-1]=-\\infty$ и $a[n]=\\infty$, тогда не нужно отдельно обрабатывать случаи, когда все элементы меньше $x$, или все элементы больше или равны $x$.\nСам алгоритм становится только проще: смотрим на середину отрезка $(l, r)$, и, в зависимости от значения элемента массива с этим индексом, сдвигаем $l$ или $r$ так, чтобы требуемые от них условия не нарушились. Когда $l$ и $r$ указывают на соседние элементы (то есть $l=r-1$ ), всё ещё верно, что $a[l] \u003c x, a[r] \\geqslant x$. Тогда если $a[r]=x$, то $r$ индекс левого вхождения $x$, иначе $x$ в массиве не встречается.\nПримерная реализация:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 lowerBound(a, n, x): # возвращает минимальное i такое, что a[i] \u003e= x l = -1, r = n # l и r изначально указывают на фиктивные элементы while r - l \u003e 1: m = (l + r) / 2 if a[m] \u003c x: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r getLeftEntry(a, n, x): # возвращает номер левого вхождения или -1 i = lowerBound(a, n, x) if i == n or a[i] != x: return -1 return i Правое вхождение ищется примерно так же: нужно требовать от границ поиска $a[l] \\leqslant x$ и $a[r]\u003ex$, тогда в конце $l$ - правое вхождение, если $a[l]=x$, иначе $x$ в массиве не встречается. Заметим, что в реализации самого поиска нужно поправить всего одну строчку: a [m] \u003c x заменить на $\\mathrm{a}[\\mathrm{m}]$ \u003c= x .\nВ C++ есть встроенные функции std::lower_bound и std::upper_bound. Первая возвращает итератор, указывающий на первый элемент, больше или равный $x$, вторая на первый элемент, строго больший $x$. Пример использования:\n1 2 3 4 5 6 7 8 i = lower_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e= x; n, если все элементы а меньше х i = upper_bound(a, a + n, x) - a; // минимальное i такое, что a[i] \u003e x; n, если все элементы а меньше или равны х i = upper_bound(a, a + n, x) - a - 1; // максимальное i такое, что a[i] \u003c= x; -1, если все элементы а больше х cnt = upper_bound(a, a + n, x) - lower_bound(a, a + n, x); // количество вхождений х в а Двоичный поиск по функции Можно рассмотреть и ещё более общую версию алгоритма. Пусть есть некоторая функция $f(i)$ такая, что $f(i)=0$ для всех $i$, меньших некоторого порога $t$, и $f(i)=1$ для всех $i \\geqslant t$. Тогда этот порог можно найти с помощью алгоритма двоичного поиска.\nИдея та же, что и при поиске левого вхождения: сначала возьмём такие границы поиска $l=L, r=R$, что $f(L)=0, f(R)=1$. После этого на каждом шаге будем сдвигать одну из границ $l, r$, в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. В тот момент, когда $l$ и $r$ отличаются на единицу, $r$ - искомый порог. Если считать, что значение функции в точке вычисляется за $O(1)$, то время работы алгоритма $-O(\\log (R-L))$, где $L, R-$ начальные границы поиска.\nЗаметим, что алгоритмы поиска левого и правого вхождения являются частными случаями этого алгоритма: в случае левого вхождения можно взять функцию $f$ такую,\nчто $f(i)=1$ тогда и только тогда, когда $a[i] \\geqslant x$; в случае правого вхождения $-f(i)=1$ тогда и только тогда, когда $a[i]\u003ex$ (и нас интересует максимальное $i$ такое, что $f(i)=0$, то есть в конце алгоритма нужно вернуть $l$, а не $r$ ).\nДвоичный поиск по функции с вещественным аргументом Двоичный поиск можно делать и по функции, принимающей значения во всех вещественных точках, а не только в целых. Решим для примера такую задачу: дана монотонно возрастающая непрерывная функция $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, и точки $L, R$ такие, что $f(L)\u003c0$, $f(R) \\geqslant 0$. Найдём точку $x$ такую, что $f(x)=0$. (R)\n✍️ Если такие $L, R$ не даны, но известно, что они существуют, то их можно найти алгоритмом экспоненциального поиска: начнём с $R=1$ и будем удваивать $R$, пока $f(R)\u003c0$. Аналогично, начнём с $L=-1$ и будем удваивать $L$, пока $f(L) \\geqslant 0$. При этом мы затратим $O\\left(\\log \\left|R^{\\prime}\\right|+\\log \\left|L^{\\prime}\\right|\\right)$ действий, где $R^{\\prime} \\geqslant 0, L^{\\prime} \\leqslant 0-$ любые такие, что $f\\left(L^{\\prime}\\right)\u003c0, f\\left(R^{\\prime}\\right) \\geqslant 0$.\nПоскольку $f$ принимает вещественные значения, мы не всегда можем найти значение такого $x$ абсолютно точно. Тем не менее, мы можем найти такое $x$ с погрешностью $\\varepsilon$ : найдём такое $x$, что существует $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0,\\left|x-x^{\\prime}\\right|\u003c\\varepsilon$.\nАлгоритм практически не меняется: начинаем с границ $l=L, r=R$, на каждом шаге сдвигаем одну из границ в зависимости от значения $f\\left(\\frac{l+r}{2}\\right)$. Единственное отличие: мы завершаем алгоритм в тот момент, когда $r-l\u003c\\varepsilon$. Поскольку в этот момент всё ещё $f(l)\u003c0, f(r) \\geqslant 0$, а функция $f$ непрерывна, на отрезке $[l, r]$ найдётся $x^{\\prime}$ такое, что $f\\left(x^{\\prime}\\right)=0$. Поскольку длина отрезка меньше $\\varepsilon$, любая из границ $l, r$ подойдёт в качестве ответа. Время работы алгоритма (при условии, что значение $f$ вычисляется за $O(1)$ ) $O\\left(\\log \\left(\\frac{R-L}{\\varepsilon}\\right)\\right)$.\nЕсли вычисления проводятся при помощи стандартных вещественнозначных типов данных (например, double в C++), то из-за накопления погрешности при пересчёте границ условие $r-l\u003c\\varepsilon$ может никогда не выполниться. Чтобы избежать этого, вместо while напишем цикл, делающий столько итераций, сколько нужно, чтобы условие точно выполнилось. Пример реализации:\n1 2 3 4 5 6 7 8 9 10 findRoot(L, R, eps): l = L, r = R k = log((r - l) / eps) # через k итераций условие r - l \u003c eps выполнится for i = 0..(k - 1): m = (l + r) / 2 if f(m) \u003c 0: l = m # условие на l не нарушилось else: r = m # условие на r не нарушилось return r Метод двух указателей Иногда много двоичных поисков на одних и тех же данных можно соптимизировать с помощью так называемого метода двух указателей. Сделаем это на примере следующей задачи: дан массив $a$ длины $n$, состоящий из неотрицательных целых чисел. Нужно найти максимальный по длине подотрезок массива, сумма на котором не превосходит $M$.\nУдобнее работать не с отрезками, а с полуинтервалами: паре $l, r$ будем сопоставлять полуинтервал $a[l, r)$, то есть элементы массива с индексами $l, l+1, \\ldots, r-1$ (это практически всегда позволяет писать меньше плюс-минус единиц в индексах массивов, что делает код короче). Самое простое решение - перебрать все возможные полуинтервалы:\n1 2 3 4 5 6 7 maxLen = 0, ansL = 0, ansR = 0 # здесь будем хранить ответ for l = 0..(n - 1): # перебираем левую границу sum = 0 # будем считать сумму на текущем полуинтервале for r = (l + 1)..n: sum += a[r - 1] if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r Получилось решение за $\\Theta\\left(n^{2}\\right)$. Заметим, что при фиксированной левой границе сумма на полуинтервале не уменьшается при увеличении правой границы. Значит сколькото первых правых границ подойдут, а все последующие уже не подойдут. При этом нас интересует максимальная подходящая правая граница. Её можно найти двоичным поиском, таким образом соптимизировав решение до $\\Theta(n \\log n)$ :\n✍️ Для того, чтобы быстро находить сумму на произвольном полуинтервале, заранее предподсчитаем префиксные суммы - массив $p$ такой, что $p[i]=\\sum_{j=0}^{i-1} a_{j}$. Тогда сумма на полуинтервале $a[l, r)$ равна $\\sum_{i=l}^{r-1} a_{i}=p[r]-p[l]$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 maxLen = 0, ansL=0, ansR = 0 int p[n + 1] # предподсчитываем префиксные суммы p[0] = 0 for i = 1..n: p[i] = p[i - 1] + a[i - 1] for l = 0..(n - 1): sl = l, sr = n + 1 # границы поиска, sum a[l..sl) \u003c= M, sum a[l..sr) \u003e M while sr - sl \u003e 1: sm = (sl + sr) / 2 if p[sm] - p[l] \u003c= M: sl = sm else: sr = sm if sl - l \u003e maxLen: # sl - максимальная подходящая правая граница maxLen = sl - l, ansL = l, ansR = sl Перейдём теперь собственно к методу двух указателей. Пусть $r$ - максимальная подходящая правая граница для левой границы $l-1$. Тогда $M \\geqslant \\sum_{i=l-1}^{r-1} a_{i} \\geqslant \\sum_{i=l}^{r-1} a_{i}$, то есть граница $r$ подойдёт и для $l$. Будем вместо двоичного поиска искать границу $r$ так же, как и в самой первой версии алгоритма: просто перебирая все границы подряд. Но начнём перебор не с $l+1$, а с правой границы, найденной на предыдущем шаге. Поскольку теперь правая граница только увеличивается в ходе алгоритма, суммарно за всё время выполнения алгоритма она сдвинется не более, чем $n$ раз. Тогда общее время работы алгоритма $-\\Theta(n)$.\n1 2 3 4 5 6 7 8 9 maxLen = 0, ansL=0, ansR = 0 r = 0, sum = 0 for l = 0..(n - 1): # перебираем левую границу while r \u003c n and sum + a[r] \u003c= M: sum += a[r] r += 1 if sum \u003c= M and r - l \u003e maxLen: maxLen = r - l, ansL = l, ansR = r sum -= a[l]",
    "description": "Двоичный поиск Если элементы массива расположены в возрастающем порядке (то есть массив отсортирован), то искать элемент в массиве можно быстрее, чем за линейное время.\nПусть мы хотим найти $x$ в отсортированном массиве $a$ длины $n$. Снова применим идею метода “разделяй и властвуй”: посмотрим на элемент в середине массива - $a\\left[\\frac{n}{2}\\right]$. Если $a\\left[\\frac{n}{2}\\right]=x$, то мы нашли $x$ в массиве $a$. Если $a\\left[\\frac{n}{2}\\right]\u003ex$, то $x$ может найтись только в $a\\left[0,1, \\ldots, \\frac{n}{2}-1\\right]$. Если же $a\\left[\\frac{n}{2}\\right] \u003c x$, то $x$ может найтись только в $a\\left[\\frac{n}{2}+1, \\ldots, n-1\\right]$. В любом из последних двух случаев, мы вдвое уменьшили длину отрезка, на котором нужно искать $x$.",
    "tags": [],
    "title": "5. Двоичный поиск",
    "uri": "/basics/binary_search/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Квадратичные сортировки Существует множество различных алгоритмов, сортирующих массив длины $n$ за $\\Theta\\left(n^{2}\\right)$. Мы поговорим лишь о двух из них.\nСортировка выбором Сортировка выбором (selection sort) на $i$-м шаге находит $i$-й по возрастанию элемент и ставит его на $i$-ю позицию. Поскольку первые $i-1$ элементов в этот момент уже стоят на своих позициях, достаточно просто найти минимальный элемент в подотрезке $[i, n)$.\n1 2 3 4 5 6 for i = 0..(n - 1): minPos = i for j = (i + 1)..(n - 1): if a[minPos] \u003e a[j]: minPos = j swap(a[i], a[minPos]) Сортировка вставками На $i$-м шаге сортировки вставками (insertion sort) первые $i$ элементов массива (образующие префикс длины $i$ ) расположены в отсортированном порядке. $i$-й шаг состоит в том, что $i$-й элемент массива вставляется в нужную позицию остортированного префикса.\n1 2 3 for i = 1..(n - 1): for (j = i; j \u003e 0 and a[j] \u003c a[j - 1]; --j): swap(a[j], a[j - 1]) Время работы В обеих сортировках два вложенных цикла в худшем случае дают время работы $\\Theta\\left(n^{2}\\right)$. Сортировка выбором полезна тем, что делает $\\Theta(n)$ операций swap. Это свойство пригождается, когда сортируются тяжёлые объекты, и каждая операция swap занимает много времени.\nИнверсией называют такую пару элементов на позициях $i \u003c j$, что $a_{i} \u003e a_{j}$. Последовательность элементов является отсортированной тогда и только тогда, когда в ней нет инверсий. Время работы сортировки вставками можно оценить как $\\Theta(n+\\operatorname{Inv}(a))$, где $\\operatorname{Inv}(a)$ - количество инверсий в массиве $a$, так как на каждом шаге внутреннего цикла количество инверсий в массиве уменьшается ровно на один. Значит, на отсортированном (или почти отсортированном) массиве время работы сортировки вставками составит $O(n)$.\nСтабильность Сортировка называется стабильной, если она оставляет равные элементы в исходном порядке. Обычно такое свойство сортировки нужно, когда помимо данных, по которым производится сортировка, в элементах массива хранятся какие-то дополнительные данные. Например, если дан список участников соревнований в алфавитном порядке, и хочется отсортировать их по убыванию набранных баллов так, чтобы участники с равным числом баллов всё ещё шли в алфавитном порядке.\nСортировка выбором стабильной не является (например, массив $[5,5,3,1]$ после первого шага изменится на $[1,5,3,5]$, при этом порядок пятёрок поменялся). Сортировка вставками стабильна, так как меняет местами только соседние элементы, образующие инверсию, поэтому ни в какой момент времени не поменяет порядок равных элементов.\n✍️ Любую сортировку можно сделать стабильной, если вместо исходных элементов сортировать пары (элемент, его номер в исходном массиве), и при сравнении пар сначала сравнивать элементы, а при равенстве номера.\nПрименение на практике Сейчас мы перейдём в алгоритмам сортировки, работающим за $\\Theta(n \\log n)$. Квадратичные сортировки, благодаря малой константе в оценке времени работы, работают быстрее более сложных алгоритмов на коротких массивах. На практике часто применяется гибридный подход: алгоритм сортировки, использующий метод “разделяй и властвуй”, работает, пока массив не поделится на части достаточно малого размера, после чего на них запускается алгоритм квадратичной сортировки.\nСортировка слиянием (Merge sort) Сортировка слиянием (Von Neumann, 1945) использует метод “разделяй и властвуй” следующим образом: массив делится на две части, каждая из них сортируется рекурсивно, после чего две отсортированных части сливаются при помощи метода двух указателей.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mergeSort(a, l, r): # coртирует a[l, r) if r - l \u003c= 1: return m = (l + r) / 2 mergeSort(a, l, m) mergeSort(a, m, r) merge(a, l, m, r) # merge использует вспомогательный массив buf достаточно большого размера merge(a, l, m, r): # сливает два отсортированных отрезка а[l, m) и а[m, r) for (i = l, j = m, k = 0; i \u003c m or j \u003c r; ): if i == m or (j \u003c r and a[i] \u003e a[j]): buf[k] = a[j] k += 1, j += 1 else: buf[k] = a[i] k += 1, i += 1 for i = 0..(r - l - 1): a[l + i] = buf[i] Время работы сортировки слиянием можно оценить с помощью рекуррентного соотношения $T(n)=2 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n)$. По основной теореме о рекуррентных соотношениях получаем $T(n)=\\Theta(n \\log n)$.\nСортировка слиянием стабильна (поскольку функция merge не меняет относительный порядок равных элементов).\nНедостатком сортировки слиянием является то, что она требует $\\Theta(n)$ дополнительной памяти (вспомогательный массив в merge).\nБыстрая сортировка (Quicksort) Быстрая сортировка (Hoare, 1959) также использует метод “разделяй и властвуй”, но немного по-другому. Возьмём какой-нибудь элемент массива - $x$. Поделим массив на три части так, что в первой все элементы меньше $x$, во второй равны $x$, в третьей больше $x$ (это можно сделать за линейное от длины массива время, например, с помощью трёх вспомогательных массивов). Остаётся рекурсивно отсортировать первую и третью части.\n1 2 3 4 5 6 quickSort(a): if len(a) \u003c= 1: return x = randomElement(a) # x - случайный элемент а a --\u003e l (\u003c x), m (= x), r (\u003e x) # делим а на три части return quickSort(l) + m + quickSort(r) На практике, чтобы алгоритм работал быстрее и использовал меньше дополнительной памяти, используют более хитрый способ деления массива на части:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # partition выбирает x - случайный элемент a[l, r], # переставляет местами элементы a[l, r] и возвращает m (l \u003c= m \u003c r) такое, что # все элементы a[l, m] меньше или равны x, все элементы a[m + 1, r] больше или равны x partition(a, l, r) -\u003e int: p = random(l, r), x = a[p] swap(a[p], a[l]) # теперь x стоит на l-й позиции i = l, j = r while i \u003c= j: while a[i] \u003c x: i += 1 while a[j] \u003e x: j -= 1 if i \u003e= j: break swap(a[i], a[j]) i += 1, j -= 1 return j quickSort(a, l, r): # coртирует a[l, r] if l == r: return m = partition(a, l, r) quickSort(a, l, m) quickSort(a, m + 1, r) Свойства Функция partition работает корректно, то есть $i$ и $j$ не выходят за границы $l, r$, функция возвращает такое $m$, что $l \\leqslant m \u003c r$, все элементы $a[l, m]$ меньше или равны $x$, все элементы $a[m+1, r]$ больше или равны $x$. Доказательство Заметим, что в любой момент времени все элементы $a[l, i)$ меньше или равны $x$, все элементы $a(j, r]$ больше или равны $x$.\nНа первой итерации внешнего цикла в момент проверки условия на $13$-ой строке верно, что $i=l \\leqslant j$. Значит либо мы сразу выйдем из внешнего цикла (если $i=j=l \u003c r$ ), либо выполнятся $15-16$ строки, после чего всегда будет верно, что $a[l] \\leqslant x, a[r] \\geqslant x, j \u003c r$. При этом 15-16 строки выполнились только при $i \u003c j$, тогда даже после их выполнения $i$, $j$ не вышли за границы $l$, $r$.\nОтдельно отметим, что после первой итерации внешнего цикла точно выполняется неравенство $j \u003c r$.\nНа всех последующих итерациях внешнего цикла после выполнения $9-12$ строк будут верны неравенства $j \\geqslant l, i \\leqslant r$ (так как $a[l] \\leqslant x, a[r] \\geqslant x$ ). При этом если условие на\n13-й строке не выполняется (то есть $i \u003c j$ ), то даже после выполнения $15-16$ строк $i, j$ не выйдут за границы $l, r$.\nНаконец, по итогам выполнения функции $l \\leqslant m=j \u003c r$, все элементы $a[m+1, r]=$ $a(j, r]$ больше или равны $x$, все элементы $a[l, m]=a[l, j]$ меньше или равны $x$ (так как это верно для элементов $a[l, i), j \\leqslant i$, причём $j=i$ только если мы вышли из внешнего цикла на $14$-ой строке, что возможно, только если $a[j]=x$ ).\nЗаметим, что такая реализация является нестабильной сортировкой.\nОценка времени работы В худшем случае массив каждый раз будет делиться очень неравномерно, и почти все элементы будут попадать в одну из частей. Время работы в худшем случае можно оценить с помощью рекуррентного соотношения $T(n)=T(n-1)+\\Theta(n)$, раскрыв которое, получаем $T(n)=\\sum_{i=1}^{n} \\Theta(i)=\\Theta\\left(n^{2}\\right)$.\nВ лучшем же случае массив каждый раз будет делиться на две примерно равные части. Получаем рекуррентное соотношение $T(n)=2 \\cdot T\\left(\\frac{n}{2}\\right)+\\Theta(n)$, тогда по основной теореме о рекуррентных соотношениях получаем $T(n)=\\Theta(n \\log n)$.\nОказывается, время работы алгоритма в среднем намного ближе к лучшему случаю, чем к худшему. Для того, чтобы это доказать, нам понадобится терминология из теории вероятностей. Определение Математическое ожидание случайной величины $X$, принимающей значение $x_{1}$ с вероятностью $p_{1}, x_{2}$ с вероятностью $p_{2}, \\ldots, x_{n}$ с вероятностью $p_{n}$ $\\left(p_{1}+\\cdots+p_{n}=1\\right)$ есть\n$$ \\mathbb{E} X=\\sum_{i=1}^{n} p_{i} x_{i} . $$ Теорема Математическое ожидание времени работы алгоритма быстрой сортировки есть $O(n \\log n)$. Доказательство Мы докажем теорему только в случае, когда все элементы массива попарно различны. Кроме того, будем рассматривать версию алгоритма, делящую массив на три части (элементы меньше $x$; элементы, равные $x$; элементы больше $x$ ).\nДля удобства будем использовать индексы элементов не в исходном, а в уже отсортированном массиве: пусть отсортированный массив имеет вид $z_{1}, z_{2}, \\ldots, z_{n}$, исходный массив $a$ - это какая-то перестановка элементов $z_{i}$.\nВремя работы алгоритма быстрой сортировки пропорционально количеству выполненных сравнений. Обозначим количество выполненных сравнений за $T(n)$, достаточно оценить его математическое ожидание.\n$z_{i}$ и $z_{j}$ могли сравниваться, только если на каком-то шаге алгоритма один из них был выбран в качестве $x$. Заметим, что такой элемент не участвует в последующих рекурсивных вызовах, поэтому каждую пару элементов алгоритм сравнит не более, чем один раз.\nПусть $\\Gamma$ - множество всех возможных сценариев выполнения алгоритма, $p(A)$ вероятность того, что произошёл сценарий $A \\in \\Gamma$. Для каждой пары $i, j$ введём величину $\\chi(A, i, j)$, равную единице, если $z_{i}$ и $z_{j}$ сравнивались в сценарии $A$, и нулю, если не сравнивались. Математическое ожидание количества выполненных алгоритмом сравнений равняется\n$$ \\mathbb{E} T(n)=\\sum_{A \\in \\Gamma}\\left(p(A) \\sum_{1 \\leqslant i \u003c j \\leqslant n} \\chi(A, i, j)\\right)=\\sum_{1 \\leqslant i \u003c j \\leqslant n} \\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j) . $$Остаётся для каждой пары $1 \\leqslant i \u003c j \\leqslant n$ посчитать $\\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j)$, то есть вероятность, с которой $z_{i}$ и $z_{j}$ сравнивались между собой.\nПосмотрим на момент, в который $z_{i}$ и $z_{j}$ при делении массива попали в разные части. Заметим, что массив, который делился на части в этот момент, после сортировки будет являться подотрезком отсортированного массива $z$, тогда вместе с $z_{i}$ и $z_{j}$ он содержит весь подотрезок $z[i, j]$.\nПоскольку $z_{i}$ и $z_{j}$ при делении попали в разные части, в качестве $x$ точно был выбран один из элементов $z[i, j]$. При этом $z_{i}$ и $z_{j}$ сравнивались между собой только если в качестве $x$ был выбран один из них. Поскольку $x$ выбирается среди всех элементов равновероятно, вероятность того, что между $z_{i}$ и $z_{j}$ произошло сравнение, равняется $\\frac{2}{j-i+1}$. Получаем\n$$ \\begin{aligned} \\mathbb{E} T(n)= \u0026 \\sum_{1 \\leqslant i \u003c j \\leqslant n} \\sum_{A \\in \\Gamma} p(A) \\chi(A, i, j)=\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac{2}{j-i+1}\\xlongequal{\\left(k=j-i\\right)} \\\\ \u0026 =\\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k+1} \u003c 2 n \\sum_{k=1}^{n} \\frac{1}{k}=O(n \\log n) \\end{aligned} $$Последний переход можно понять, например, следующим способом:\n$$ \\begin{gathered} \\sum_{k=1}^{n} \\frac{1}{k}=\\frac{1}{1}+\\left(\\frac{1}{2}+\\frac{1}{3}\\right)+\\left(\\frac{1}{4}+\\frac{1}{5}+\\frac{1}{6}+\\frac{1}{7}\\right)+\\cdots\u003c \\\\ \u003c \\\\ \\frac{1}{1}+\\left(\\frac{1}{2}+\\frac{1}{2}\\right)+\\left(\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{1}{4}\\right)+\\cdots \\leqslant\\left\\lfloor\\log _{2} n\\right\\rfloor+1 . \\end{gathered} $$ ✍️ Отсюда следует и более сильное утверждение: время работы алгоритма есть $O(n \\log n)$ с вероятностью, близкой в единице. Это следует из следующего утверждения, известного как неравенство Маркова: для неотрицательной случайной величины $X$ с математическим ожиданием $\\mathbb{E}(X)$ вероятность того, что $X\u003ek \\cdot \\mathbb{E}(X)$, не превосходит $\\frac{1}{k}$. Это верно, так как в противном случае математическое ожидание $X$ оказалось бы больше $\\frac{1}{k} \\cdot k \\cdot \\mathbb{E}(X)=\\mathbb{E}(X)$. В нашем случае, например, для $k=100$ получаем, что с вероятностью $99 \\%$ время работы не превосходит $O(100 \\cdot n \\log n)=O(n \\log n)$.\nВзятие случайного элемента - достаточно медленная операция, поэтому на практике вместо случайного элемента часто берут какой-то конкретный, например самый левый, самый правый, или средний; также часто используют средний по значению из этих трёх. Для подобных версий алгоритма можно построить массив, на котором сортировка будет работать за $\\Theta\\left(n^{2}\\right)$. С этим борются разными способами, например, когда глубина рекурсии превышает $\\log n$, переключаются на какой-нибудь другой алгоритм сортировки.\nНа практике алгоритм быстрой сортировки оказывается одним из самых быстрых и часто используемых. Встроенная в C++ сортировка - std::sort, использует алгоритм Introsort, который начинает сортировать массив алгоритмом быстрой сортировки, на большой глубине рекурсии переключается на heapsort (который мы скоро изучим), а массивы совсем небольшой длины сортирует сортировкой вставками.\nПоиск $k$-й порядковой статистики $k$-я порядковая статистика на массиве из $n$ элементов - это $k$-й по возрастанию элемент. Например, при $k=1$ это минимум, при $k=n$ - максимум. Медиана - это элемент, который оказался бы в середине массива, если бы его отсортировали. Если длина массива чётна, то в нём есть две медианы на позициях $\\left\\lfloor\\frac{n+1}{2}\\right\\rfloor$ и $\\left\\lceil\\frac{n+1}{2}\\right\\rceil$. Для определённости под медианой будем иметь в виду $\\left\\lfloor\\frac{n+1}{2}\\right\\rfloor$-ю порядковую статистику.\nМинимум и максимум в массиве очень легко ищется за $O(n)$ одним проходом по всем элементам массива. $k$-й по возрастанию элемент так просто уже не найти.\nМожно отсортировать массив за $O(n \\log n)$, тогда $k$-я порядковая статистика окажется на $k$-й позиции (если нумеровать элементы массива, начиная с единицы). Однако существуют и более быстрые алгоритмы, находящие $k$-ю порядковую статистику для произвольного $k$ за $O(n)$.\nВернёмся к алгоритму быстрой сортировки. Когда мы поделили массив на две части, можно понять, в какой из этих частей находится $k$-й по возрастанию элемент: если размер левой части хотя бы $k$, то он находится в ней, иначе он находится в правой части. Тогда, если нас интересует не весь отсортированный массив, а только $k$-й по возрастанию элемент, можно сделать рекурсивный запуск только от той части, в которой он лежит.\n1 2 3 4 5 6 7 8 kthElement(a, l, r, k): # находит k-ю порядковую статистику в a[l, r] if l == r: return a[l] m = partition(a, l, r) if m - l + 1 \u003e= k: return kthElement(a, l, m, k) else: return kthElement(a, m + 1, r, k - (m - l + 1)) В худшем случае такой алгоритм будет работать по-прежнему за $\\Theta\\left(n^{2}\\right)$. Однако оказывается, что оценка среднего времени работы после такой оптимизации улучшается с $O(n \\log n)$ до $O(n)$.\nТеорема Математическое ожидание времени работы алгоритма kthElement есть $O(n)$. Доказательство Мы докажем теорему в случае, когда все элементы массива попарно различны.\nБудем говорить, что алгоритм находится в $j$-й фазе, если размер текущего отрезка массива не больше $\\left(\\frac{3}{4}\\right)^{j} n$, но строго больше $\\left(\\frac{3}{4}\\right)^{j+1} n$. Оценим время работы алгоритма в каждой фазе отдельно.\nНазовём элемент центральным, если хотя бы четверть элементов в текущем отрезке массива меньше его, и хотя бы четверть больше. Если в качестве разделителя $x$ был выбран центральный элемент, то размер отрезка, от которого будет сделан рекурсивный запуск, будет не больше $\\frac{3}{4}$ от размера текущего отрезка, то есть текущая фаза алгоритма точно закончится. При этом вероятность выбрать центральный элемент равна $\\frac{1}{2}$ (так как ровно половина элементов отрезка являются центральными). Тогда математическое ожидание количества рекурсивных запусков, сделанных в течение $j$-й фазы, не превосходит\n$$ 1+\\frac{1}{2}\\left(1+\\frac{1}{2}(1+\\cdots)\\right)=1+\\frac{1}{2}+\\frac{1}{4}+\\cdots=2 . $$При этом каждая итерация алгоритма на $j$-й фазе совершает $O\\left(\\left(\\frac{3}{4}\\right)^{j} n\\right)$ действий. Математическое ожидание времени работы алгоритма равняется сумме математических ожиданий времён работы каждой фазы, которая не превосходит\n$$ \\sum_{j} O\\left(\\left(\\frac{3}{4}\\right)^{j} n\\right) \\cdot 2=O\\left(n \\cdot \\sum_{j}\\left(\\frac{3}{4}\\right)^{j}\\right)=O\\left(n \\cdot \\frac{1}{1-3 / 4}\\right)=O(n) $$ В C++ есть встроенная реализация этого алгоритма - std::nth_element.\n✍️ Алгоритм можно модифицировать так, чтобы он работал за $O(n)$ в худшем случае. Это делается так: поделим массив на $n / 5$ групп по 5 элементов, в каждой за $O(1)$ найдём медиану. Теперь рекурсивным запуском алгоритма найдём медиану среди этих медиан, и уже её будем использовать в качестве разделителя. Тогда в половине групп хотя бы 3 элемента окажутся меньше разделителя, а в другой половине хотя бы 3 элемента окажутся больше разделителя. Значит каждая из частей, на которые поделился массив, будет иметь размер хотя бы $3 n / 10$. Получаем в худшем случае рекуррентное соотношение $T(n)=\\Theta(n)+T(n / 5)+T(7 n / 10)$. Можно показать, что в этом случае верно $T(n)=\\Theta(n)$.\nОценка снизу на время работы сортировки сравнениями Алгоритм сортировки сравнениями может копировать сортируемые объекты и сравнивать их друг с другом, но никак не использует внутреннюю структуру объектов. Все сортировки, изученные нами до этого момента, являются сортировками сравнения. Можно показать, что никакая сортировка сравнениями не может в общем случае работать быстрее, чем за $\\Theta(n \\log n)$.\nТеорема Любой алгоритм сортировки сравнениями имеет время работы $\\Omega(n \\log n)$ в худшем случае. Доказательство Алгоритм сортировки сравнениями должен уметь корректно сортировать любую перестановку чисел от 1 до $n$. Пусть алгоритм на каждой перестановке делает не более $k$ сравнений. Заметим, что если зафиксировать результаты всех сравнений в ходе работы алгоритма, то он будет выдавать всегда одну и ту же перестановку данного на вход массива.\n✍️ Это не совсем верно для алгоритмов, использующих случайные числа (например, для алгоритма быстрой сортировки), поскольку сама последовательность сравнений может зависеть от того, какие случайные числа выпали. Однако это верно, если зафиксировать последовательность случайных чисел, которую получает алгоритм. Поскольку алгоритм должен корректно работать на любой данной ему последовательности случайных чисел, дальнейшие рассуждения остаются верны.\nПоскольку алгоритм делает не более $k$ сравнений, и равенств не бывает (поскольку мы сортируем перестановки), существует не более $2^{k}$ различных перестановок данного на вход массива, которые он может выдать. Поскольку алгоритм корректно сортирует произвольную перестановку, получаем $2^{k} \\geqslant n!$. Тогда\n$$ k \\geqslant \\log (n!) \\geqslant \\log \\left(\\left(\\frac{n}{2}\\right)^{n / 2}\\right)=\\frac{n}{2} \\log \\frac{n}{2}=\\Omega(n \\log n) $$ Тем не менее, если обладать какой-то дополнительной информацией о свойствах сортируемых объектов, иногда можно воспользоваться этими свойствами, чтобы отсортировать объекты быстрее, чем за $\\Theta(n \\log n)$.\nСортировка подсчётом Если известно, что все числа во входном массиве целые, неотрицательные и меньше некоторого $k$, то их можно отсортировать за $\\Theta(n+k)$ (при этом понадобится $\\Theta(k)$ вспомогательной памяти). Для этого посчитаем, сколько раз встретилось каждое число от 1 до $k$, после чего просто выпишем каждое число в ответ столько раз, сколько он встречалось в исходном массиве.\n1 2 3 4 5 6 7 8 9 10 11 int c[k] countingSort(a, n): for i = 0..(k - 1): c[i] = 0 for i = 0..(n - 1): c[a[i]] += 1 p = 0 for i = 0..(k - 1): for j = 0..(c[i] - 1): a[p] = i p += 1 Ясно, что таким же образом можно сортировать целые числа, лежащие в диапазоне $[L, R)$, за $\\Theta(n+(R-L))$.\nЕсли воспользоваться ещё одним вспомогательным массивом, сортировку можно сделать стабильной:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int c[k] countingSort(a, n): for i = 0..(k - 1): c[i] = 0 for i = 0..(n - 1): c[a[i]] += 1 for i = 1..(k - 1): c[i] += c[i - 1] # теперь с[i] - количество элементов массива, не превосходящих i int b[n] for i = (n - 1)..0: c[a[i]] -= 1 b[c[a[i]]] = a[i] for i = 0..(n - 1): a[i] = b[i] Стабильная сортировка подсчётом пригодится нам в следующем алгоритме сортировки.\nПоразрядная сортировка (Radix sort) Пусть дан массив из $n$ чисел, записанных в $k$-ичной системе счисления и имеющих не более $d$ разрядов каждое. Отсортируем числа сортировкой подсчётом $d$ раз - сначала по младшему разряду, потом по следующему, и так далее, в конце - по старшему разряду. При этом будем пользоваться стабильной версией сортировки подсчётом.\nПосле первого шага числа будут отсортированы по 0-му разряду, после второго по 1-му разряду, а при равенстве цифр в 1-м разряде - по 0-му. В конце числа будут отсортированы по $(d-1)$-му разряду, при равенстве цифр в $(d-1)$-м разряде - по ( $d-2)$-му,…, при равенстве цифр во всех разрядах, кроме 0-го - по 0-му. Значит числа просто окажутся отсортированы в порядке возрастания.\n1 2 3 radixSort(a, n, d): for i = 0..(d - 1): countingSort(a, n, i) # стабильная сортировка подсчётом по i-му разряду Каждый шаг алгоритма работает за $\\Theta(n+k)$, тогда время работы всего алгоритма $\\Theta(d(n+k))$. Поразрядная сортировка является стабильной.\nС помощью поразрядной сортировки можно сортировать любые объекты, которые можно лексикографически упорядочить. Так, можно лексикографически отсортировать $n$ строк длины $d$ каждая, в записи которых используется $k$ различных символов, за $\\Theta(d(n+k))$.\nПусть нам даны $n$ неотрицательных целых чисел, меньших $m$. Если мы переведём их в $k$-ичную систему счисления, то сможем отсортировать их поразрядной сортировкой за $\\Theta\\left(\\left(1+\\log _{k} m\\right)(n+k)\\right)$ (используя $\\Theta\\left(n\\left(1+\\log _{k} m\\right)+k\\right)$ дополнительной памяти). При $n=k$ получаем время работы $\\left.\\Theta\\left(n+n \\log _{n} m\\right)\\right)=\\Theta\\left(n+n \\frac{\\log m}{\\log n}\\right)$.",
    "description": "Квадратичные сортировки Существует множество различных алгоритмов, сортирующих массив длины $n$ за $\\Theta\\left(n^{2}\\right)$. Мы поговорим лишь о двух из них.\nСортировка выбором Сортировка выбором (selection sort) на $i$-м шаге находит $i$-й по возрастанию элемент и ставит его на $i$-ю позицию. Поскольку первые $i-1$ элементов в этот момент уже стоят на своих позициях, достаточно просто найти минимальный элемент в подотрезке $[i, n)$.\n1 2 3 4 5 6 for i = 0..(n - 1): minPos = i for j = (i + 1)..(n - 1): if a[minPos] \u003e a[j]: minPos = j swap(a[i], a[minPos])",
    "tags": [],
    "title": "6. Сортировки",
    "uri": "/basics/sorts/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Пусть мы хотим поддерживать множество элементов и быстро выполнять на нём следующие операции: добавлять и удалять элементы, а также находить минимум.\nМожно попытаться сделать это с помощью динамического массива или списка. Тогда мы сможем добавлять и удалять элементы за $O(1)$ (чтобы быстро удалить элемент из массива, поменяем его местами с последним элементом, после чего уменьшим длину массива на один). Однако быстро находить минимум не получится: можно пытаться поддерживать указатель на минимальный элемент, но после каждого удаления минимума новый минимум получится найти лишь перебором всех оставшихся элементов за $\\Theta(n)$ (где $n$ - число элементов в множестве). Можно было бы поддерживать массив в отсортированном порядке (тогда минимум всегда будет в начале), но тогда не получится быстро добавлять или удалять произвольный элемент.\nДвоичная куча (пирамида), binary heap - структура данных, которая позволяет добавлять и удалять элементы за $O(\\log n)$, а также в любой момент иметь доступ к минимальному элементу в множестве за $O(1)$.\nОпределение Двоичная куча из $n$ элементов - это массив $a$ с индексами от 1 до $n$, образующий двоичное дерево: для любого $1 \u003c i \\leqslant n$ родитель $i$-го элемента - это элемент с номером $\\left\\lfloor\\frac{i}{2}\\right\\rfloor ;$ соответственно, для любого $1 \\leqslant i \\leqslant n$ дети $i$-го элемента имеют номера $2 i, 2 i+1$ (если элементы с такими номерами существуют).\nПри этом выполняется свойство кучи: значение любого элемента не меньше значения его родителя: $a[i] \\geqslant a\\left[\\left\\lfloor\\frac{i}{2}\\right]\\right]$. Такую кучу также могут называть неубывающая пирамида. Аналогично, если $a[i] \\leqslant a\\left[\\left\\lfloor\\frac{i}{2}\\right]\\right]$ - то невозрастающая пирамида.\nЗаметим, что отсортированный массив всегда является кучей. Но фиксированный набор элементов обычно можно упорядочить и многими другими способами так, что тоже получится куча. Будем пользоваться терминами из теории графов. Элементы дерева (кучи) будем называть вершинами. Корень дерева - единственная вершина без родителя (в нашем случае это $a[1]$ ). Потомки вершины - это она сама, её дети, а также все их потомки; у листа (вершины без детей) потомков (кроме него самого) нет. Предок вершины $v$ - любая такая вершина $u$, что $v$ - потомок $u$. Поддерево вершины $v$ состоит из всех её потомков; $v$ - корень своего поддерева.\nИногда мы будем называть кучей не массив с индексами от 1 до $n$, а просто дерево, в котором свойство кучи выполняется для всех пар родитель-ребёнок. В этом смысле любое поддерево кучи тоже является кучей.\nГлубина $x$ - количество вершин на пути от корня дерева до $x$ (все вершины являются потомками корня). Высота вершины $x$ - максимальное количество вершин на пути от $x$ до какого-либо её потомка. Высота дерева - максимум из высот вершин, то есть высота корня.\nГлубина $i$-й вершины равна $\\lfloor\\log i\\rfloor+1$, так как $i$ нужно поделить на два нацело $\\lfloor\\log i\\rfloor$ раз, чтобы получить 1.\nВысота корня - это максимум из глубин всех его потомков. Максимальная глубина у $n$-й вершины $-\\lfloor\\log n\\rfloor+1$. Значит, высота кучи из $n$ элементов равна $\\lfloor\\log n\\rfloor+1$.\nБазовые операции Научимся выполнять три простых операции: getMin() - поиск минимального элемента в куче, insert(x) - добавление нового элемента $x$ в кучу, и extractMin() - удаление минимума из кучи. Последние две операции будут пользоваться вспомогательными операциями $\\operatorname{siftUp(i)~и~siftDown(i).~}$\nПоиск минимума Из определения кучи сразу же следует, что минимальный элемент всегда будет находиться в корне. Тогда просто вернём $a[1]$, время работы $-O(1)$.\n1 2 getMin(): return a[1] Добавление нового элемента Пусть свойство кучи “практически” выполняется: для некоторого $i$ известно, что $a[i]=x$ можно увеличить так, что массив $a$ станет кучей. Такую почти кучу можно “починить”, не меняя множество лежащих в ней значений, следующим образом: просто будем “поднимать” $x$ вверх, пока $x$ меньше значения в родителе.\n1 2 3 4 siftUp(i): while i \u003e 1 and a[i] \u003c a[i / 2]: swap(a[i], a[i / 2]) i /= 2 Свойства Пусть известно, что $a[i]=x$ можно увеличить так, что массив $a$ станет кучей. Тогда после выполнения $\\operatorname{siftUp}(i)$ массив $a$ станет кучей. Доказательство Докажем утверждение индукцией по $i$. База. Если $i=1$, то $a$ уже является кучей (если корень можно увеличить так, что он станет не больше детей, то он и сейчас не больше детей). siftUp(1) не меняет массив $a$.\nПереход. Заметим сначала, что если $a$ уже является кучей, то $\\operatorname{siftUp}(i)$ ничего не сделает, поэтому утверждение предложения верно.\nЕсли $a$ не является кучей, то единственная пара родитель-ребёнок, для которой не выполняется свойство кучи - это $x=a[i]",
    "description": "Пусть мы хотим поддерживать множество элементов и быстро выполнять на нём следующие операции: добавлять и удалять элементы, а также находить минимум.\nМожно попытаться сделать это с помощью динамического массива или списка. Тогда мы сможем добавлять и удалять элементы за $O(1)$ (чтобы быстро удалить элемент из массива, поменяем его местами с последним элементом, после чего уменьшим длину массива на один). Однако быстро находить минимум не получится: можно пытаться поддерживать указатель на минимальный элемент, но после каждого удаления минимума новый минимум получится найти лишь перебором всех оставшихся элементов за $\\Theta(n)$ (где $n$ - число элементов в множестве). Можно было бы поддерживать массив в отсортированном порядке (тогда минимум всегда будет в начале), но тогда не получится быстро добавлять или удалять произвольный элемент.",
    "tags": [],
    "title": "7. Двоичная куча",
    "uri": "/basics/heap/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Хеш-таблица Пусть мы хотим поддерживать множество $A$ элементов - ключей, то есть уметь добавлять ключ в $A$, удалять ключ из $A$, а также искать ключ в $A$. Мы будем считать, что все ключи берутся из множества $U=\\{0,1, \\ldots,|U|-1\\}$.\n✍️ В общем случае ключами могут быть какие угодно объекты, которым можно каким-либо образом сопоставить числа (например, строки).\nЕсли $|U|$ не очень велико, можно просто создать массив размера $|U|$ и хранить каждый ключ в ячейке с номером, равным этому ключу. Чтобы понимать, каких ключей в $A$ нет, в соответствующих ячейках будем хранить специальное значение, не равное ни одному из ключей, например, -1 . Тогда все операции можно осуществлять за $O(1)$.\nНедостаток этого подхода в том, что он он использует $|U|$ памяти, и поэтому работает, только если множество $U$ не слишком велико. Кроме того, если в любой момент времени $|A| \\leqslant n$, и $n$ намного меньше $|U|$, то большая часть массива никак не будет использоваться, что непрактично.\nХеш-таблица позволяет решить ту же задачу, используя $\\Theta(n)$ памяти, и осуществляя все операции в среднем за $O(1)$. Идея состоит в том, чтобы использовать массив размера $m$, где $m$ намного меньше $|U|$, и ключ $x$ хранить в ячейке с номером $h(x)$, пользуясь хеш-функцией $h: U \\rightarrow M=\\{0,1, \\ldots, m-1\\}$.\nМожет случиться так, что $x \\neq y$, но $h(x)=h(y)$. Такую ситуацию мы будем называть коллизией. Хеш-функцию стараются выбрать так, чтобы минимизировать число коллизий. Тем не менее, поскольку $m\u003c|U|$, всегда найдётся пара ключей из $U$ с одинаковым значением хеш-функции, то есть какой бы хорошей ни была хеш-функция, коллизии иногда будут происходить.\nМетод цепочек Самый простой способ разрешения коллизий - в каждой ячейке массива хранить “цепочку” - список всех ключей, попавших в эту ячейку. Этот список можно реализовывать как с помощью связного списка, так и с помощью динамического массива, или даже ещё одной внутренней хеш-таблицы (чуть позже мы увидим пример, когда именно такой способ оказывается полезен).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 vector\u003cint\u003e T[m] # хеш-таблица find(x): # возвращает True, если x лежит в хеш-таблице p = h(x) for i = 0..(T[p].size() - 1): if T[p][i] == x: return True return False insert(x): if not find(x): T[h(x)].push_back(x) delete(x): p = h(x) for i = 0..(T[p].size() - 1): if T[p][i] == x: swap(T[p][i], T[p].back()) T[p].pop_back() break Все операции работают за время, пропорциональное длине списка в соответствующей ячейке, то есть за $O(|T[h(x)]|+1)$. Если ключи распределены по таблице равномерно, то есть в каждом списке оказалось примерно $\\frac{n}{m}$ элементов, то операции будут работать в среднем за $O\\left(\\frac{n}{m}+1\\right)$, то есть за $O(1)$ при $m=\\Omega(n)$ (мы докажем подобное утверждение более формально, когда будем рассматривать технику универсального хеширования).\nКак добиться равномерного распределения? Подобрать “хорошую” хеш-функцию. Иногда это можно сделать, пользуясь информацией о том, с какими ключами придётся работать.\nТак, если известно, что ключи выбираются из множества $U$ случайно равновероятно, и $|U|$ делится на $m$, то подойдёт хеш-функция $h(x)=x \\bmod m$ : каждый ключ попадёт в каждую ячейку таблицы с равной вероятностью.\nНа практике часто используют хеш-функцию $h(x)=x \\bmod m$, даже если о ключах, с которыми придётся работать, ничего не известно. При этом $m$ обычно стараются выбирать простым.\nОткрытая адресация Поговорим о ещё одном способе разрешения коллизий. В хеш-таблицах с открытой адресацией в каждой ячейке хранится не более одного ключа, а при поиске ключа ячейки проверяются в некотором порядке, пока не найдётся этот ключ или пустая ячейка. Этот порядок, конечно же, будет зависеть от того, какой ключ мы ищем. Поскольку в каждой ячейке хранится не более одного ключа, в таблице размера $m$ не может храниться больше $m$ ключей.\nБолее формально, теперь мы будем работать с хеш-функцией от двух аргументов $h: U \\times\\{0, \\ldots, m-1\\} \\rightarrow\\{0, \\ldots, m-1\\}$, и при поиске $x$ перебирать ячейки в порядке $h(x, 0), h(x, 1), \\ldots, h(x, m-1)$. При этом мы будем требовать, чтобы эта последовательность (будем называть её последовательностью проб) была перестановкой чисел от 0 до $m-1$ (чтобы не проверять одну ячейку несколько раз, и чтобы рано или поздно проверить каждую ячейку).\nПоследовательность проб чаще всего строят одним из следующих трёх способов (здесь $h^{\\prime}, h_{1}, h_{2}: U \\rightarrow\\{0, \\ldots, m-1\\}$ - вспомогательные хеш-функции):\nЛинейное пробирование. $h(x, i)=\\left(h^{\\prime}(x)+c \\cdot i\\right) \\bmod m$. Обычно используют $c=1$. Квадратичное пробирование. $h(x, i)=\\left(h^{\\prime}(x)+c_{1} \\cdot i+c_{2} \\cdot i^{2}\\right) \\bmod m$. Двойное хеширование. $h(x, i)=\\left(h_{1}(x)+i \\cdot h_{2}(x)\\right) \\bmod m$. При этом $c$ в первом способе и $h_{2}(x)$ в третьем должны быть взаимно просты с $m$ (чтобы последовательность пробегала все ячейки). Во втором способе по тем же причинам тоже подойдут не все $c_{1}, c_{2}$.\nЛинейное пробирование самое простое и имеет наименьшую константу во времени работы, но страдает от кластеризации: блоки из лежащих в таблице подряд ключей со временем становятся всё больше и больше (потому что всё больше и больше становится вероятность попасть в такой блок).\nДвойное хеширование отличается тем, что может дать $\\Theta\\left(m^{2}\\right)$ различных последовательностей (в первых двух способах вся последовательность проб однозначно определяется по $h^{\\prime}(x)$, поэтому всего возможно $\\Theta(m)$ различных последовательностей), поэтому менее подвержено кластеризации. Квадратичное пробирование - некий компромисс между линейным пробированием и двойным хешированием.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int T[m] # Хеш-таблица # считаем, что ключи - неотрицательные целые числа, # в пустых ячейках лежит -1 getIndex(x): # возвращает индекс ячейки, в которой лежит x, # или индекс первой встретившейся пустой ячейки for i = 0..(m-1): p = h(x, i) if T[p] == x or T[p] == -1: return p error \"Hash table is full\" find(x): # возвращает True, если x лежит в хеш-таблице return T[getIndex(x)] == x insert(x): p = getIndex(x) if T[p] != -1: return # x уже есть в таблице T[p] = x Удалить ключ из хеш-таблицы теперь не так просто. Если просто пометить ячейку, где лежал ключ, как свободную, то поиск других ключей может перестать работать корректно (потому что при вставке другого ключа мы могли проходить через эту ячейку при поиске свободного места). Можно при удалении специальным образом помечать ячейку и пропускать её при дальнейшей работе с таблицей, но такую ячейку никогда нельзя будет переиспользовать.\n1 2 3 4 delete(x): p = getIndex(x) if T[p] != -1: T[p] = -2 # getIndex всегда будет пропускать такую ячейку $$ 1+\\alpha(1+\\alpha(1+\\alpha(1+\\cdots))) \\leqslant \\frac{1}{1-\\alpha} $$Мы эвристически оценили время работы операций в среднем как $O\\left(\\frac{1}{1-\\alpha}\\right)$. На практике хеш-таблицы с открытой адресацией действительно работают быстро, пока $\\alpha$ достаточно далеко от единицы. Когда $\\alpha$ приближается к единице (скажем, когда $\\alpha\u003e\\frac{2}{3}$ ), можно построить новую таблицу вдвое большего размера, и перенести все элементы в неё (удалённые элементы при этом, конечно, переносить не надо).\nАссоциативный массив В хеш-таблице можно хранить не просто ключи, а пары (ключ, значение). Получится ассоциативный массив - массив с произвольными индексами.\nВ С++ есть встроенные реализации хеш-таблицы и ассоциативного массива - это std::unordered_set, std::unordered_map.\nОтметим, что множество ключей хеш-таблицы (и реализации ассоциативного массива через хеш-таблицу) никак не упорядочено.\nВ С++ есть и реализации упорядоченного множества и ассоциативного массива с упорядоченными ключами - std::set, std::map. Эти структуры позволяют совершать больше различных операций (например, находить следующий по значению ключ). Однако они устроены сильно сложнее (мы поговорим об их устройстве позже), а операции работают медленнее: даже поиск ключа требует в худшем случае $\\Theta(\\log n)$ времени.\nСортировка Киркпатрика-Рейша С помощью хеш-таблицы можно соптимизировать поразрядную сортировку, улучшив время работы до $O(n \\log \\log C)$ на массиве из $n$ целых чисел от 0 до $C-1$.\nАлгоритм (Kirkpatrick, Reisch, 1984) выглядит следующим образом: пусть $C=2^{2^{k}}$ (если надо, округлим $C$ вверх до ближайшего числа такого вида, при этом $\\log \\log C$ увеличится не более, чем на один). Если $C \\leqslant n$, просто отсортируем числа подсчётом за $O(n)$, иначе представим каждое число в $\\sqrt{C}$-ичной системе счисления; при этом каждое число будет состоять из двух $2^{k-1}$-битных цифр ( $\\sqrt{C}=2^{2^{k-1}}$ ). Осталось отсортировать числа по старшей цифре, а при равенстве по младшей рекурсивными вызовами.\nХеш-таблицы (точнее, ассоциативные массивы) нужны, чтобы сгруппировать числа в блоки с равной старшей цифрой.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 sort(vector\u003cint\u003e a, int k): n = a.size() if n \u003e= 2 ** (2 ** k): countingSort(a) return vector\u003cint\u003e l # сюда запишем все встретившиеся старшие цифры unordered_map\u003cint, vector\u003cint\u003e \u003e t # ассоциативный массив: в t[x] будем хранить список всех таких у, # что a[i] = x * 2 ** (2 ** (k - 1)) + y для некоторого i for i = 0..(n - 1): x = a[i] / (2 ** (2 ** (k - 1))) # x - старшие 2 ** (k - 1) 6ит a[i] y = a[i] % (2 ** (2 ** (k - 1))) # y - младшиие 2 ** (k - 1) бит а[i] if t.find(x) == t.end(): # x встретилось впервые l.push_back(x) t[x].push_back(y) sort(l, k - 1) # отсортировали старшие цифры a.clear() for (x in l): # перебираем старшие цифры в порядке возрастания r = t[x] i = maxElementIndex(r) # найдём индекс максимального элемента в r rmax = r[i] swap(r[i], r.back()) r.pop_back() sort(r, k - 1) # отсортировали все младшие цифры, кроме максимальной for (y in r): a.push_back(x * 2 ** (2 ** (k - 1)) + y) a.push_back(x * 2 ** (2 ** (k - 1)) + rmax) Получаем рекуррентное соотношение\n$$ T(n)=\\Theta(n)+T(|l|)+\\sum_{x \\in l} T(|t[x]|-1) $$Заметим, что\n$$ |l|+\\sum_{x \\in l}(|t[x]|-1)=|l|-|l|+\\sum_{x \\in l}|t[x]|=n, $$то есть на каждом уровне рекурсии суммарный размер подзадач равен $n$. При этом глубина рекурсии не превышает $k$, значит суммарно на всех уровнях совершено $O(k n)=$ $O(n \\log \\log C)$ действий. Отметим, что это оценка времени работы в среднем, поскольку мы используем хеш-таблицы.\nНа практике из-за большой константы во времени работы хеш-таблиц алгоритм как правило оказывается не быстрее обычных алгоритмов сортировки.\n✍️ На похожей идее основано дерево ван Эмде Боаса (van Emde Boas, 1975), позволяющее делать те же операции, что и двоичная куча, на целых числах от 0 до $C-1$ в среднем за $O(\\log \\log C)$. На практике оно практически не используется по тем же причинам\nУниверсальное хеширование Можно ли раз и навсегда выбрать для хеш-таблицы фиксированную хеш-функцию $h: U \\rightarrow M$, которая будет хорошо работать на любых входных данных? Нет: поскольку $|U|\u003e|M|$, всегда найдутся $\\lceil|U| /|M|\\rceil$ ключей с одинаковым значением хеш-функции (причём отношение $|U| /|M|$, как правило, достаточно велико). Тогда, если в запросах будет встречаться много таких ключей (например, если злоумышленник, знающий хеш-функцию, будет специально посылать такие запросы), эти запросы будут обрабатываться долго.\nЭту проблему можно решить следующим образом: до начала работы хеш-таблицы выберем хеш-функцию случайно из некоторого семейства. Если правильно подобрать семейство, запросы по-прежнему будут обрабатываться в среднем быстро, вне зависимости от того, какие ключи подаются на вход.\nОпределение Семейство $\\mathcal{H}$ хеш-функций, действующих из множества $U$ во множество $M=\\{0,1, \\ldots, m-1\\}$, называется универсальным, если для любой пары различных ключей $k, l \\in U$ количество таких хеш-функций $h \\in \\mathcal{H}$, что $h(k)=h(l)$, не превосходит $\\frac{|\\mathcal{H}|}{m}$.\nТеорема Универсальное хеширование Пусть хеш-функция $h \\in \\mathcal{H}, h: U \\rightarrow\\{0,1, \\ldots, m-1\\}$ была случайно выбрана из универсального семейства $\\mathcal{H}$, и использована при работе хеш-таблицы размера $m$. Пусть в хеш-таблицу уже были добавлены $n$ ключей $l_{1}, \\ldots, l_{n}$, коллизии разрешались методом цепочек. Тогда для любого ключа $k \\in U$ математическое ожидание длины списка в ячейке с индексом $h(k)$ не превосходит $1+\\frac{n}{m}$. Доказательство Для каждой пары ключей $a, b \\in U$ введём величину $\\chi(h, a, b)$, равную единице, если $h(a)=h(b)$, и нулю иначе. $\\chi(h, a, a)=1$ для любой $h$. При этом для $a \\neq b$ математическое ожидание $\\chi(h, a, b)$ не превосходит $\\frac{1}{m}$ по определению универсального семейства:\n$$ \\mathbb{E}(\\chi(h, a, b))=\\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\chi(h, a, b) \\leqslant \\frac{1}{|\\mathcal{H}|} \\cdot \\frac{|\\mathcal{H}|}{m}=\\frac{1}{m} $$Обозначим $L=\\left\\{l_{1}, \\ldots, l_{n}\\right\\}$. Математическое ожидание длины списка в ячейке с индексом $h(k)$ равняется\n$$ \\mathbb{E}\\left(\\sum_{l \\in L} \\chi(h, k, l)\\right)=\\sum_{l \\in L} \\mathbb{E}(\\chi(h, k, l)) \\leqslant 1+\\sum_{l \\in L \\backslash\\{k\\}} \\mathbb{E}(\\chi(h, k, l)) \\leqslant 1+\\frac{n}{m} $$ Следствие Пусть хеш-таблица размера $m$ использует технику универсального хеширования и метод цепочек для разрешения коллизий. Математическое ожидание суммарного времени работы $k$ операций $\\operatorname{insert}$, $\\operatorname{find}$, $\\operatorname{delete}$, среди которых $O(m)$ операций $\\operatorname{insert}$, есть $\\Theta(k)$. Доказательство В любой момент времени в таблице лежат не более $O(m)$ элементов. Время работы любой операции не превосходит времени вычисления хеш-функции (которое мы считаем константным) и времени прохода по списку в ячейке с индексом, равным значению хеш-функции. По предыдущей теореме, математическое ожидание времени выполнения каждой операции не превосходит $O\\left(1+\\frac{O(m)}{m}\\right)=O(1)$. Тогда математическое ожидание суммарного времени работы всех операций не превосходит $O(k)$, то есть равняется $\\Theta(k)$.\nЕщё раз отметим, что теорема и следствие выполняются для любых последовательностей запросов; математическое ожидание берётся не по входным данным, а по случайному выбору хеш-функции.\nПостроение универсального семейства хеш-функций Построим универсальное семейство хеш-функций для чисел, влезающих в машинное слово (считаем, что арифметические операции над числами выполняются за $O(1)$ ).\nТеорема Пусть $p$ - такое простое, что $U \\subset\\{0,1, \\ldots, p-1\\}$; пусть $m \u003c p$. Для любых целых $1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p$ определим хеш-функцию\n$$ h_{a, b}(k)=((a k+b) \\bmod p) \\bmod m $$Тогда семейство хеш-функций\n$$ \\mathcal{H}_{p, m}=\\left\\{h_{a, b}: 1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p\\right\\} $$является универсальным.\nДоказательство Пусть $k \\neq l$ - два различных ключа. Поймём, для каких $a, b$ верно $h_{a, b}(k)=h_{a, b}(l)$.\nЗаметим сначала, что если $r=(a k+b) \\bmod p, s=(a l+b) \\bmod p$, то $r \\neq s$, так как $r-s \\equiv a(k-l) \\not \\equiv 0(\\bmod p)$.\nБолее того, разным парам $(a, b)$ соответствуют разные пары $(r, s)$, так как по $(r, s)$ можно восстановить $(a, b): a \\equiv(r-s)(k-l)^{-1}(\\bmod p), b \\equiv(r-a k)(\\bmod p)$.\nРазличных пар $(a, b)$ с $1 \\leqslant a \u003c p, 0 \\leqslant b \u003c p$ всего $(p-1) \\cdot p$. Различных пар $(r, s)$ с $0 \\leqslant r \\neq s \u003c p$ всего $p^{2}-p$, то есть столько же. Значит, мы установили взаимно однозначное соответствие между множествами пар ( $a, b$ ) и пар $(r, s)$.\nОстаётся заметить, что $h_{a, b}(k)=h_{a, b}(l)$ тогда и только тогда, когда $r \\equiv s(\\bmod m)$. Для фиксированного $0 \\leqslant r \u003c p$ количество $0 \\leqslant s \u003c p$ таких, что $s \\neq r$, но $r \\equiv s(\\bmod m)$, не превосходит $\\left\\lceil\\frac{p}{m}\\right\\rceil-1 \\leqslant \\frac{p+m-1}{m}-1=\\frac{p-1}{m}$.\nТогда количество хеш-функций $h_{a, b} \\in \\mathcal{H}_{p, m}$, для которых $h_{a, b}(k)=h_{a, b}(l)$ равняется количеству пар $(r, s)$ таких, что $0 \\leqslant r \\neq s \u003c p, r \\equiv s(\\bmod m)$, которое не превосходит $p \\cdot \\frac{p-1}{m}=\\frac{\\left|\\mathcal{H}_{p, m}\\right|}{m}$.\nСовершенное хеширование Если множество используемых ключей известно заранее (например, в языках программирования множество зарезервированных слов фиксировано), то можно построить хеш-таблицу, операции в которой будут работать за $O(1)$ в худшем случае. При этом можно добиться того, чтобы хеш-таблица имела размер $O(n)$, где $n$ - число используемых ключей.\nСнова считаем, что все ключи - целые неотрицательные числа, меньшие некоторого простого $p$. Построим хеш-таблицу с $m=n$ ячейками, при этом хеш-функцию аккуратно выберем из универсального семейства $\\mathcal{H}_{p, m}$. В каждой ячейке этой таблицы вместо списка мы заведём хеш-таблицу второго уровня, причём, если в ячейку с индексом $j$ попало $n_{j}$ ключей, то внутреннюю хеш-таблицу в этой ячейке сделаем размера $m_{j}=n_{j}^{2}$. Хешфункцию для внутренней таблицы мы аккуратно выберем из универсального семейства $\\mathcal{H}_{p, m_{j}}$.\nОказывается, можно подобрать такие хеш-функции, что во внутренних таблицах не будет коллизий, а суммарный размер всех таблиц будет оцениваться как $O(n)$.\nВ доказательстве оценок мы будем пользоваться следующим фактом (под $\\mathbb{P}(A)$ имеется в виду вероятность того, что произошло событие $A$ ):\nНеравенство Маркова Пусть $X$ - неотрицательная случайная величина с конечным математическим ожиданием. Тогда для любого $a\u003e0$ верно\n$$ \\mathbb{P}(X \\geqslant a) \\leqslant \\frac{\\mathbb{E} X}{a} $$ Доказательство Поскольку $X$ неотрицательно, $\\mathbb{E} X \\geqslant 0 \\cdot \\mathbb{P}(X \u003c a)+a \\cdot \\mathbb{P}(X \\geqslant a)$, откуда следует требуемое неравенство.\nТеорема Пусть в хеш-таблице размера $m=n^{2}$ хранятся $n$ ключей, причём хешфункция $h(\\cdot)$ была случайно выбрана из $\\mathcal{H}_{p, m}$. Тогда с вероятностью более чем $\\frac{1}{2}$ коллизий нет (все ключи хранятся в разных ячейках). Доказательство Для каждой пары различных ключей введём величину $\\chi(h, a, b)$, равную единице, если $h(a)=h(b)$, и нулю иначе. Как и в теореме Универсального хеширования 8.6.1, $\\mathbb{E}(\\chi(h, a, b)) \\leqslant \\frac{1}{m}=\\frac{1}{n^{2}}$. Рассмотрим случайную величину $X$ - количество коллизий. Тогда\n$$ \\mathbb{E} X=\\mathbb{E}\\left(\\sum_{a \\neq b}(\\chi(h, a, b))\\right)=\\sum_{a \\neq b} \\mathbb{E}(\\chi(h, a, b)) \\leqslant \\frac{n(n-1)}{2} \\cdot \\frac{1}{n^{2}}\u003c\\frac{1}{2} $$Поскольку $X$ принимает только целые неотрицательные значения, по неравенству Маркова\n$$ \\mathbb{P}(X\u003e0)=\\mathbb{P}(X \\geqslant 1) \\leqslant \\mathbb{E} X\u003c\\frac{1}{2} $$ Из этой теоремы сразу же следует, что если мы можем позволить себе использовать хеш-таблицу размера $n^{2}$, то, сделав несколько попыток, мы подберём такую хеш-функцию, что все ключи попадут в разные ячейки таблицы (вероятность того, что мы не найдём такую хеш-функцию за $s$ попыток, меньше $2^{-s}$; в среднем понадобится не более двух попыток). Наша двухуровневая схема нужна, чтобы уменьшить количество используемой памяти до $O(n)$.\nТе же рассуждения показывают, что, сделав несколько попыток, мы подберём хешфункцию без коллизий для каждой внутренней хеш-таблицы. Осталось понять, почему суммарный размер таблиц можно сделать линейным от числа ключей.\nТеорема Пусть в хеш-таблице размера $m=n$ хранятся $n$ ключей, причём хешфункция $h(\\cdot)$ была случайно выбрана из $\\mathcal{H}_{p, m}$. Пусть в $j$-ю ячейку попало $n_{j}$ ключей, которые были помещены во внутреннюю хеш-таблицу размера $m_{j}=n_{j}^{2}$. Тогда математическое ожидание суммарного размера внутренних хеш-таблиц меньше $2 n$. Доказательство Заметим сначала, что\n$$ \\begin{aligned} \u0026 \\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right)=\\mathbb{E}\\left(\\sum_{j=0}^{m-1} n_{j}^{2}\\right)=\\mathbb{E}\\left(\\sum_{j=0}^{m-1}\\left(n_{j}+2 \\cdot \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right)\\right)= \\\\ = \u0026 \\mathbb{E}\\left(\\sum_{j=0}^{m-1} n_{j}\\right)+2 \\cdot \\mathbb{E}\\left(\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right)=n+2 \\cdot \\mathbb{E}\\left(\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}\\right) . \\end{aligned} $$Теперь заметим, что $\\sum_{j=0}^{m-1} \\frac{n_{j}\\left(n_{j}-1\\right)}{2}$ - это суммарное количество случившихся коллизий. Из доказательства предыдущей теоремы мы знаем, что математическое ожидание количества коллизий не превосходит $\\frac{n(n-1)}{2} \\cdot \\frac{1}{m}=\\frac{n-1}{2}$, так как $m=n$. Получаем\n$$ \\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right) \\leqslant n+2 \\cdot \\frac{n-1}{2}=2 n-1\u003c2 n . $$ Следствие В предположениях теоремы суммарный размер внутренних хеш-таблиц окажется больше или равен $4 n$ с вероятностью меньше $\\frac{1}{2}$. Доказательство Воспользуемся неравенством Маркова:\n$$ \\mathbb{P}\\left(\\sum_{j=0}^{m-1} m_{j} \\geqslant 4 n\\right) \\leqslant \\frac{\\mathbb{E}\\left(\\sum_{j=0}^{m-1} m_{j}\\right)}{4 n}\u003c\\frac{2 n}{4 n}=\\frac{1}{2} $$ Снова получаем, что за $s$ попыток с вероятностью хотя бы $1-2^{-s}$ (и не больше чем за две попытки в среднем) мы подберём такую хеш-функцию для внешней хеш-таблицы, что суммарный размер всех внутренних хеш-таблиц будет оцениваться как $O(n)$ (будет меньше $4 n$ ).",
    "description": "Хеш-таблица Пусть мы хотим поддерживать множество $A$ элементов - ключей, то есть уметь добавлять ключ в $A$, удалять ключ из $A$, а также искать ключ в $A$. Мы будем считать, что все ключи берутся из множества $U=\\{0,1, \\ldots,|U|-1\\}$.\n✍️ В общем случае ключами могут быть какие угодно объекты, которым можно каким-либо образом сопоставить числа (например, строки).\nЕсли $|U|$ не очень велико, можно просто создать массив размера $|U|$ и хранить каждый ключ в ячейке с номером, равным этому ключу. Чтобы понимать, каких ключей в $A$ нет, в соответствующих ячейках будем хранить специальное значение, не равное ни одному из ключей, например, -1 . Тогда все операции можно осуществлять за $O(1)$.",
    "tags": [],
    "title": "8. Хеширование",
    "uri": "/basics/hashing/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Начиная с этой части, при оценке сложности алгоритмов будем использовать обозначение $M(n)$, имея в виду сложность умножения двух чисел длины $n$. Можно показать (с помощью метода Ньютона), что деление нацело имеет ту же сложность, что и умножение, поэтому для деления отдельного обозначения вводить не будем.\nСложение и умножение по модулю Сложение и умножение по $n$-битному модулю $N$ имеет ту же сложность, что и обычные сложение и умножение, то есть $O(n)$ и $O(M(n))$ : нужно произвести вычисление без модуля, при этом результат будем иметь не более $2 n$ бит, после чего вычислить остаток от деления результата на $N$. В случае сложения результат меньше $2 N$, поэтому достаточно просто (возможно) вычесть $N$, что требует ещё $O(n)$ операций и не увеличивает оценку времени работы. В случае умножения осуществляем деление с остатком за $O(M(n))$.\nВозведение в степень по модулю $N$ Воспользуемся той же идеей, что и в рекурсивном алгоритме умножения:\n$$ a^{b}= \\begin{cases}\\left(a^{\\left\\lfloor\\frac{b}{2}\\right\\rfloor}\\right)^{2}, \u0026 \\text { если } b \\text { чётно, } \\\\ a \\cdot\\left(a^{\\left\\lfloor\\frac{b}{2}\\right\\rfloor}\\right)^{2}, \u0026 \\text { иначе. }\\end{cases} $$ 1 2 3 4 5 6 7 8 modExp(a, b, N): # a и b - двоичные записи чисел, N - модуль if b == 0: return 1 c = modExp(a, b / 2, N) # деление нацело if b % 2 == 0: return c ** 2 mod N else: return a * c ** 2 mod N Если $n$ - максимальная длина чисел $a, b, N$, то происходит $O(n)$ рекурсивных вызовов, на каждом из которых не более двух умножений по модулю $N$. Получаем оценку сложности $O(n \\cdot M(n))$.\nАлгоритм Евклида Алгоритм Евклида находит наибольший общий делитель (НОД, greatest common divisor, $\\operatorname{gcd}$) двух чисел $a$ и $b$.\nЛемма Для любых $a, b \\geqslant 0$ выполняется $\\operatorname{gcd}(a, b)=\\operatorname{gcd}(a \\bmod b, b)$. Доказательство Ясно, что любой общий делитель $a$ и $b$ является и делителем $a-b$. Наоборот, любой общий делитель $a-b$ и $b$ является делителем $a$. Тогда $\\operatorname{gcd}(a, b)=$ $\\operatorname{gcd}(a-b, b)$. Остаётся $\\left\\lfloor\\frac{a}{b}\\right\\rfloor$ раз вычесть $b$ из $a$.\nАлгоритм Евклида пользуется вышеописанным правилом, пока не окажется, что $b=0$. Ясно, что $\\operatorname{gcd}(a, 0)=a$ для любого $a$.\n1 2 3 4 5 6 7 8 9 gcd(a, b): # a, b \u003e= 0 if b == 0: return a return gcd(b, a % b) gcd(a, b): # нерекурсивная версия while b \u003e 0: a %=b swap(a, b) return a Оценим время работы алгоритма:\nЛемма Если $a \\geqslant b \\geqslant 0$, то $a \\bmod b\u003c\\frac{a}{2}$.\nДоказательство Если $b \\leqslant \\frac{a}{2}$, то $a \\bmod b \u003c b \\leqslant \\frac{a}{2}$. Если $b \u003e \\frac{a}{2}$, то $a \\bmod b=a-b \u003c \\frac{a}{2}$.\nСледствие Время работы алгоритма Евклида на числах длины не более $n$ есть $O(n \\cdot M(n))$.\nДоказательство За каждые две итерации алгоритма $a$ и $b$ уменьшаются хотя бы вдвое, поэтому число итераций не превосходит $2 n$. На каждой происходит одно деление с остатком, имеющее сложность $O(M(n))$.\nЕсли использовать алгоритм деления в столбик, можно показать оценку получше:\nТеорема Время работы использующего деление в столбик алгоритма Евклида на числах длины не более $n$ есть $O\\left(n^{2}\\right)$. Доказательство Вспомним, что если мы пользуемся алгоритмом деления в столбик, то остаток от деления $k$-битного числа $a$ на $l$-битное число $b$ вычисляется за $O(k(k-l+1)$ ).\nПусть в процессе выполнения алгоритма Евклида мы работали с числами $z_{1}=a, z_{2}=$ $b, z_{3}=a \\bmod b, \\ldots, z_{k+1}=z_{k-1} \\bmod z_{k}=0$, имеющими длины $n_{1}, n_{2}, \\ldots, n_{k}, n_{k+1}$. Суммарное время выполнения всех делений -\n$$ \\sum_{i=1}^{k-1} O\\left(n_{i}\\left(n_{i}-n_{i+1}+1\\right)\\right)=O\\left(n k+n \\cdot \\sum_{i=1}^{k-1}\\left(n_{i}-n_{i+1}\\right)\\right)=O\\left(n k+n\\left(n_{1}-n_{k}\\right)\\right)=O\\left(n^{2}\\right) $$ Расширенный алгоритм Евклида Расширенный алгоритм Евклида одновременно с $d=\\operatorname{gcd}(a, b)$ находит такие $x$ и $y$, что $a x+b y=d$. Эти $x$ и $y$ можно использовать как сертификат, подтверждающий, что $d$ действительно НОД $a$ и $b$ :\nЛемма Если $a$ и $b$ делятся на $d$, и $d=a x+b y$ для некоторых $x, y$, то $d=\\operatorname{gcd}(a, b)$.\nДоказательство $a$ и $b$ делятся на $d$, тогда $d \\leqslant \\operatorname{gcd}(a, b)$. С другой стороны $a$ и $b$ делятся на $\\operatorname{gcd}(a, b)$, тогда и $d=a x+b y$ делится на $\\operatorname{gcd}(a, b)$, то есть $d \\geqslant \\operatorname{gcd}(a, b)$.\nТакие $x$ и $y$ всегда можно найти следующим алгоритмом:\n1 2 3 4 5 extendedEuclid(a, b): # возвращает x,y такие, что ax + by = gcd(a, b) if b == 0: return 1, 0 x, y = extendedEuclid(b, a % b) return y, x - (a / b) * y # деление нацело Следствие Вышеописанный алгоритм возвращает такие $x, y$, что $a x+b y=$ $\\operatorname{gcd}(a, b)$. Доказательство Докажем утверждение индукцией по $b$.\nБаза. Если $b=0$, то $a=a \\cdot 1+b \\cdot 0$.\nПереход. Пусть рекурсивный вызов вернул $x, y$. По предположению индукции выполняется равенство $\\operatorname{gcd}(b, a \\bmod b)=b \\cdot x+(a \\bmod b) \\cdot y$.\nТогда $\\operatorname{gcd}(a, b)=\\operatorname{gcd}(b, a \\bmod b)=b \\cdot x+(a \\bmod b) \\cdot y=b \\cdot x+\\left(a-\\left\\lfloor\\frac{a}{b}\\right\\rfloor b\\right) \\cdot y=$ $a \\cdot y+b \\cdot\\left(x-\\left\\lfloor\\frac{a}{b}\\right\\rfloor y\\right)$.\nЯсно, что число итераций этого алгоритма совпадает с числом итераций обычного алгоритма Евклида (так как рекурсивные переходы устроены точно так же). Чтобы оценить время работы расширенного алгоритма Евклида, нужно ещё понять, насколько большими могут быть $x$ и $y$ :\nСледствие Если $a, b\u003e0$, то для возвращаемых алгоритмом $x$ и $y$ верно, что $|x| \\leqslant b,|y| \\leqslant a$. Доказательство Снова воспользуемся индукцией по $b$.\nБаза. При $b=0$ алгоритм возвращает $x=1, y=0$ (случай $b=0$ не подходит под условие предложения, но нужен нам как база индукции).\nПереход. Если $a \\bmod b=0$, то extendedEuclid(b, a mod b) вернул $x^{\\prime}=1, y^{\\prime}=0$. Тогда extendedEuclid(a, b) вернёт $x=0 \\leqslant b, y=1 \\leqslant a$.\nЕсли же $a \\bmod b \\neq 0$, то по предположению индукции extendedEuclid(b, a mod b) вернул такие $x^{\\prime}, y^{\\prime}$, что $\\left|x^{\\prime}\\right| \\leqslant a \\bmod b,\\left|y^{\\prime}\\right| \\leqslant b$. Тогда extendedEuclid(a, b) вернёт $|x|=\\left|y^{\\prime}\\right| \\leqslant b,|y|=\\left|x^{\\prime}-\\left\\lfloor\\frac{a}{b}\\right\\rfloor y^{\\prime}\\right| \\leqslant\\left|x^{\\prime}\\right|+\\left\\lfloor\\frac{a}{b}\\right\\rfloor\\left|y^{\\prime}\\right| \\leqslant a \\bmod b+\\left\\lfloor\\frac{a}{b}\\right\\rfloor b=a$.\nПусть длина битовой записи $a$ и $b$ не превосходит $n$. Поскольку $a$ и $b$ только уменьшаются при рекурсивных вызовах, ясно, что все числа, возникающие в процессе промежуточных вычислений, имеют длину не более $n$. Теперь уже ясно, что оценка времени работы алгоритма Евклида $O(n \\cdot M(n))$ остаётся верной и для расширенной версии.\nПокажем, что остаётся верной и оценка $O\\left(n^{2}\\right)$ :\nСледствие Время работы использующего деление в столбик расширенного алгоритма Евклида на числах длины не более $n$ есть $O\\left(n^{2}\\right)$. Доказательство Для того, чтобы повторить доказательство теоремы 9.3, достаточно показать, что суммарное время работы всех операций, кроме рекурсивного вызова, в extendedEuclid(a,b) на числах длины $n$ и $m$ соответственно, есть $O(n(n-m+1))$.\nЧастное $\\left\\lfloor\\frac{a}{b}\\right\\rfloor$ можно вычислить одновременно с остатком $a \\bmod b$ за $O(n(n-m+1))$. Также нужно произвести умножение $\\left\\lfloor\\frac{a}{b}\\right\\rfloor$ на $y$, но, поскольку длина $\\left\\lfloor\\frac{a}{b}\\right\\rfloor$ не превосходит $n-m+1$, а длина $y$ не превосходит $n$, это умножение имеет ту же сложность, что и деление выше. Помимо умножения и деления, происходит ещё лишь линейное от $n$ число действий.\nНахождение обратного по модулю $N$ Число, обратное к $а$ по модулю $N\\left(\\right.$ об. $\\left.a^{-1}\\right)$ - это такое $x$, что $a x \\equiv 1(\\bmod N)$. Такое $x$ существует тогда и только тогда, когда $a x+N y=1$ для некоторого $y$. Как мы уже знаем, такое уравнение имеет решение только когда $\\operatorname{gcd}(a, N)=1$, то есть когда $a$ взаимно просто с $N$.\nС помощью расширенного алгоритма Евклида мы можем за $O\\left(n^{2}\\right)$ проверить, существует ли обратное к $a$ по модулю $N$, и если существует, то найти его.\nЕсли для числа $a$ существует обратное по модулю $N$ число $a^{-1}$, то можно осуществлять деление на $a$ по модулю $N$ : это равносильно умножению на $a^{-1}$.",
    "description": "Начиная с этой части, при оценке сложности алгоритмов будем использовать обозначение $M(n)$, имея в виду сложность умножения двух чисел длины $n$. Можно показать (с помощью метода Ньютона), что деление нацело имеет ту же сложность, что и умножение, поэтому для деления отдельного обозначения вводить не будем.\nСложение и умножение по модулю Сложение и умножение по $n$-битному модулю $N$ имеет ту же сложность, что и обычные сложение и умножение, то есть $O(n)$ и $O(M(n))$ : нужно произвести вычисление без модуля, при этом результат будем иметь не более $2 n$ бит, после чего вычислить остаток от деления результата на $N$. В случае сложения результат меньше $2 N$, поэтому достаточно просто (возможно) вычесть $N$, что требует ещё $O(n)$ операций и не увеличивает оценку времени работы. В случае умножения осуществляем деление с остатком за $O(M(n))$.",
    "tags": [],
    "title": "9. Арифметика сравнений",
    "uri": "/basics/numerical_algos_arithmetic_comparison/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Решето Эратосфена Решето Эратосфена - алгоритм, находящий все простые числа, не превосходящие некоторой границы $n$. Алгоритм рассматривает последовательно все числа от 2 до $n$. Если очередное число ещё не помечено как составное, он помечает его как простое, а все числа, делящиеся на него, помечает как составные.\nПоскольку у любого составного числа $x$ есть делитель, не больший корня из $x$, достаточно перебирать кратные простого числа, начиная с его квадрата.\n1 2 3 4 5 6 vector\u003cbool\u003e isPrime(n + 1, 1) isPrime[0] = isPrime[1] = 0 for i = 2..n: if isPrime[i]: for (j = i * i; j \u003c= n; j += i): isPrime[j] = 0 Время работы алгоритма есть\n$$ \\sum_{\\substack{p \\text {-простое, } \\\\ p \\leqslant n}} \\frac{n}{p}=n \\cdot\\left(\\sum_{\\substack{p-п р о с т о е, ~}}^{p \\leqslant n}\u003c~ \\frac{1}{p}\\right)=O(n \\log \\log n) $$поскольку\n$$ \\sum_{\\substack{p-\\text { простое, } \\\\ p \\leqslant n}} \\frac{1}{p}=O(\\log \\log n) $$(факт из теории чисел, который мы доказывать не будем).\nРешето с линейным временем работы Существует алгоритм со временем работы $O(n)$ (Gries, Misra, 1978), который устроен похожим образом, но помечает каждое составное число ровно один раз. Более того, для каждого числа, не большего $n$, он находит его наименьший простой делитель, что позволяет потом быстро факторизовать (разложить на простые множители) любое из чисел.\nОбозначим наименьший простой делитель $x$ за $p[x]$. Алгоритм помечает составное число $x$ в тот момент, когда он рассматривает число $\\frac{x}{p[x]}$. Заметим, что $p[x] \\leqslant p\\left[\\frac{x}{p[x]}\\right]$.\nЧтобы сделать все такие пометки, достаточно, рассматривая число $y$, пометить все числа вида $y q$, где $q$ - простое, меньшее или равное $p[y]$.\n1 2 3 4 5 6 7 8 vector\u003cint\u003e primes # список простых чисел vector\u003cint\u003e d(n + 1, -1) # d[i] - номер p[i] в списке простых for i = 2..n: if d[i] == -1: d[i] = sz(primes) primes.push_back(i) for (j = 0; j \u003c= d[i] and i * primes[j] \u003c= n; ++j): d[i * primes[j]] = j Перебор делителей Пусть теперь мы хотим проверить на простоту конкретное число $n$. Самый простой способ - это перебрать все числа, большие единицы, но меньшие $n$, и проверить, делится ли $n$ хоть на одно из них. На самом деле достаточно перебирать числа, не большие $\\sqrt{n}$, поскольку у составного $n$ точно найдётся нетривиальный делитель не больше $\\sqrt{n}$. Тем не менее, если $b$ - длина битовой записи $n$, такой алгоритм всё ещё будет иметь сложность, экспоненциально зависящую от $b$ : он совершит $\\Omega(\\sqrt{n})=\\Omega\\left(2^{b / 2}\\right)=\\Omega\\left((\\sqrt{2})^{b}\\right)$ итераций.\nВ криптографии часто возникает потребность в больших простых числах. Соответственно, хочется иметь алгоритм, умеющий проверять число на простоту за время, полиномиально зависящее от длины числа.\nТест Ферма Вспомним малую теорему Ферма:\nМалая теорема Ферма Для любого простого $p$ и любого $1 \\leqslant a \\leqslant p-1$ выполняется $a^{p-1} \\equiv 1(\\bmod p)$.\nНа основе этой теоремы можно проверять $n$ на простоту следующим тестом: возьмём случайное $1 \\leqslant a \\leqslant n-1$ и проверим, правда ли, что $a^{n-1} \\equiv 1(\\bmod n)$. Если сравнение не выполняется (то есть $n$ не прошло тест), то $n$ точно составное.\nПроблема в том, что из того, что сравнение выполняется, совсем не следует, что $n$ простое. Например, $341=31 \\cdot 11$ - составное, но $2^{340}=1024^{34} \\equiv 1(\\bmod 341)$. Можно пытаться бороться с этой проблемой, запуская тест несколько (s) раз с различными $a$.\n1 2 3 4 5 6 7 8 # возвращает False, если n составное; # возвращает True, если все проверки пройдены успешно FermatTest(n): for i = 0..(s - 1): a = random(1, n - 1) # случайное число от 1 до n - 1 if modExp(a, n - 1, n) != 1: return False # n точно составное return True # надеемся, что n простое Время работы теста Ферма на $b$-битном числе $n-O(s b \\cdot M(b))$.\nКак нам поможет несколько запусков теста? Заметим во-первых, что если $a$ окажется не взаимно просто с $n$ (то есть $\\operatorname{gcd}(a, n) \\neq 1)$, то $a^{n-1} \\not \\equiv 1(\\bmod n)$, потому что такое $a$ не обратимо по модулю $n$. Это замечание, однако, не очень полезно, потому что таких $a$ может быть мало (например, если $n$ - произведение двух больших простых чисел), тогда вероятность случайно наткнуться на них невелика.\nЗато можно доказать, что если $a^{n-1} \\not \\equiv 1(\\bmod n)$ хотя бы для одного $a$, взаимно простого с $n$, то сравнение не выполняется и для многих других $a$ тоже:\nНапомним, что $\\mathbb{Z}_{n}$ - кольцо вычетов по модулю $n ; \\mathbb{Z}_{n}^{*}$ - мультипликативная группа обратимых элементов кольца вычетов по модулю $n$.\nПредложение Пусть нашлось $a^{\\prime}$ такое, что $\\operatorname{gcd}\\left(a^{\\prime}, n\\right)=1,\\left(a^{\\prime}\\right)^{n-1} \\not \\equiv 1(\\bmod n)$. Тогда\n$$\\left|\\left\\{a: 1 \\leqslant a \u003c n, a^{n-1} \\not \\equiv 1 \\quad(\\bmod n)\\right\\}\\right| \\geqslant \\frac{n-1}{2}$$ Доказательство Рассмотрим множество из всех остальных $a$ :\n$$A=\\left\\{a: 1 \\leqslant a \u003c n, a^{n-1} \\equiv 1 \\quad(\\bmod n)\\right\\}$$Нужно показать, что $|A| \\leqslant \\frac{n-1}{2}$. Заметим, что $A$ - подгруппа $\\mathbb{Z}_{n}^{*}$. При этом по условию леммы $a^{\\prime} \\in \\mathbb{Z}_{n}^{*} \\backslash A$, поэтому $A-$ собственная подгруппа. Поскольку порядок подгруппы делит порядок группы, $|A| \\leqslant \\frac{\\left|\\mathbb{Z}_{n}^{*}\\right|}{2} \\leqslant \\frac{n-1}{2}$.\nТаким образом, если существует хоть одно $a$, взаимно простое с $n$ и такое, что $a^{n-1} \\not \\equiv 1(\\bmod n)$, то $n$ не пройдёт тест со случайным а с вероятностью хотя бы $\\frac{1}{2}$. Значит, алгоритм, запустивший тест $s$ раз, ошибётся и назовёт такое составное число $n$ простым с вероятностью не больше, чем $\\frac{1}{2^{s}}$. Скажем, при $s=50$ вероятность ошибки не превосходит $2^{-50}$, что уже пренебрежимо мало.\nЧисла Кармайкла K сожалению, существуют составные числа $n$, для которых сравнение $a^{n-1} \\equiv 1(\\bmod n)$ выполняется для всех $a$, взаимно простых с $n$. Такие $n$ называются числами Кармайкла. Наименьшее число Кармайкла равняется $561=3 \\cdot 11 \\cdot 17$. Тест Ферма поймёт, что число Кармайкла $n$ составное, только если случайно наткнётся на $a$, имеющее с $n$ общий делитель, вероятность чего мала.\nЧисел Кармайкла бесконечно много, но при этом они встречаются достаточно редко. Пусть $C(n)$ - количество чисел Кармайкла, не превосходящих $n$. Известно, что $C(n)\u003e$ $n^{2 / 7}$ для достаточно больших $n$, но, с другой стороны, $\\frac{C(n)}{n}$ стремится к нулю с ростом $n$. Поэтому большие простые числа иногда генерируют следующим образом: берут случайное число заданной длины, и проверяют его тестом Ферма (иногда даже не случайными $a$, а $a=2,3, \\ldots$ ). Если длина числа большая, то вероятность того, что оно оказалось числом Кармайкла, невелика, тогда теста Ферма вполне достаточно.\nЕсли же хочется проверить на простоту конкретное число, тестом Ферма не обойтись.\nТест Миллера-Рабина Тест Миллера-Рабина (Artjuhov, 1967; Miller, 1976; Rabin, 1980) делает более тонкую проверку, которая работает и в случае чисел Кармайкла.\nОпределение Назовём $a$ нетривиальным корнем из единицы по модулю $n$, если $a^{2} \\equiv 1(\\bmod n)$, но $a \\not \\equiv \\pm 1(\\bmod n)$.\nЛемма Нетривиальных корней по простому модулю $p$ не существует.\nДоказательство Доказательство. По модулю 2 есть всего два остатка - $0$ и $1 \\equiv-1$, ясно, что $0$ - не корень из единицы. Дальше считаем $p \\geqslant 3$.\nЕсли $a^{2} \\equiv 1(\\bmod p)$, то $(a-1)(a+1)$ делится на $p . \\operatorname{gcd}(a-1, a+1) \\leqslant 2$, а $p \\geqslant 3$, тогда либо $a-1$ делится на $p$, и $a \\equiv 1(\\bmod p)$, либо $a+1$ делится на $p$, и $a \\equiv-1$ $(\\bmod p)$.\nТест Миллера-Рабина помимо проверки условия из теоремы Ферма пытается найти нетривиальный корень из единицы по модулю $n$. Делается это следующим образом: пусть дано нечётное $n$ (чётные числа проверять на простоту очень просто). Из чётного числа $n-1$ выделяются все степени двойки: $n-1=2^{k} m$ для $k \\geqslant 1$ и нечётного $m$. После этого вычисляется последовательность $a^{m} \\bmod n, a^{2 m} \\bmod n, \\ldots, a^{2^{k} m} \\bmod n=a^{n-1} \\bmod n$.\nЕсли $a^{n-1} \\not \\equiv 1(\\bmod n)$, то, как и в тесте Ферма, мы делаем вывод, что число составное. Иначе найдём в последовательности первую единицу, и посмотрим на элемент перед ней. Если он есть (то есть последовательность состоит не только из единиц), и он не равен $-1 \\bmod n$, то мы нашли нетривиальный корень из единицы по модулю $n$, то есть $n$ - составное.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # возвращает False, если n составное; # возвращает True, если все проверки пройдены успешно MillerRabinTest(n): m}=\\textrm{n}-1,\\textrm{k}=0\\mathrm{ # найдём k, m такие, что n - 1 = m * 2 ** k while m % 2 == 0: m /= 2, k += 1 for i = 0..(s - 1): a = random(1, n - 1) # случайное число от 1 до n - 1 x = modExp(a, m, n) for j = 0..(k - 1): y = x * x mod n if y == 1 and x != 1 and x != n - 1: return False # нашли нетривиальный корень из единицы, т.е. n - составное x = y if x != 1: return False # не выполнилось условие теоремы Ферма, т.е. n - составное return True # надеемся, что n простое Время работы $s$ раундов теста Миллера-Рабина на $b$-битном числе - снова $O(s b \\cdot M(b))$.\nОценка вероятности ошибки Назовём $1 \\leqslant a \\leqslant n-1$ свидетелем непростоты $n$, если $n$ не проходит тест Миллера-Рабина при использовании $a$, то есть если $a^{n-1} \\not \\equiv 1(\\bmod n)$; либо если для некоторого $0 \\leqslant j",
    "description": "Решето Эратосфена Решето Эратосфена - алгоритм, находящий все простые числа, не превосходящие некоторой границы $n$. Алгоритм рассматривает последовательно все числа от 2 до $n$. Если очередное число ещё не помечено как составное, он помечает его как простое, а все числа, делящиеся на него, помечает как составные.\nПоскольку у любого составного числа $x$ есть делитель, не больший корня из $x$, достаточно перебирать кратные простого числа, начиная с его квадрата.\n1 2 3 4 5 6 vector\u003cbool\u003e isPrime(n + 1, 1) isPrime[0] = isPrime[1] = 0 for i = 2..n: if isPrime[i]: for (j = i * i; j \u003c= n; j += i): isPrime[j] = 0 Время работы алгоритма есть",
    "tags": [],
    "title": "10. Проверка на простоту и факторизация",
    "uri": "/basics/numerical_algos_prime_test_and_factorization/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Одна из классических задач криптографии звучит следующим образом. Есть Алиса и Боб, которые хотят поговорить без свидетелей. Также есть Ева (от англ. eavesdropper), которая их подслушивает. Алиса и Боб хотят, чтобы Ева, даже если она подслушает все сообщения, не поняла, о чём был разговор.\nБолее формально, пусть Алиса хочет послать Бобу секретное сообщение - битовую строку $x$. Она шифрует её с помощью некоторой функции $E(\\cdot)$ (от англ. encoder) и посылает Бобу зашифрованное сообщение $E(x)$. Боб использует функцию $D(\\cdot)$ (от англ. decoder), чтобы восстановить исходное сообщение: $D(E(x))=x$.\nАлиса и Боб хотят, чтобы Ева, даже подслушав $E(x)$, не получила никакой информации $\\circ x$.\nОбычно считается, что функции $E(\\cdot), D(\\cdot)$ вычисляются при помощи некоторых известных всем (в том числе Еве) алгоритмов, но результат вычислений зависит от некоторых параметров - ключей, известных только Алисе и Бобу. Это правило известно как принцип Керкгоффса (1883). Такая схема обмена сообщениями оказывается более гибкой: если бы вся сложность расшифровки сообщения состояла в том, что Еве неизвестны алгоритмы, которыми пользуются Алиса и Боб, то, если бы Ева откуда-то узнала эти алгоритмы, Алисе и Бобу пришлось бы с нуля договариваться о новой схеме. Если же алгоритмы известны всем, и Ева откуда-то узнаёт ключи, то достаточно поменять ключи, не меняя всю схему целиком.\nСхемы с закрытым ключом Схемы с закрытым ключом (также их называют симметричными схемами) используют один и тот же ключ для шифрования и расшифровки сообщения. Такие схемы подразумевают, что Алиса и Боб заранее договариваются о некотором секретном ключе, который больше никому не известен.\nОдноразовый блокнот В этом протоколе шифрования Алиса и Боб заранее встречаются и выбирают битовую строку $r$ той же длины, что и будущее сообщение $x$. Тогда шифрование сообщения состоит в побитовом сложении (xor) сообщения с ключом: $E_{r}(x)=x \\oplus r$. Расшифровка устроена ровно так же: $D_{r}(x)=x \\oplus r$, тогда $D_{r}\\left(E_{r}(x)\\right)=D_{r}(x \\oplus r)=x \\oplus r \\oplus r=x$.\nЕсли выбирать каждый бит $r$ случайно равновероятно, то все биты $E_{r}(x)=x \\oplus r$ также будут равны 0 или 1 равновероятно, так что зашифрованное сообщение не даст Еве никакой информации (оно могло получиться из каждого сообщения $x$ с одной и той же вероятностью).\nНедостаток схемы состоит в том, что таким $r$ можно воспользоваться лишь один раз: если передать два сообщения $x$ и $z$, пользуясь одним $r$, то Ева узнает $x \\oplus z=(x \\oplus r) \\oplus(z \\oplus r)$. Отсюда уже можно извлечь полезную информацию: например, если в одном из сообщений есть длинная последовательность нулей, то в $x \\oplus z$ на том же месте будет просто кусок другого сообщения. Известна история проекта “Venona”, в рамках которого американцы во время холодной войны расшифровали многие сообщения советских разведчиков, пользуясь в том числе тем, что те иногда переиспользовали одноразовые блокноты.\nЕсли Алиса и Боб хотят передать много сообщений, им нужно заранее запастись очень большим общим набором случайных бит, что непрактично.\nБлочные шифры Существует целое семейство блочных шифров, устроенных по следующей схеме: Алиса и Боб заранее выбирают секретный ключ - случайную строку $r$, но уже фиксированной длины (например, 128 бит). Сообщение делится на блоки одинаковой длины (например, тоже по 128 бит), после чего каждый из блоков шифруется одной и той же сложно устроенной обратимой функцией $E_{r}(\\cdot)$.\nПримерно так устроены, например, следующие схемы шифрования:\nAES (advanced encryption standard) - симметричный алгоритм шифрования, принятый в 2001 году в качестве стандарта шифрования правительством США; симметричные шифры “Кузнечик” и “Магма” входят в ГОСТ 34.12-2018, принятый в качестве стандарта шифрования на территории России и СНГ. Предполагается, что несмотря на то, что один и тот же ключ используется для шифрования многих блоков, воспользоваться этим для расшифровки сообщений, как в случае с одноразовым блокнотом, не удастся, потому что функция $E_{r}(\\cdot)$ устроена достаточно сложно. Одним из важных свойств такой функции является лавинный эффект при изменении одного бита $x$ должны меняться в среднем порядка половины бит $E_{r}(x)$. Если это не так, то по зашифрованным данным можно пытаться восстановить исходные сообщения с помощью статистического анализа.\nСхемы с открытым ключом У симметричных схем есть следующий недостаток: для того, чтобы договориться о секретном ключе, Алисе и Бобу всё равно нужно как-то поговорить без свидетелей до того, как схема начнёт ими использоваться. Непонятно, что же им делать, если они никогда раньше не общались, и Ева может читать их переписку с самого начала.\nВ 1970-е годы было открыто сразу несколько схем, позволяющих Алисе и Бобу начать общаться, не договариваясь ни о каких секретных ключах заранее. Эти схемы стали называть асимметричными, или схемами с открытым ключом.\nВ общих чертах схемы с открытым ключом устроены примерно так: для того, чтобы Алиса могла передавать Бобу сообщения, Боб выбирает два ключа - закрытый, который он не показывает никому (даже Алисе); и открытый, который он показывает Алисе или даже публикует в открытом доступе. При этом функция шифрования сообщения $E(\\cdot)$ использует открытый ключ, а функция расшифровки $D(\\cdot)$ - закрытый.\nПусть Алиса хочет отправить Бобу сообщение $x$. Алиса, пользуясь открытым ключом, отправляет Бобу зашифрованное сообщение $E(x)$. Боб, пользуясь известным только ему закрытым ключом, расшифровывает сообщение: $D(E(x))=x$.\nВажно, чтобы только Боб мог вычислять значения функции $D(\\cdot)$ за разумное время (то есть, не обладая закрытым ключом, функцию $E(\\cdot)$ должно быть очень сложно обратить). Для достижения такого эффекта обычно используют задачи, которые не умеют решать быстро (например, задачу факторизации числа).\nЗаметим, что, поскольку открытый ключ доступен вообще всем, не только Алиса, а вообще кто угодно может посылать сообщения Бобу, не опасаясь, что их сможет прочитать кто-то, кроме Боба.\nДля того, чтобы Боб мог посылать сообщения Алисе, ей тоже нужно выбрать открытый и закрытый ключи, и опубликовать открытый ключ.\nRSA Одной из первых асимметричных схем была RSA (Rivest, Shamir, Adleman, 1977). Она основана на том, что генерировать большие простые числа умеют быстро, а вот быстро раскладывать числа на простые множители не умеют.\nПротокол RSA Боб выбирает открытый и закрытый ключи: Боб выбирает два случайных больших простых числа $p \\neq q$, вычисляет $n=p q$, $\\varphi(n)=(p-1)(q-1)$. Боб выбирает $e$, взаимно простое с $\\varphi(n)$. Обычно берётся небольшое $e$, чтобы шифрование занимало меньше времени. Боб публикует пару ( $n, e$ ) - открытый ключ. Боб вычисляет $d$, обратное к $e$ по модулю $\\varphi(n): d e \\equiv 1(\\bmod \\varphi(n))$. Пара $(n, d)$ - закрытый ключ Боба. Алиса хочет передать Бобу сообщение $x$. Считаем, что $0 \\leqslant x",
    "description": "Одна из классических задач криптографии звучит следующим образом. Есть Алиса и Боб, которые хотят поговорить без свидетелей. Также есть Ева (от англ. eavesdropper), которая их подслушивает. Алиса и Боб хотят, чтобы Ева, даже если она подслушает все сообщения, не поняла, о чём был разговор.\nБолее формально, пусть Алиса хочет послать Бобу секретное сообщение - битовую строку $x$. Она шифрует её с помощью некоторой функции $E(\\cdot)$ (от англ. encoder) и посылает Бобу зашифрованное сообщение $E(x)$. Боб использует функцию $D(\\cdot)$ (от англ. decoder), чтобы восстановить исходное сообщение: $D(E(x))=x$.",
    "tags": [],
    "title": "11. Криптография",
    "uri": "/basics/numerical_algos_cryptography/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Определения Граф задаётся множеством вершин $V$ и множеством рёбер $E$, соединяющих пары вершин (в английском языке граф - graph, вершина - vertex или node, ребро - edge).\nВ ориентированном (directed graph, digraph) графе каждое ребро имеет направление, то есть является упорядоченной парой вершин (может быть ребро из вершины $a$ в вершину $b$, но не быть ребра из $b$ в $a$ ). В неориентированном (undirected) графе рёбра направлений не имеют, то есть являются неупорядоченными парами (считаем, что ребро, ведущее из $a$ в $b$, ведёт и из $b$ в $a$ тоже).\nНеориентированный и ориентированный графы\nПример неориентированного графа - карта стран (вершины - страны, рёбра есть между странами, имеющими общую границу). Пример ориентированного графа - интернет (вершины - сайты, рёбра - гиперссылки). Ещё примеры графов - транспортные сети (автомобильные, железнодорожные, авиационные), графы друзей в соцсетях.\nВ графе может быть несколько рёбер между одной парой вершин (несколько рёбер в одном направлении в ориентированном графе). Такие рёбра называются кратными (multiple edges). Ребро, концы которого совпадают, называют петлёй (loор). Графы без петель и кратных рёбер называют простыми (simple).\nГраф с петлями и кратными рёбрами\nДля ребра $e=(a, b)$ вершины $a$ и $b$ называют его концами (endpoints) (или началом и концом в случае ориентированного графа). Ребро инцидентно (incident) каждому из\nсвоих концов. Две вершины неориентированного графа, соединённые ребром, называют смежными (adjacent). Два ребра неориентированного графа, имеющие общий конец, тоже называют смежными.\nСтепень (degree) вершины неориентированного графа $\\operatorname{deg}(v)$ - это количество инцидентных ей рёбер (при этом каждая петля считается два раза). В ориентированном графе у вершины $v$ есть входящая степень (indegree) $\\operatorname{deg}_{i n}(v)$ - количество рёбер, входящих в $v$, и исходящая степень (outdegree) $\\operatorname{deg}_{\\text {out }}(v)$ - количество рёбер, исходящих из $v$.\nПодграф (subgraph) $H=(W, F)$ графа $G=(V, E)$ - это граф на подмножестве вершин $W \\subseteq V$ и подмножестве рёбер $F \\subseteq E$ графа $G$, так что оба конца любого ребра из $F$ лежат в $W$. Индуцированный подграф (induced subgraph) $H=(W, F)$ содержит все рёбра исходного графа, оба конца которых лежат в $W$. Остовный подграф (spanning subgraph) содержит все вершины исходного графа ( $W=V$ ), но не обязательно все рёбра.\nСпособы хранения графа Чаще всего граф хранят одним из следующих двух способов (считаем, что $|V|=n$, $\\left.V=\\left\\{v_{0}, \\ldots, v_{n-1}\\right\\}\\right)$ :\nМатрица смежности (adjacency matrix) графа - это матрица (двумерный массив) $a$ размера $n \\times n$, где $a_{i, j}$ - количество рёбер, идущих из $v_{i}$ в $v_{j}$. Матрица смежности простого графа состоит из нулей и единиц, а её главная диагональ (множество элементов вида $a_{i, i}$ ) состоит из нулей. Матрица смежности неориентированного графа симметрична ( $a_{i, j}=a_{j, i}$ ). Списки смежности (adjacency lists) графа - это набор списков, по одному для каждой вершины: в списке смежности вершины $v$ хранится список вершин, в которые ведут рёбра из $v$ (или список самих рёбер). В графе с кратными рёбрами вершина $u$ встречается в списке $v$ столько раз, сколько рёбер из $v$ в $u$ есть в графе. В неориентированном графе каждое ребро входит в списки обоих своих концов; в ориентированном - только в список своего начала. Размер списка смежности вершины $v$ равен $\\operatorname{deg}(v)\\left(\\operatorname{deg}_{\\text {out }}(v)\\right.$ в ориентированном графе). $$ \\begin{pmatrix} 0 \u0026 1 \u0026 0 \u0026 2 \u0026 0 \\\\ 0 \u0026 1 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \u0026 1 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\end{pmatrix} \\qquad \\begin{array}{cl} 1 \u0026 \\to 2, 4, 4 \\\\ 2 \u0026 \\to 2, 3 \\\\ 3 \u0026 \\to 2, 5 \\\\ 4 \u0026 \\to \\varnothing \\\\ 5 \u0026 \\to 4 \\\\ \\end{array} $$Матрица смежности позволяет для любой пары вершин за $O(1)$ проверить, соединены ли эти вершины ребром. Однако она занимает $O\\left(n^{2}\\right)=O\\left(V^{2}\\right)$ памяти (здесь и далее для краткости в асимптотических оценках вместо $|V|,|E|$ будем писать просто $V, E)$. Если хочется проверять наличие ребра за $O(1)$, используя меньше памяти, можно вместо всей матрицы смежности хранить в хеш-таблице пары вершин, связанных ребром.\nСписки смежности можно реализовывать как динамическими массивами, так и связными списками. В любом случае, они занимают суммарно $O(V+E)$ памяти, и позволяют для любой вершины быстро просмотреть всех её соседей. Проверка наличия ребра между парой вершин $u, v$ при использовании только списка смежности займёт $O(\\min (\\operatorname{deg}(u), \\operatorname{deg}(v)))$ ( $\\operatorname{deg}_{\\text {out }}$ в ориентированном графе).",
    "description": "Определения Граф задаётся множеством вершин $V$ и множеством рёбер $E$, соединяющих пары вершин (в английском языке граф - graph, вершина - vertex или node, ребро - edge).\nВ ориентированном (directed graph, digraph) графе каждое ребро имеет направление, то есть является упорядоченной парой вершин (может быть ребро из вершины $a$ в вершину $b$, но не быть ребра из $b$ в $a$ ). В неориентированном (undirected) графе рёбра направлений не имеют, то есть являются неупорядоченными парами (считаем, что ребро, ведущее из $a$ в $b$, ведёт и из $b$ в $a$ тоже).",
    "tags": [],
    "title": "12. Графы. Определения и способы хранения",
    "uri": "/basics/graph_definition/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Большая часть рассуждений применима как к ориентированным, так и к неориентированным графам (если не сказано обратное); допускаются кратные рёбра и петли (если не сказано обратное).\nОбход вершин, достижимых из данной Поиск в глубину (depth-first search, DFS) по вершине $w \\in V$ находит множество вершин, достижимых из неё, то есть таких, в которые можно попасть, сделав несколько переходов по рёбрам, начиная из вершины $w$.\nИзначально алгоритм запускается от вершины $w$. Он перебирает исходящие из текущей вершины рёбра и смотрит, куда они ведут. Каждый раз, когда алгоритм встречает ещё не посещённую вершину, он запускается от неё рекурсивно, а после возврата из рекурсии продолжает перебирать рёбра, исходящие из текущей вершины.\n1 2 3 4 5 6 7 8 dfs(v): visited[v] = True for u in es[v]: # es[v] - список смежности вершины v if not visited[u]: dfs(u) fill(visited, False) dfs(w) # visited[u] = True, если u достижима из w Почему алгоритм посетил все достижимые из $w$ вершины? Пусть $u$ достижима из $w$, тогда существует путь (path) из $w$ в $u$ - последовательность вершин $w=v_{1}, v_{2}, \\ldots, v_{k}=u$, в которой для $1 \\leqslant i \u003c k$ есть ребро из $v_{i}$ в $v_{i+1}$ (путём, в зависимости от контекста, мы будем называть как такую последовательность вершин, так и последовательность соединяющих их рёбер). Пусть $v_{i}$ - первая вершина на пути, которую не посетил алгоритм. Тогда он посетил $v_{i-1}$ и проверил ребро, ведущее из $v_{i-1}$ в $v_{i}$, что противоречит тому, что он не посетил $v_{i}$.\nВремя работы алгоритма (при использовании списков смежности) $-O(V+E)$, так как каждая вершина посещена не более одного раза, время обработки каждой посещённой вершины пропорционально её исходящей степени, сумма исходящих степеней не превосходит числа рёбер в графе (удвоенного числа рёбер в неориентированном графе).\nКомпоненты связности Неориентированный граф называется связным (connected), если в нём существует путь между любой парой вершин.\nНа произвольном графе можно ввести отношение достижимости: $a \\sim b$, если $b$ достижимо из $a$. Это отношение рефлексивно ( $a \\sim a$ ) и транзитивно (если $a \\sim b$ и $b \\sim c$, то $a \\sim c$ ). В неориентированных графах это отношение ещё и симметрично (если $a \\sim b$, то $b \\sim a$ ), поэтому оно является отношением эквивалентности: все вершины однозначно разбиваются на классы эквивалентности - множества, в каждом из которых любые две вершины достижимы друг из друга; при этом любые две вершины из разных классов эквивалентности не достижимы друг из друга (класс эквивалентности вершины - это просто множество вершин, достижимых из неё; из рефлексивности, транзитивности и симметричности следует, что для разных вершин такие множества либо не пересекаются, либо совпадают).\nИндуцированные подграфы на каждом из этих классов эквивалентности называют компонентами связности (connected components) (иногда компонентами связности будем называть сами классы эквивалентности, то есть множества вершин этих подграфов). Каждая компонента связности является связным подграфом, при этом рёбер между вершинами из разных компонент связности в графе нет. Связный граф состоит из одной компоненты связности, содержащей все вершины.\nВ ориентированных графах отношение ~ не симметрично, поэтому понятие “компонента связности” для них не определено. Есть понятие сильной связности, которое мы обсудим чуть позже.\nКомпоненты связности этого графа - $\\{1,2,4,5\\}$, $\\{3,6,8\\}$ и $\\{7\\}$\nАлгоритм поиска в глубину по вершине находит все достижимые из неё вершины, то есть её компоненту связности. Тогда для того, чтобы найти все компоненты связности, запустим поиск в глубину из каждой вершины по очереди. Для того, чтобы не обрабатывать одну компоненту связности несколько раз, просто не будем обнулять пометки того, была ли вершина посещена, между запусками алгоритма от разных вершин. Если очередная рассматриваемая вершина уже посещена, то она входит в одну из уже найденных компонент связности. Значит, запускать алгоритм от неё не надо.\n1 2 3 4 5 6 7 8 9 10 11 12 vector\u003cvector\u003cint\u003e \u003e components dfs(v): visited[v] = True components.back().push_back(v) for u in es[v]: if not visited[u]: dfs(u) fill(visited, False) for v = 0..(n - 1): # вершины пронумерованы от 0 до n - 1 if not visited[v]: components.push_back(vector\u003cint\u003e()) dfs(v) Время работы алгоритма - по-прежнему $O(V+E)$, так как каждую вершину алгоритм теперь посещает ровно один раз.\nДерево(лес) поиска в глубину неориентированного графа Пусть $G=(V, E)$ - связный граф, из вершины $v \\in V$ которого был запущен поиск в глубину. Рассмотрим остовный подграф $H=(V, F)$, где $F$ - множество рёбер, при рассмотрении которых алгоритм делал рекурсивный запуск (рёбер, которые вели в ещё не посещённые вершины). Заметим, что $H$ - связный граф, так как поиск в глубину на связном графе $G$ посетил все вершины.\nЦикл - это путь с совпадающими концами и ненулевым числом рёбер, все рёбра в котором различны. Граф $H$ не содержит циклов: ориентируем рёбра $H$ в сторону вершины, которая была непосещённой в момент прохода по ребру алгоритмом; в каждую вершину входит не более одного ребра $H$. Но если бы нашёлся цикл, то в ту вершину цикла, которая была посещена алгоритмом позже всех, вело бы сразу два ребра.\nИтак, $H$ - связный граф без циклов, то есть дерево. Если ориентировать рёбра этого дерева, как сказано выше, получится корневое дерево (корень - вершина $v$ ). Мы снова будем пользоваться терминами, которыми пользовались при изучении двоичных куч: ребёнок, родитель, потомок, предок, лист и так далее.\n$H$ называют деревом поиска в глубину графа $G$. Конечно, дерево поиска в глубину не единственно: оно зависит от начальной вершины и порядка рёбер в списках смежности.\nСвязный граф и дерево поиска в глубину (поиск запущен из вершины 1, в списках смежности вершины упорядочены по возрастанию номеров)\nРебро графа $G$, не являющееся ребром дерева поиска в глубину (то есть любое ребро из $E \\backslash F$ ) будем называть обратным (back edge). Рассмотрим любое обратное ребро $e=(v, u)$. Пусть вершину $v$ алгоритм посетил раньше вершины $u$. Тогда вершина $u$ была посещена после вершины $v$, но до того, как алгоритм рассмотрел ребро $e$, то есть $u$ была посещена в одном из рекурсивных вызовов, произошедших во время обработки $v$. Значит, в дереве поиска в глубину $u$ является потомком $v$.\nИтак, любое обратное ребро соединяет пару “предок-потомок”. Значит, в графе нет перекрёстных рёбер - рёбер, соединяющих пары вершин, ни одна из которых не является потомком другой в дереве поиска в глубину.\nЛес поиска в глубину При поиске компонент связности в несвязном графе получается своё дерево поиска в глубину в каждой компоненте связности. Их совокупность - лес поиска в глубину. Все рёбра графа снова делятся на рёбра деревьев и обратные рёбра; рёбер между разными деревьями нет.\nПоиск цикла в неориентированном графе Как проверить, есть ли в неориентированном графе цикл? Поскольку деревья поиска в глубину не имеют циклов, любой цикл будет проходить хотя бы через одно обратное ребро. С другой стороны, любое обратное ребро $e=(v, u)$ соответствует циклу в графе: если $v$ - предок $u$, то поднимемся по дереву из $u$ в $v$, и вернёмся в $u$ по ребру $e$. Значит, в графе есть цикл тогда и только тогда, когда при поиске в глубину нашлось хотя бы одно обратное ребро.\nКак отличить обратное ребро от ребра дерева? Если граф простой, то обратное ребро любое ребро, которое ведёт в уже посещённую вершину, не являющуюся родителем текущей в дереве поиска в глубину. То есть достаточно для каждой вершины запомнить её родителя (или просто передавать его отдельным параметром при рекурсивном запуске).\nЕсли в графе имеются кратные рёбра, то надо хранить (передавать в рекурсивный запуск) не просто родителя вершины, а ещё и указатель на ребро дерева (или просто его номер, если рёбра пронумерованы), по которому мы пришли в текущую вершину из родителя. Так мы сможем отличить ребро дерева от других рёбер между той же парой вершин.\nЕсли цикл нужно явно найти и вывести, то, обнаружив обратное ребро $e=(u, v)$, будем переходить, начиная из текущей вершины $u$, по ссылкам на родителя, пока не встретим $v$, и выписывать вершины на пути.\nВремена входа и выхода В процессе работы поиска в глубину для каждой вершины $v$ запомним время входа (время начала обработки вершины) $t_{i n}[v]$ и время выхода (время конца обработки вершины) $t_{\\text {out }}[v]$. Для этого заведём специальный счётчик, который будем увеличивать на единицу каждый раз, когда происходит одно из этих событий; текущее значение этого счётчика и будет текущим временем.\n1 2 3 4 5 6 7 8 T = 0 # счётчик времени dfs(v): tin[v] = T, T += 1 visited[v] = True for u in es[v]: if not visited[u]: dfs(u) tout[v] = T, T += 1 Граф и его лес поиска в глубину с отмеченными временами входа и выхода\nЗаметим, что если вершина $u$ начала обрабатываться в одном из рекурсивных вызовов, произошедших во время обработки другой вершины $v$, то $u$ закончит обрабатываться раньше, чем $v$. Значит, для любых вершин $v, u \\in V$ либо отрезки $\\left[t_{\\text {in }}(v), t_{\\text {out }}(v)\\right]$ и $\\left[t_{i n}(u), t_{\\text {out }}(u)\\right]$ не пересекаются, либо один из них содержится в другом. Более того, отрезок $u$ содержится в отрезке $v$ тогда и только тогда, когда $u$ является потомком $v$ в дереве поиска в глубину.\nВремена входа и выхода имеют множество различных применений. Простой пример: пусть $G$ - корневое дерево; запустим поиск в глубину из корня $G$ и вычислим времена входа и выхода каждой вершины. Получившееся дерево поиска в глубину совпадает с графом $G$. Тогда мы можем за $O(1)$ проверить для любой пары вершин, является ли одна из них предком другой в $G$ : достаточно проверить, вложен ли отрезок времени обработки одной из вершин в отрезок другой вершины.\nДерево(лес) поиска в глубину ориентированного графа, типы рёбер Дерево(лес) поиска в глубину определяются для ориентированных графов так же, как и для неориентированных: будем перебирать все вершины и запускать поиск в глубину из ещё не посещённых, не обнуляя пометки о посещённости в процессе.\nВремена входа и выхода для ориентированных графов вычисляются так же, как и для неориентированных, при этом по прежнему либо отрезки времён обработки вершин не пересекаются, либо один из них вложен в другой.\nГраф и его лес поиска в глубину с отмеченными временами входа и выхода\nВ ориентированных графах рёбра, не вошедшие в лес поиска в глубину, делятся уже на три типа:\nпрямые (forward) - ведущие от вершины к её потомку (ребро $1 \\rightarrow 4$); обратные (back) - ведущие от вершины к её предку (ребро $4 \\rightarrow 2$); перекрёстные (cross) - ведущие от вершины к другой вершине, не являющейся ни предком, ни потомком первой (рёбра $3 \\rightarrow 2$ и $7 \\rightarrow 5$). Понять тип ребра можно по временам входа и выхода его концов: ребро $v \\rightarrow u$ является прямым (либо ребром леса), если $t_{\\text {in }}(v) \u003c t_{\\text {in }}(u), t_{\\text {out }}(u) \u003c t_{\\text {out }}(v)$; обратным, если $t_{\\text {in }}(u) \u003c t_{i n}(v), t_{\\text {out }}(v) \u003c t_{\\text {out }}(u)$; перекрёстным, если $t_{\\text {out }}(u) \u003c t_{\\text {in }}(v)$. Заметим, что ситуация $t_{\\text {out }}(v) \u003c t_{\\text {in }}(u)$ невозможна: в момент рассмотрения такого ребра вершина $u$ была бы непосещённой, тогда ребро вошло бы в дерево поиска в глубину, и было бы верно $t_{\\text {in }}(u) \u003c t_{\\text {out }}(u) \u003c t_{\\text {out }}(v)$.\nПоиск цикла в ориентированном графе Как и в случае неориентированного графа, в ориентированном графе есть цикл тогда и только тогда, когда при поиске в глубину нашлось хотя бы одно обратное ребро: по\nлюбому обратному ребру цикл строится точно так же, как и в неориентированном случае; с другой стороны, пусть в графе есть цикл $v_{1} \\rightarrow v_{2} \\rightarrow \\cdots \\rightarrow v_{k} \\rightarrow v_{1}$, и вершину $v_{i}$ алгоритм обработал первой среди вершин цикла. Остальные вершины цикла достижимы из $v_{i}$, поэтому все они будут потомками $v_{i}$ в дереве поиска в глубину. Но тогда ребро $v_{i-1} \\rightarrow v_{i}\\left(v_{k} \\rightarrow v_{1}\\right.$ при $\\left.i=1\\right)$ является обратным.\nОбратное ребро можно обнаружить при помощи времён входа и выхода. Альтернативный способ - для каждой вершины поддерживать пометку одного из трёх типов: “непосещённая”, “в обработке”, “обработана”. Тогда обратные рёбра - ровно те, что ведут в вершину с пометкой “в обработке”.\nТопологическая сортировка Ориентированный ациклический граф (directed acyclic graph, dag) - это ориентированный граф без циклов. С помощью ориентированных ациклических графов удобно представлять отношения зависимости: иерархические отношения (“руководитель-подчинённый”), причинно-следственные связи.\nТопологическая сортировка (topological sort) ориентированного графа - это такой порядок на вершинах, что любое ребро графа ведёт из меньшей вершины в большую. Ясно, что граф, в котором есть цикл, топологически отсортировать невозможно.\nВозникает вопрос: какие ациклические графы можно топологически отсортировать? Оказывается, что все: достаточно расположить вершины в порядке убывания времён выхода. Действительно, если нашлось такое ребро $v \\rightarrow u$, что $t_{\\text {out }}(v) \u003c t_{\\text {out }}(u)$, то это ребро обратное, значит, граф не ациклический.\nМожно не вычислять времена входа и выхода напрямую, а просто добавлять вершину в конец списка в конце её обработки. Тогда вершины в списке окажутся в порядке возрастания времён выхода. Остаётся развернуть список в конце, чтобы получить порядок топологической сортировки.\n1 2 3 4 5 6 7 8 9 10 11 12 vector\u003cint\u003e topOrder # сюда запишем вершины в порядке топологической сортировки dfs(v): visited[v] = True for u in es[v]: if not visited[u]: dfs(u) topOrder.push_back(v) fill(visited, False) for v = 0..(n - 1): # вершины пронумерованы от 0 до n - 1 if not visited[v]: dfs(v) reverse(topOrder.begin(), topOrder.end()) Исток (source) - это вершина ориентированного графа, в которую не входит рёбер. Сток (sink) - вершина, из которой, наоборот, не выходит рёбер. Заметим, что первая вершина в порядке топологической сортировки всегда является истоком, а последняя стоком. В частности, в любом ациклическом ориентированном графе всегда есть хотя бы по одному истоку и стоку (это утверждение несложно доказать и без использования топологической сортировки).\nКомпоненты сильной связности Рассмотрим на произвольном ориентированном графе следующее отношение: $a \\sim b$ ( $a$ сильно связно (strongly connected) с $b$ ), если одновременно $a$ достижимо из $b$, и $b$\nдостижимо из $a$. Такое отношение рефлексивно, транзитивно и симметрично, поэтому оно однозначно разбивает вершины графа на классы эквивалентности, которые называют компонентами сильной связности (strongly connected components). Граф, состоящий из одной такой компоненты, называют сильно связным (strongly connected graph).\nПусть $V_{1}, \\ldots, V_{k}$ - компоненты сильной связности графа $G$. Построим вспомогательный граф, вершины которого $-V_{1}, \\ldots, V_{k}$, а ребро $V_{i} \\rightarrow V_{j}$ проведено, только если в $G$ было ребро $v_{i} \\rightarrow v_{j}$ для некоторых $v_{i} \\in V_{i}, v_{j} \\in V_{j}$ (при этом не будем проводить петли и кратные рёбра). Получившийся граф называют метаграфом (meta-graph) или конденсацией графа $G$. Метаграф не содержит циклов, так как все вершины компонент сильной связности, вошедших в цикл, были бы одновременно достижимы друг из друга, то есть должны были бы лежать в одной компоненте.\nОриентированный граф и его метаграф\nНаша цель - научиться находить компоненты сильной связности произвольного ориентированного графа $G$ за $O(V+E)$. Первое, что мы сделаем - запустим алгоритм топологической сортировки на графе $G$. Он вернёт список вершин в порядке убывания времён выхода; обозначим этот список за $v_{1}, \\ldots, v_{n}$. Этот список в общем случае не является порядком топологической сортировки (так как в $G$ могут быть циклы), но всё равно обладает полезными свойствами.\nПредложение Пусть $A, B$ - компоненты сильной связности графа $G$, в метаграфе есть ребро $A \\rightarrow B$. Тогда максимальное время выхода среди вершин в $A$ больше, чем максимальное время выхода среди вершин в $B$.\nДоказательство Пусть $v \\in A \\cup B$ - первая вершина из $A \\cup B$, посещённая поиском в глубину. Если $v \\in A$, то во время обработки вершины $v$ будут посещены все вершины обеих компонент, то есть $t_{\\text {out }}(v)$ будет строго больше времён выхода остальных вершин из $A \\cup B$.\nЕсли $v \\in B$, то к концу обработки $v$ будут посещены все вершины $B$, но не будет посещено ни одной вершины из $A$. Значит, время выхода любой вершины из $A$ окажется строго больше времени выхода любой вершины из $B$.\nОтсюда следует, что метаграф можно топологически упорядочить, если расположить компоненты сильной связности в порядке убывания максимального значения времён выхода их вершин. В частности, $v_{1}$ принадлежит первой в топологическом порядке компоненте, то есть компоненте-истоку. Обозначим эту компоненту за $A_{1}$.\nКак по одной вершине из компоненты-истока найти все вершины, лежащие в этой компоненте? Рассмотрим транспонированный граф (transpose graph) $G^{T}$, полученный из $G$ изменением направления всех рёбер. Заметим, что компоненты сильной связности $G^{T}$ совпадают (как множества вершин) с компонентами $G$, а метаграф $G^{T}$ является транспонированным графом метаграфа $G$. В частности, компонента-исток в метаграфе $G$ является компонентой-стоком в метаграфе $G^{T}$.\nТранспонированный граф и его метаграф\nМножество вершин, достижимых из любой вершины компоненты-стока, совпадает с этой компонентой. Значит, для того, чтобы найти $A_{1}$, можно запустить поиск в глубину в графе $G^{T}$ из $v_{1}$.\nКак найти все остальные компоненты? Удалим из графа $G$ компоненту $A_{1}$ (обозначим полученный граф за $G_{1}$ ) и повторим те же действия: найдём $v_{i}$ - вершину с максимальным временем выхода из ещё не удалённых; $v_{i}$ лежит в компоненте-истоке $A_{2}$ графа $G_{1}$, которую можно найти, запустив поиск в глубину из $v_{i}$ в графе $G_{1}^{T}$. После этого удалим компоненту $A_{2}$ и тем же способом найдём следующую компоненту, и так далее, пока вершины не закончатся.\nУдалять вершины и перестраивать списки смежности не нужно, можно просто считать посещённые вершины удалёнными.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # уже найден topOrder - список вершин в порядке убывания времени выхода vector\u003cvector\u003cint\u003e \u003e components dfsT(v): visited[v] = True components.back().push_back(v) for u in esT[v]: # список смежности транспонированного графа if not visited[u]: dfsT(u) fill(visited, False) for v in topOrder: # перебираем вершины в порядке убывания времени выхода if not visited[v]: # v - вершина с максимальным временем выхода из ещё не посещённых components.push_back(vector\u003cint\u003e()) dfsT(v) Алгоритм топологической сортировки работает за $O(V+E)$, суммарное время поисков в глубину по транспонированному графу также есть $O(V+E)$, так как каждая вершина посещается ровно один раз.\nПоиск мостов и компонент рёберной двусвязности Мост (bridge) - ребро в неориентированном графе, удаление которого увеличивает количество компонент связности.\nВведём на вершинах графа отношение рёберной двусвязности: $a \\sim b$, если между $a$ и $b$ есть два не пересекающихся по рёбрам пути. Это отношение симметрично, рефлексивно (из вершины в неё саму есть два пути длины ноль, которые совпадают, но не пересекаются по рёбрам), а также транзитивно: пусть $a \\sim b, b \\sim c$, покажем, что $a \\sim c$. Два не пересекающихся по рёбрам пути между $b$ и $c$ образуют цикл $C$. Тогда пусть $u, v$ - первые вершины на двух не пересекающихся по рёбрам путях из $a$ в $b$, которые лежат на цикле $C$; на цикле всегда найдутся не пересекающиеся по рёбрам пути из $u$ и $v$ в $c$.\nИтак, ~ - отношение эквивалентности. Классы эквивалентности, на которые оно разбивает вершины графа, называют компонентами рёберной двусвязности (2-edge-connected components). Граф, состоящий из одной такой компоненты, называют рёберно двусвязным (2-edge-connected graph).\nМножество мостов совпадает со множеством рёбер, концы которых лежат в разных компонентах рёберной двусвязности: это рёбра, которые являются единственным путём между своими концами. Отсюда также следует, что все мосты всегда входят в любой лес поиска в глубину.\nПоиск мостов Здесь и далее будем обозначать ребро дерева поиска в глубину между вершиной и и её родителем за $e_{u}$.\nКак понять по ребру $e_{u}=(v, u)$ дерева поиска в глубину, является ли оно мостом? Если $e_{u}$ - мост, то из поддерева $u$ нет обратных рёбер, ведущих за пределы поддерева. Если же $e_{u}$ - не мост, то существует путь из $u$ в $v$, не проходящий по ребру $e_{u}$, то есть содержащий обратное ребро, ведущее в $v$ или её предка.\nПусть $\\operatorname{low}(u)$ - минимальное время входа среди всех вершин поддерева $u$ и всех концов обратных рёбер, инцидентных вершинам этого поддерева. Тогда $e_{u}$ - мост тогда и только тогда, когда $\\operatorname{low}(u)\u003et_{i n}(v)$. Значение $\\operatorname{low}(u)$ можно вычислить во время поиска в глубину:\n$$ \\operatorname{low}(u)=\\min \\left(t_{i n}(u), \\min _{(u, w) \\text {-обратное }} t_{\\text {in }}(w), \\min _{w-\\text { ребёнок } u} l o w(w)\\right) . $$\nГраф и его дерево поиска в глубину с отмеченными временами входа и значениями low. Мосты помечены красным цветом. Компоненты рёберной двусвязности: $\\{1,2,5,7\\},\\{3,4,8\\},\\{6\\},\\{9\\},\\{10\\},\\{11\\}$.\nПоиск компонент рёберной двусвязности Зная мосты, несложно найти и компоненты рёберной двусвязности: компонента, содержащая корень дерева, состоит из вершин, на пути в дереве от корня до которых нет мостов; если ребро дерева $e_{u}=(v, u)$ - мост, то компонента, содержащая вершину $u$, состоит из вершин поддерева $u$, на пути от $u$ до которых нет мостов. Значит, компоненты можно найти, запуская поиски в глубину на уже построенном лесе поиска в глубину, в котором помечены мосты.\nМожно найти компоненты и в процессе поиска мостов: будем складывать вершину в стек в начале её обработки. Каждый раз, когда мы находим мост $e_{u}=(v, u)$, будем доставать вершины из стека, пока не достанем $u$. При этом мы достанем вершины, лежащие в поддереве $u$, на пути в дереве от $u$ до которых не было моста (иначе мы достали бы их раньше). Значит, мы достанем ровно компоненту вершины $u$. По тем же причинам в конце в стеке останется ровно компонента корня дерева.\nПолучаем алгоритм поиска мостов и компонент рёберной двусвязности за $O(V+E)$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 vector\u003cvector\u003cint\u003e \u003e components vector\u003cint\u003e stack T = 0 dfs(v, parent): visited[v] = True tin[v] = low[v] = T, T += 1 stack.push_back(v) for u in es[v]: if u != parent: # проверяем, что это не ребро, по которому пришли в v; # если есть кратные рёбра, нужно проверять аккуратнее if not visited[u]: dfs(u, v) if low[u] \u003e tin[v]: # (v, u) - mост vector\u003cint\u003e component while component.size() == 0 or component.back() != u: component.push_back(stack.back()) stack.pop_back() components.push_back(component) low[v] = min(low[v], low[u]) else: # обратное ребро low[v] = min(low[v], tin[u]) fill(visited, False) for v = 0..(n - 1): if not visited[v]: stack.clear() dfs(v, -1) components.push_back(stack) # компонента корня дерева По любому графу можно построить лес, вершинами которого будут компоненты рёберной двусвязности графа, а рёбрами - мосты.\nПоиск точек сочленения и компонент вершинной двусвязности Точка сочленения (cut vertex, articulation point) - вершина неориентированного графа, удаление которой увеличивает количество компонент связности.\nВведём на рёбрах графа отношение вершинной двусвязности: $e_{1} \\sim e_{2}$, если $e_{1}=e_{2}$, или если существует простой цикл (цикл, в котором вершины не повторяются), проходящий одновременно через $e_{1}$ и $e_{2}$ (или, что то же самое, есть два не пересекающихся по вершинам пути из концов $e_{1}$ в концы $e_{2}$ ). Это отношение рефлексивно, симметрично и транзитивно: покажем, что если $e_{1} \\sim e_{2}$ и $e_{2} \\sim e_{3}$, то $e_{1} \\sim e_{3}$. Можно считать, что $e_{1}, e_{2}, e_{3}$ попарно различны (иначе доказывать нечего). Пусть $C$ - простой цикл, проходящий через $e_{2}$ и $e_{3}$. Пусть $u, v$ - первые вершины на не пересекающихся по вершинам путях из концов $e_{1}$ в концы $e_{2}$, которые лежат на цикле $C ; u \\neq v$, на цикле найдётся два не пересекающихся по вершинам пути из $u, v$ в концы $e_{3}$.\nЗначит, ~ - отношение эквивалентности; классы эквивалентности, на которые разбиваются рёбра, называют компонентами вершинной двусвязности, или просто компонентами двусвязности (biconnected components), а также блоками (blocks). Граф, состоящий из одной компоненты двусвязности, называют вершинно двусвязным, или просто двусвязным (biconnected graph).\nДля каждой компоненты двусвязности $F$ можно рассмотреть подграф $H=(W, F)$, где $W$ - множество концов рёбер из $F$. Этот подграф тоже будем называть компонентой двусвязности или блоком. Заметим, что такой подграф всегда связен.\nЛюбые два блока (как подграфы) либо не пересекаются по вершинам, либо пересекаются ровно по одной вершине: пусть компоненты $H_{1}, H_{2}$ пересеклись по вершинам $a \\neq b$; между $a$ и $b$ есть два пути - один в $H_{1}$, другой в $H_{2}$. Объединение этих путей даёт простой цикл, на котором лежат рёбра из разных компонент двусвязности, что невозможно.\nМножество вершин, лежащих в пересечении нескольких компонент двусвязности, совпадает со множеством точек сочленения: это вершины, у которых есть соседи из разных компонент двусвязности.\nРассмотрим вспомогательный граф, вершины которого - это точки сочленения и блоки, а рёбра соответствуют парам из точки сочленения и блока, в котором эта точка лежит. В этом графе нет циклов (точки сочленения на таком цикле не были бы точками сочленения). При этом если исходный граф связен, то этот граф тоже связен: в этом случае он является деревом, и его называют деревом блоков и точек сочленения (block-cut tree). В случае несвязного графа такое дерево есть у каждой компоненты связности.\nГраф и его дерево блоков и точек сочленения. Точки сочленения помечены синим цветом.\nЗаметим, что мосты соответствуют компонентам двусвязности, состоящим из одного ребра.\nПоиск точек сочленения Для поиска точек сочленения снова воспользуемся значениями low. Пусть $v$ - вершина, не являющаяся корнем дерева поиска в глубину. $v$ - точка сочленения тогда и только тогда, когда у вершины $v$ есть такой ребёнок $u$, что $\\operatorname{low}(u) \\geqslant t_{i n}(v)$ (то есть из поддерева $u$ нет обратных рёбер, ведущих за пределы поддерева $v$ ). Действительно, это условие равносильно тому, что при удалении $v$ корень и некоторая вершина $u$, соседняя с $v$, окажутся в разных компонентах связности.\nКорень дерева же является точкой сочленения тогда и только тогда, когда он имеет хотя бы два ребёнка (это следует из того, что перекрёстных рёбер не бывает).\nПоиск компонент двусвязности Как найти компоненты двусвязности? Во-первых, заметим, что любое обратное ребро между вершиной $u$ и её предком $w$ лежит в той же компоненте, что и ребро дерева $e_{u}$ (они лежат на простом цикле, состоящем из пути в дереве от $w$ до $u$ и обратного ребра).\nОстаётся разбить на компоненты рёбра дерева. Назовём ребро дерева $e_{u}=(v, u)$ интересным, если low $(u) \\geqslant t_{i n}(v)$. В частности, все рёбра дерева между корнем и его ребенком - интересные; при поиске точек сочленения, не являющихся корнем, мы искали как раз интересные рёбра.\nЗаметим, что если ребро $e_{u}=(v, u)$ - не интересное, то $e_{u}$ и $e_{v}$ лежат в одной компоненте двусвязности. Если же ребро $e_{u}=(v, u)$ - интересное, то все рёбра дерева из той же компоненты двусвязности, что и $e_{u}$, находятся в поддереве вершины $u$.\nПолучаем следующий алгоритм: будем обходить рёбра дерева в порядке поиска в глубину, тогда ребро дерева $e_{u}=(v, u)$ либо лежит в той же компоненте, что и $e_{v}$ (если $e_{u}$ не интересное), либо лежит в новой компоненте, рёбра из которой мы до этого не встречали.\nМожно строить компоненты и прямо во время поиска точек сочленения (подробности на следующей странице).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # для удобства считаем, что граф простой (если это не так, некоторые проверки # нужно проводить аккуратнее, а в стеке нужно различать кратные рёбра) vector\u003cvector\u003cpair\u003cint, int\u003e \u003e \u003e components vector\u003cpair\u003cint, int\u003e \u003e stack T = 0 dfs(v, parent): visited[v] = True tin[v] = low[v] = T, T += 1 count = 0 # считаем количество детей for u in es[v]: if u != parent: # проверяем, что это не ребро, по которому пришли в V if not visited[u] or tin[u] \u003c tin[v]: # видим ребро в первый раз stack.push_back({v,u}) if not visited[u]: count += 1 dfs(u, v) if low[u] \u003e= tin[v]: # (v, u) - интересное vector\u003cpair\u003cint, int\u003e \u003e component while component.size() == 0 or component.back() != {v,u}: component.push_back(stack.back()) stack.pop_back() components.push_back(component) if parent != -1 or count \u003e= 2: ... # v - точка сочленения low[v] = min(low[v], low[u]) else: # oбратноe pe6po low[v] = min(low[v], tin[u]) fill(visited, False) for v = 0..(n - 1): if not visited[v]: dfs(v, -1) Будем класть ребро в стек, когда видим его в первый раз (в простом графе это равносильно тому, что ребро ведёт в непосещённую вершину, либо в вершину с меньшим временем входа, чем у текущей). Каждый раз, когда мы находим интересное ребро дерева $e_{u}=(v, u)$, будем доставать рёбра из стека, пока не достанем $e_{u}$. При этом любое обратное ребро $(a, b)$ ( $a$ - потомок $b$ ) мы достанем одновременно с ребром $e_{a}$ (и они действительно должны лежать в одной компоненте). Что касается рёбер дерева, мы достанем все рёбра из поддерева $u$, которые не интересные сами, и на пути от которых до $e_{u}$ нет других интересных рёбер (все остальные мы достали раньше). Это ровно те рёбра дерева, которые лежат в одной компоненте с $e_{u}$. Поскольку все рёбра из корня в его детей - интересные, по завершении обработки корня стек будет пустой.\nПолучаем алгоритм поиска точек сочленения и компонент двусвязности за $O(V+E)$.\n2-SAT Булева переменная (boolean variable) - переменная, принимающая одно из двух значений: истина или ложь; true или false; 1 или 0. Литерал (literal) - это булева переменная или её отрицание. Если $x$ - булева переменная, то $x, \\neg x$ (так обозначается отрицание) литералы.\nКонъюнктивная нормальная форма (KHФ, conjunctive normal form, CNF) представляет собой конъюнкцию (conjunction), то есть логическое “И”, нескольких дизъюнктов. Каждый дизъюнкт (clause) представляет собой дизъюнкцию (disjunction), то есть логическое “ИЛИ”, нескольких литералов. Пример КНФ от переменных $x_{1}, x_{2}, x_{3}, x_{4}$ (конъюнкция обозначается как $\\wedge$, дизъюнкция - как $\\vee$ ):\n$$ \\left(x_{1} \\vee \\neg x_{3} \\vee x_{4}\\right) \\wedge\\left(\\neg x_{2} \\vee \\neg x_{4}\\right) \\wedge\\left(x_{3}\\right) \\wedge\\left(\\neg x 1 \\vee x_{2} \\vee \\neg x_{3} \\vee \\neg x_{4}\\right) . $$Дизъюнкт называется выполненным (satisfied), если хотя бы один его литерал имеет значение true. KHФ выполнима (satisfiable), если существует набор значений переменных, при котором выполнены все дизъюнкты. Такой набор значений называют выполняющим набором. Один из выполняющих наборов для формулы выше - true,false,true,false.\nВ 2-KHФ каждый дизъюнкт состоит из двух литералов. Задача 2-SAT (2-satisfiability) формулируется следующим образом: дана 2-КНФ; нужно проверить, выполнима ли она, и предъявить выполняющий набор, если он существует. Примеры 2-KНФ:\n$$ \\left(x_{1} \\vee \\neg x_{2}\\right) \\wedge\\left(\\neg x_{1} \\vee \\neg x_{3}\\right) \\wedge\\left(x_{1} \\vee x_{2}\\right) \\wedge\\left(\\neg x_{3} \\vee x_{4}\\right) \\wedge\\left(\\neg x_{1} \\vee x_{4}\\right) $$выполнима (выполняющий набор - true,false,false,true);\n$$ \\left(x_{1} \\vee x_{2}\\right) \\wedge\\left(\\neg x_{1} \\vee x_{2}\\right) \\wedge\\left(\\neg x_{2} \\vee x_{3}\\right) \\wedge\\left(\\neg x_{2} \\vee \\neg x_{3}\\right) $$невыполнима.\nРешение задачи 2-SAT Пусть дана 2-КНФ $A$ на $n$ переменных $x_{1}, \\ldots, x_{n}$, состоящая из $m$ дизъюнктов. Построим по ней граф импликаций $G_{A}$. Вершинами этого графа будут все $2 n$ литералов. Для каждого дизъюнкта ( $a \\vee b$ ) ( $a, b$ - литералы) проведём два ребра: $\\neg a \\rightarrow b$ и $\\neg b \\rightarrow a$ (если $a=\\neg x_{i}$, то $\\left.\\neg a=\\neg\\left(\\neg x_{i}\\right)=x_{i}\\right)$.\nЗаметим, что дизъюнкт ( $a \\vee b$ ) эквивалентен любой из двух импликаций (implications, “если…, то…”) $\\neg a \\Rightarrow b$ и $\\neg b \\Rightarrow a$. Таким образом, для любого пути в графе импликаций из $c$ в $d$ и значений $c, d$ из любого выполняющего набора верно, что $c \\Rightarrow d$.\nГрафы импликаций 2-КНФ из примеров выше\nЕсли для какой-то переменной $x_{i}$ литералы $x_{i}$ и $\\neg x_{i}$ оказались в одной компоненте сильной связности графа импликаций, то $A$ невыполнима. Действительно, если бы нашёлся выполняющий набор, для него было бы верно одновременно $x_{i} \\Rightarrow \\neg x_{i}$ и $\\neg x_{i} \\Rightarrow x_{i}$. Но если $x_{i}$ равно true, то неверна первая импликация, а если $x_{i}$ равно false, то вторая.\nПусть это не так, то есть никакая компонента сильной связности не содержит одновременно литерал и его отрицание. Тогда построим выполняющий набор следующим образом: пронумеруем компоненты сильной связности в порядке топологической сортировки (алгоритм нахождения компонент сильной связности находит их именно в таком порядке); пусть $k[a]$ - номер компоненты литерала $a . k\\left[x_{i}\\right] \\neq k\\left[\\neg x_{i}\\right]$ для любой переменной $x_{i}$; присвоим переменной $x_{i}$ значение true, если $k\\left[x_{i}\\right]\u003ek\\left[\\neg x_{i}\\right]$, и значение false иначе.\nПочему получился выполняющий набор? Пусть нашёлся невыполненный дизъюнкт $(a \\vee b)(a, b-$ литералы). В графе импликаций ему соответствуют рёбра $\\neg a \\rightarrow b$ и $\\neg b \\rightarrow a$, откуда следует $k[b] \\geqslant k[\\neg a], k[a] \\geqslant k[\\neg b]$. С другой стороны, из того, что дизъюнкт невыполнен, следует, что $k[\\neg a]\u003ek[a], k[\\neg b]\u003ek[b]$. Получаем $k[b] \\geqslant k[\\neg a]\u003ek[a] \\geqslant k[\\neg b]\u003ek[b]$, что невозможно.\nТаким образом, получаем следующий алгоритм: строим граф импликаций; находим его компоненты сильной связности и значения $k$ для всех вершин графа; проверяем, верно ли, что $k\\left[x_{i}\\right] \\neq k\\left[\\neg x_{i}\\right]$ для всех переменных. Если это не так, то $A$ невыполнима; если это так, то выполняющий набор строится следующим образом: присвоим true всем $x_{i}$ таким, что $k\\left[x_{i}\\right]\u003ek\\left[\\neg x_{i}\\right]$, остальным присвоим f alse.\nПоскольку в графе импликаций $2 n$ вершин и $2 m$ рёбер, время работы алгоритма $O(n+m)$.",
    "description": "Большая часть рассуждений применима как к ориентированным, так и к неориентированным графам (если не сказано обратное); допускаются кратные рёбра и петли (если не сказано обратное).\nОбход вершин, достижимых из данной Поиск в глубину (depth-first search, DFS) по вершине $w \\in V$ находит множество вершин, достижимых из неё, то есть таких, в которые можно попасть, сделав несколько переходов по рёбрам, начиная из вершины $w$.\nИзначально алгоритм запускается от вершины $w$. Он перебирает исходящие из текущей вершины рёбра и смотрит, куда они ведут. Каждый раз, когда алгоритм встречает ещё не посещённую вершину, он запускается от неё рекурсивно, а после возврата из рекурсии продолжает перебирать рёбра, исходящие из текущей вершины.",
    "tags": [],
    "title": "13. Графы. Поиск в глубину",
    "uri": "/basics/graph_depth_search/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Во взвешенном графе (weighted graph) каждое ребро имеет вес (weight) $w_{e}$. В зависимости от задачи, веса могут быть как целыми, так и вещественными; иногда допускаются отрицательные веса.\nДлина (length) пути во взвешенном графе - это сумма весов рёбер на пути. Можно считать, что все рёбра в невзвешенном графе имеют вес, равный единице. Тогда длина пути в невзвешенном графе - это просто количество рёбер в этом пути. Кратчайший путь (shortest path) между двумя вершинами - это путь минимальной длины между этими вершинами (путь с минимальным числом рёбер в невзвешенном случае). Расстояние (distance) между вершинами - длина кратчайшего пути между ними. Если пути между вершинами нет, расстояние считается бесконечным.\nАлгоритмы поиска кратчайших путей, которые мы изучим, применимы как для неориентированных, так и для ориентированных графов. Отличие лишь в том, что в ориентированном графе расстояние от вершины $a$ до вершины $b$ может не совпадать с расстоянием от вершины $b$ до вершины $a$.\nПоиск в ширину Поиск в ширину (breadth-first search, BFS) находит расстояния в невзвешенном графе от одной вершины $s$ до всех остальных вершин. Пусть $d_{v}$ - расстояние от $s$ до вершины $v$; $A_{k}$ - множество вершин, расстояние до которых равняется $k: A_{k}=\\left\\{v \\in V: d_{v}=k\\right\\}$.\nБудем находить множества $A_{k}$ последовательно: $A_{0}=\\{s\\}$; пусть уже найдены $A_{0}, \\ldots, A_{k-1}$, тогда $A_{k}$ - это множество вершин из $V \\backslash A_{0} \\backslash \\cdots \\backslash A_{k-1}$, в которые ведут рёбра из $A_{k-1}$. Быстро понимать, лежит ли уже вершина $v$ в одном из множеств, можно с помощью массива расстояний dist: $\\operatorname{dist}[v]=k$, если $v \\in A_{k} ; \\operatorname{dist}[v]=\\infty$, если множество, в котором лежит $v$, ещё не найдено (или если $v$ не достижима из $s$ ). Здесь $\\infty$ - бесконечность; на практике в качестве $\\infty$ можно использовать число, заведомо большее, чем длина любого кратчайшего пути; поскольку мы работаем с невзвешенными графами, подойдёт $\\infty=|V|$.\n1 2 3 4 5 6 7 8 9 10 11 12 vector\u003cvector\u003cint\u003e \u003e A vector\u003cint\u003e dist(n) # в графе n вершин fill(dist, inf) # inf - \"бесконечность\" dist[s] = 0 A.push_back({s}) for (i = 0; A[i].size() \u003e 0; i += 1): A.push_back({}) for v in A[i]: for u in es[v]: if dist[u] == inf: dist[u] = i + 1 A[i + 1].push_back(u) Этот алгоритм уже работает за $O(V+E)$, так как он просматривает список смежности каждой вершины не более одного раза. Можно ещё упростить алгоритм: представим, что множества $A_{0}, A_{1}, \\ldots$ хранятся подряд в одном массиве: $q=A_{0} A_{1} A_{2} \\ldots$ Тогда вышеописанный алгоритм просматривает элементы этого массива по порядку (то есть по очереди достаёт вершины из начала массива), и иногда добавляет новые вершины в конец массива. Значит, $q$ - это просто очередь; можно не хранить множества $A_{k}$ явно, вместо этого поддерживая очередь $q$. Получившийся алгоритм и называют поиском в ширину.\n1 2 3 4 5 6 7 8 9 10 vector\u003cint\u003e dist(n) # в графе n вершин fill(dist, inf) dist[s] = 0 q \u003c-- s # кладём s в очередь while not q.empty(): q --\u003e v # достаём v из очереди for u in es[v]: if dist[u] == inf: dist[u] = dist[v] + 1 q \u003c-- u Дерево кратчайших путей Пока мы научились только находить расстояния от $s$ до остальных вершин. Что делать, если нужно восстановить сами кратчайшие пути? Аналогично дереву поиска в глубину, можно построить дерево поиска в ширину: в него войдут рёбра, при просмотре которых алгоритм добавлял новые вершины в очередь. В простом графе достаточно для каждой вершины $u$ запомнить её предка в дереве поиска в ширину; в графе с кратными рёбрами нужно запомнить, какое именно из кратных рёбер алгоритм просматривал при добавлении u в очередь.\nПуть в этом дереве из $s$ в любую вершину $u$ - кратчайший; поэтому дерево поиска в ширину называют также деревом кратчайших путей. Для того, чтобы восстановить кратчайший путь из $s$ в $u$, нужно подниматься по рёбрам дерева из $u$, пока не попадём в $s$.\n1 2 3 4 if dist[u] == inf: dist[u] = dist[v] + 1 p[u] = v # v - родитель u в дереве кратчайших путей q \u003c-- u Граф и его дерево кратчайших путей\n1 2 3 4 5 6 vector\u003cint\u003e path while u != s: path.push_back(u) u = p[u] path.push_back(s) reverse(path.begin(), path.end()) 0-1-BFS Пусть теперь каждое ребро в графе имеет вес, равный 0 или 1 . Модифицируем алгоритм поиска в ширину следующим образом: вместо очереди будем использовать дек; при просмотре ребра $e$ из $v$ в $u$ будем класть $u$ в дек, если нашёлся более короткий путь до $u$, чем был известен ранее (если $\\operatorname{dist}[u]\u003e\\operatorname{dist}[v]+w_{e}$ ). При этом положим $u$ в начало дека, если $w_{e}=0$, и в конец дека, если $w_{e}=1$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 vector\u003cint\u003e dist(n) # в графе n вершин fill(dist, inf) dist[s] = 0 deque \u003c-- s # кладём s в дек while not deque.empty(): v \u003c-- deque # достаём v из начала дека for u, w in es[v]: # u - конец ребра, w - его вес if dist[u] \u003e dist[v] + w: dist[u] = dist[v] + w if w == 0: u --\u003e deque # кладём и в начало дека else: deque \u003c-- u # кладём u в конеп дека Заметим сначала, что для любой вершины $v$ значение $\\operatorname{dist}[v]$ равно $\\infty$, либо соответствует длине какого-то пути из $s$ в $v$, поэтому $\\operatorname{dist}[v]$ не может оказаться меньше длины кратчайшего пути.\nПосмотрим на все вершины в порядке, в котором мы достаём их из дека в первый раз (пока для удобства будем считать, что алгоритм игнорирует повторные попадания вершины в дек). Сначала мы достанем вершину $s$, затем все вершины, достижимые из $s$ только по рёбрам веса 0 . K этому моменту мы достали ровно множество $A_{0}$, при этом в момент извлечения этих вершин из дека расстояние до них уже посчитано корректно.\nПосле этого мы достанем из дека вершины из $V \\backslash A_{0}$, достижимые из $A_{0}$ по ребру веса 1 , а также все вершины, достижимые из этих по рёбрам веса 0 , то есть ровно множество $A_{1}$; расстояние до этих вершин также уже посчитано корректно в момент извлечения их из дека. Аналогично, после этого мы достанем множества $A_{2}, A_{3}$, и так далее; в момент извлечения любой вершины из дека расстояние до неё уже посчитано корректно.\nПоймём теперь, почему каждая вершина попала в дек не более двух раз (тогда время работы алгоритма оценивается как $O(V+E)$ ). Пусть $u$ попала в дек в первый раз в момент обработки ребра $e=(v, u)$, в $\\operatorname{dist}[u]$ при этом было записано $\\operatorname{dist}[v]+w_{e}$. K этому моменту уже была хотя бы раз обработана любая вершина $t$ с $d_{t} \u003c d_{v}$. Значит, $d_{u} \\geqslant d_{v}$. Тогда, если $w_{e}=0$, то $\\operatorname{dist}[u]=d_{u}$, то есть $u$ больше никогда не будет добавлена в дек. Если же $w_{e}=1$, то $\\operatorname{dist}[u] \\leqslant d_{u}+1$, тогда $u$ может быть добавлена в дек ещё не более одного раза.\n$1-k$-BFS Пусть теперь рёбра в графе могут иметь любой целый вес от 1 до $k$. Как найти кратчайшие пути от $s$ до остальных вершин в таком графе?\nПервый способ - заменим каждое ребро длины $l\u003e1$ на $l$ рёбер длины 1 , добавив при этом в граф $l-1$ новую вершину: ребро $e=(u, v), w_{e}=l\u003e1$ заменим на $l$ рёбер $e_{1}=\\left(u, a_{1}\\right), e_{2}=\\left(a_{1}, a_{2}\\right), \\ldots, e_{l}=\\left(a_{l-1}, v\\right)$, где $a_{1}, \\ldots, a_{l-1}$ - новые вершины (свои для каждого такого ребра). Расстояния между вершинами исходного графа в новом графе не изменились. При этом все рёбра нового графа имеют длину 1 , поэтому расстояния в нём можно найти поиском в ширину. Поскольку в новом графе $O(V+(k-1) E)$ вершин и $O(k E)$ рёбер, получаем время работы $O(V+k E)$.\nВторой способ - будем поддерживать свою очередь $q_{i}$ для каждого расстояния $i$. Вначале $q_{0}=\\{s\\}$, остальные очереди пустые; $\\operatorname{dist}[s]=0, \\operatorname{dist}[v]=\\infty$ для $v \\neq s$. Будем рассматривать очереди $q_{i}$ в порядке возрастания $i$. Для каждой вершины $v$ из $q_{i}$ такой, что $\\operatorname{dist}[v]=i$, и каждого ребра $e=(v, u)$ такого, что $\\operatorname{dist}[u]\u003e\\operatorname{dist}[v]+w_{e}$, обновим $\\operatorname{dist}[u]$ и положим $u$ в $q_{d i s t[v]+w_{e}}$.\n1 2 3 4 5 6 7 8 9 10 11 12 13 vector\u003cint\u003e dist(n) # в графе n вершин fill(dist, inf) dist[s] = 0 q[0] \u003c-- s for i = 0..((n - 1) * k): # (n - 1) * k - максимально возможное расстояние while not q[i].empty(): v \u003c-- q[i] if dist[v] != i: continue for u, w in es[v]: if dist[u] \u003e dist[v] + w: dist[u] = dist[v] + w q[i + w] \u003c-- u $\\operatorname{dist}[v]$ снова всегда равняется длине какого-то пути от $s$ до $v$ (либо $\\infty$ ). Покажем по индукции, что вершины $v$ такие, что $\\operatorname{dist}[v]=i$ в момент извлечения $v$ из $q_{i}$ - это ровно вершины множества $A_{i}$. Это верно для $i=0$; для $i\u003e0$ в $q_{i}$ попадают вершины из $V \\backslash A_{0} \\backslash \\cdots \\backslash A_{i-1}$, в которые ведёт ребро веса 1 из $A_{i-1}$, веса 2 из $A_{i-2}, \\ldots$ или веса $k$ из $A_{i-k}$ (то есть вершины множества $A_{i}$ ), а также, возможно, часть вершин из $A_{0} \\cup \\cdots \\cup A_{i-1}$, в которые ведут такие же рёбра. Тогда такие вершины $v$, что $\\operatorname{dist}[v]=i$ в момент извлечения $v$ из $q_{i}$ - это ровно вершины множества $A_{i}$.\nПри этом каждая вершина попадёт в не более чем $k$ различных очередей: пусть вершина $u$ попала в $q_{d i s t[v]+w_{e}}$ при рассмотрении ребра $e=(v, u)$, до этого было верно $\\operatorname{dist}[u]=\\infty$. Тогда в этот момент $\\operatorname{dist}[v]=d_{v}$, откуда $d_{u}\u003ed_{v}$ (иначе $u$ уже попала бы до этого в очередь $q_{i}$ для $i \\leqslant d_{v}$ ). Но тогда $\\operatorname{dist}[u]=\\operatorname{dist}[v]+w_{e}",
    "description": "Во взвешенном графе (weighted graph) каждое ребро имеет вес (weight) $w_{e}$. В зависимости от задачи, веса могут быть как целыми, так и вещественными; иногда допускаются отрицательные веса.\nДлина (length) пути во взвешенном графе - это сумма весов рёбер на пути. Можно считать, что все рёбра в невзвешенном графе имеют вес, равный единице. Тогда длина пути в невзвешенном графе - это просто количество рёбер в этом пути. Кратчайший путь (shortest path) между двумя вершинами - это путь минимальной длины между этими вершинами (путь с минимальным числом рёбер в невзвешенном случае). Расстояние (distance) между вершинами - длина кратчайшего пути между ними. Если пути между вершинами нет, расстояние считается бесконечным.",
    "tags": [],
    "title": "14. Графы. Алгоритмы поиска кратчайших путей",
    "uri": "/basics/graph_shortest_path_algos/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Алгоритм Хаффмана Алгоритм Хаффмана (Huffman, 1952) - один из самых известных алгоритмов сжатия текста. Пусть дан текст, состоящий из символов алфавита $\\Sigma$, который мы хотим закодировать как можно более короткой последовательностью бит.\nПрефиксные коды Самый простой способ - кодировать каждый символ уникальной последовательностью из $k$ бит (будем называть такую последовательность кодовым словом (codeword)), где $2^{k} \\geqslant|\\Sigma|$ (чтобы такое кодирование вообще было возможно). Такой способ не всегда эффективен: например, пусть $\\Sigma=\\{a, b, c, d\\}$, текст имеет длину 10000 , но большая часть символов текста равна $a$ (скажем, $a$ встречается в тексте 7000 раз, а остальные символы по 1000 раз). Тогда вышеописанный способ кодирования потребует $20000$ бит. Если же кодовое слово символа $а$ будет равно $0$ , символа $b-10, c-110, d-111$, то суммарно потребуется лишь $7000 \\cdot 1+1000 \\cdot 2+1000 \\cdot 3+1000 \\cdot 3=15000$ бит.\nИтак, иногда оказывается выгодным использовать кодовые слова разной длины для разных символов. Заметим, что при этом нельзя использовать одновременно кодовые слова, одно из которых является префиксом другого: в этом случае нельзя будет понять, где в закодированной последовательности бит заканчивается одно кодовое слово, и начинается следующее. Коды, в которых ни одно кодовое слово не является префиксом другого, называют префиксными (prefix codes) (логичнее было бы называть их беспрефиксными, но термин “префиксный код” является общепринятым).\nКакой префиксный код будет оптимальным для данного текста $t$ ? Пусть $\\operatorname{cnt}(t, a)$ количество вхождений символа $a$ в текст (будем называть это количество частотой), $l e n(C, a)$ - длина кодового слова символа $a$ в префиксном коде $C$. Тогда количество бит, которое понадобится для того, чтобы закодировать текст $t$ кодом $C$, равняется\n$$ L(C, t)=\\sum_{a} c n t(t, a) \\cdot \\operatorname{len}(C, a) . $$Мы хотим найти префиксный код $C$, минимизирующий $L(C, t)$.\nСвязь с двоичными деревьями Любому префиксному коду можно сопоставить двоичное дерево: кодовые слова будут соответствовать листьям дерева; для того, чтобы по листу получить кодовое слово, нужно пройти по пути из корня в этот лист, и выписывать 0 при спуске в левого ребёнка, 1 при спуске в правого.\nВ дереве оптимального префиксного кода ни у какой вершины не может быть ровно один ребенок: такую вершину можно просто удалить, поставив её ребёнка на её место; при этом новое дерево будет всё ещё соответствовать префиксному коду, но длина некоторых кодовых слов уменьшится.\nПрефиксный код и соответствующее ему двоичное дерево\nЖадный алгоритм Посмотрим на самый глубокий лист $v$ в дереве оптимального префиксного кода. У родителя $v$ есть ещё один ребёнок; обозначим его за $u$. Пути от корня до $v$ и $u$ являются кодовыми словами каких-то двух символов. Если это не два самых редко встречающихся в тексте $t$ символа, то поменяем их местами с самыми редкими. При этом длина закодированного текста может только уменьшиться. Значит, всегда найдётся оптимальный префиксный код, в дереве которого два самых редких символа находятся в соседних листьях на максимальной глубине.\nПусть $C$ - такой оптимальный код, $a$ и $b$ - два самых редких символа в $t$; они находятся в листьях $v, u$ с общим родителем $w$. Введём новый символ $c$ и рассмотрим текст $t_{1}$, полученный из $t$ заменой всех вхождений $a$ и $b$ на $c$. Также рассмотрим префиксный код $C_{1}$, соответствующий дереву кода $C$, из которого удалили $v$ и $u$, а в ставшую листом вершину $w$ поместили символ $c$. Заметим, что $L(C, t)=L\\left(C_{1}, t_{1}\\right)+\\operatorname{cnt}(t, a)+c n t(t, b)$.\n$$ L(D, t)=L\\left(D_{1}, t_{1}\\right)+\\operatorname{cnt}(t, a)+\\operatorname{cnt}(t, b) \u003c L\\left(C_{1}, t_{1}\\right)+c n t(t, a)+\\operatorname{cnt}(t, b)=L(C, t) . $$Но это противоречит оптимальности $C$, значит такого $D_{1}$ не существует, и $C_{1}$ - оптимален. Кроме того, по любому оптимальному коду для $t_{1}$ вышеописанной операцией мы можем построить оптимальный код для $t$.\nБудем применять эти рассуждения рекурсивно, и получать строки, в которых всё меньше и меньше различных символов. В конце концов мы получим строку, в которой встречается всего два различных символа. Оптимальный префиксный код для такой строки кодирует каждый символ одним битом.\nРеализация Будем хранить символы в очереди с приоритетами: приоритет символа равен его частоте. Пока в очереди больше одного символа, будем доставать из очереди два символа $a, b$ с минимальными приоритетами $c n t_{a}, c n t_{b}$, и складывать в очередь новый символ $c$ с приоритетом $c n t_{c}=c n t_{a}+c n t_{b}$. В этот момент будем подвешивать вершины, соответствующие символам $a$ и $b$, детьми к вершине, соответствующей символу $c$. В конце в очереди останется только один символ; вершина, соответствующая этому символу, будет корнем дерева оптимального префиксного кода. Получаем время работы $O(|\\Sigma| \\log |\\Sigma|)$ (если считать, что частоты символов уже предподсчитаны).\n1 2 3 4 5 6 7 8 9 10 # для удобства считаем, что символы - числа от 0 до n - 1 for i = 0..(n - 1): q \u003c-- (i, cnt[i]) # кладём в очередь i с приоритетом cnt[i] for c = n..(2 * n - 2): # n - 1 итерация a \u003c-- q, b \u003c-- q # извлекаем два элемента с минимальными приоритетами cnt[c] = cnt[a] + cnt[b] v[c]-\u003e1 = v[a], v[c]-\u003er = v[b] # подвешиваем a и b детьми к c q \u003c-- (c, cnt[c]) r \u003c-- q return v[r] # возвращаем корень дерева ✍️ Для того, чтобы закодированный текст можно было потом раскодировать, необходимо вместе с ним в каком-то виде хранить дерево префиксного кода или таблицу частот, по которой дерево можно будет построить заново.\nОптимальное кэширование Под кэшированием (caching) обычно имеют в виду хранение малого количества данных в быстрой памяти, производимое для того, чтобы реже взаимодействовать с медленной памятью. В современных компьютерах кэширование происходит одновременно на многих уровнях: есть кэш процессора, более медленная оперативная память, ещё более медленный жёсткий диск. Сам жёсткий диск используется браузерами для кэширования часто посещаемых веб-страниц: чтение с диска быстрее, чем их повторная загрузка из интернета.\nКэширование тем эффективнее, чем чаще оказывается, что запрашиваемые данные уже находятся в кэше. Алгоритм управления кэшом определяет, какую информацию хранить в кэше, и какую информацию удалять из него, если требуется записать в кэш новые данные.\nСформулируем задачу в абстрактном виде: есть множество из $n$ фрагментов данных, хранящихся в основной памяти. Более быстрая кэш-память способна хранить $k \u003c n$ фрагментов данных; можно считать, что в начале работы алгоритма кэш пустой, либо что он уже содержит какие-то $k$ элементов (дальнейшие рассуждения от этого не зависят). Нужно обработать последовательность обращений к памяти $d_{1}, \\ldots, d_{m}$; алгоритм должен постоянно принимать решение о том, какие элементы хранить в кэше. Запрашиваемый элемент $d_{i}$ читается очень быстро, если он уже находится в кэше. В противном случае его нужно скопировать в кэш из основной памяти; при этом, если кэш заполнен, нужно предварительно удалить из кэша какой-то другой элемент. Такая ситуация называется кэш-промахом (cache miss). Требуется минимизировать количество операций записи в кэш при обработке последовательности запросов к памяти.\nЗаметим, что, вообще говоря, количество операций записи в кэш может не совпадать с количеством кэш-промахов: алгоритм управления кэшом мог бы записывать в кэш элементы, которые понадобятся не прямо сейчас, а когда-нибудь потом. Поймём, почему такие действия бессмысленны: пусть в какой-то момент алгоритм записывает в кэш элемент $x$; если $x$ ни разу не будет запрошен до момента его удаления из кэша (или до конца работы алгоритма), то эту запись можно было просто не производить. Если же $x$ будет запрошен позже, то эту запись можно отложить непосредственно до момента, когда он будет запрошен: ячейка кэш-памяти, которую он занимает, всё равно до этого момента не будет никак использоваться.\nТаким образом, по любому алгоритму мы можем построить его “ленивую” версию, которая записывает элемент в кэш, только если сразу после этого он будет запрошен. При этом она делает не больше операций записи в кэш, чем исходный алгоритм; количество операций записи в кэш для неё совпадает с количеством кэш-промахов.\n✍️ Конечно, на практике алгоритм управления кэшом не обладает информацией о будущих запросах. Тем не менее, обладая этой информацией, можно решить задачу оптимально и получить теоретически минимально возможное количество промахов для данной последовательности запросов. Таким образом, алгоритм, который мы сейчас изучим, используется для оценки качества применяемых на практике алгоритмов.\nАлгоритм Белади Алгоритм Белади (Bélády, 1966) следует следующему правилу: когда нужно записать в кэш элемент $d_{i}$, и свободного места в кэше нет, он удаляет из кэша элемент, который понадобится в следующий раз позже всех остальных.\nОбозначим алгоритм Белади за $B$. Почему $B$ оптимален? Заметим, что $B$ - ленивый. Пусть $S_{0}$ - ленивый алгоритм, делающий минимально возможное количество кэш-промахов на последовательности запросов $d_{1}, \\ldots, d_{m}$. Для каждого $1 \\leqslant i \\leqslant m$ построим $S_{i}$ - ленивый алгоритм, делающий не больше промахов, чем $S_{0}$, и при этом при обработке первых $i$ запросов делающий те же действия, что и $B$. Тогда $S_{m}$ делает то же количество промахов, что и $B$, а значит, $B$ делает не больше промахов, чем $S_{0}$.\nПусть $1 \\leqslant i \\leqslant m, S_{i-1}$ уже построен. При обработке первых $i-1$ запросов $B$ и $S_{i-1}$ делали одни и те же действия, в частности, к моменту обработки $i$-го запроса содержимое кэша для этих алгоритмов совпадает. Если $d_{i}$ уже находится в кэше, или если $B$ и $S_{i-1}$ при записи $d_{i}$ в кэш удаляют из кэша один и тот же элемент, можно взять $S_{i}=S_{i-1}$.\nПусть при записи $d_{i}$ в кэш $S_{i-1}$ удаляет из кэша элемент $a$, а $B$ - элемент $b \\neq a$. Заметим, что $a$ будет запрошен раньше, чем $b$ (либо они оба больше не будут запрошены). Определим $S_{i}$ следующим образом: при обработке первых $i-1$ запросов он ведёт себя как $S_{i-1}$ и $B$; при обработке $i$-го запроса оне ведёт себя как $B$, то есть удаляет из кэша $b$ и записывает на его место $d_{i}$. Далее он ведёт себя как $S_{i-1}$, пока не произойдёт одно из двух событий:\n$S_{i-1}$ удаляет $b$ из кэша, и записывает на его место запрошенный элемент $d_{j}$. В этот момент $S_{i}$ запишет $d_{j}$ в кэш на место $a$; после этого содержимое кэша для $S_{i-1}$ и $S_{i}$ совпадает, поэтому дальше $S_{i}$ просто повторяет те же действия, что и $S_{i-1}$. Запрашивается элемент $d_{j}=a$, и $S_{i}$ удаляет из кэша элемент $x$, чтобы записать на его место $a$. Если $x=b$, то $S_{i-1}$ никак не меняет кэш; если $x \\neq b$, то $S_{i-1}$ удаляет $x$, и записывает на его место $b$. В любом случае, после этого содержимое кэша для $S_{i-1}$ и $S_{i}$ совпадает, поэтому дальше $S_{i}$ ведёт себя, как $S_{i-1}$. Здесь есть тонкость: в случае $x \\neq b$ алгоритм $S_{i}$ ведёт себя не лениво, так как он записывает в кэш $b$ перед запросом $d_{j}=a \\neq b$. Однако такой $S_{i-1}$ можно сделать ленивым, не увеличивая количество операций записи в кэш; переобозначим за $S_{i}$ его ленивую версию. В любом из вышеописанных случаев полученный алгоритм $S_{i}$ является ленивым и делает не больше кэш-промахов, чем $S_{i-1}$, то есть не больше, чем $S_{0}$. ✍️ Многие используемые на практике алгоритмы кэширования основаны на принципе LRU (least recently used): из кэша удаляется элемент, который дольше всех не запрашивался. В каком-то смысле это алгоритм Белади, но с изменённым направлением времени: вместо будущих запросов изучаются предыдущие. Этот принцип часто оказывается эффективным, поскольку для многих приложений характерна локальность обращений (locality of reference) - программа часто продолжает обращаться к данным, к которым обращалась недавно.",
    "description": "Алгоритм Хаффмана Алгоритм Хаффмана (Huffman, 1952) - один из самых известных алгоритмов сжатия текста. Пусть дан текст, состоящий из символов алфавита $\\Sigma$, который мы хотим закодировать как можно более короткой последовательностью бит.\nПрефиксные коды Самый простой способ - кодировать каждый символ уникальной последовательностью из $k$ бит (будем называть такую последовательность кодовым словом (codeword)), где $2^{k} \\geqslant|\\Sigma|$ (чтобы такое кодирование вообще было возможно). Такой способ не всегда эффективен: например, пусть $\\Sigma=\\{a, b, c, d\\}$, текст имеет длину 10000 , но большая часть символов текста равна $a$ (скажем, $a$ встречается в тексте 7000 раз, а остальные символы по 1000 раз). Тогда вышеописанный способ кодирования потребует $20000$ бит. Если же кодовое слово символа $а$ будет равно $0$ , символа $b-10, c-110, d-111$, то суммарно потребуется лишь $7000 \\cdot 1+1000 \\cdot 2+1000 \\cdot 3+1000 \\cdot 3=15000$ бит.",
    "tags": [],
    "title": "15. Жадные алгоритмы",
    "uri": "/basics/greedy_algorithms/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования \u003e  Basics",
    "content": "Остовное дерево (spanning tree), или остов, связного неориентированного графа - это остовный подграф этого графа, являющийся деревом, или, другими словами, связный остовный подграф без циклов. Вес $w(T)$ остова $T$ взвешенного графа - это сумма весов его рёбер.\nПусть дан связный неориентированный граф с неотрицательными весами рёбер, и мы хотим выбрать подмножество рёбер как можно меньшего суммарного веса, с помощью которого можно было бы добраться из любой вершины в любую. Если в подмножестве рёбер есть цикл, из него всегда можно выкинуть одно из рёбер, не увеличив суммарный вес и не нарушив связности. Значит, мы хотим найти остовное дерево минимально возможного веса, или минимальное остовное дерево (minimum spanning tree).\nЛемма о разрезе Поставленную задачу мы научимся решать жадно сразу двумя способами. В основе обоих алгоритмов лежит одна и та же лемма.\nРазрез (cut) $(A, B)$ графа $G=(V, E)$ - это произвольное разбиение множества вершин графа на два непересекающихся множества: $A \\cup B=V, A \\cap B=\\emptyset$. Будем говорить, что ребро $e$ проходит через разрез $(A, B)$, если оно соединяет вершину из $A$ с вершиной из $B$.\nЛемма о разрезе Пусть множество рёбер $F \\subset E$ входит в некоторый минимальный остов графа $G=(V, E)$. Пусть $(A, B)$ - разрез $G$, причём ни одно ребро из $F$ не проходит через разрез $(A, B)$. Пусть $e$ - ребро с наименьшим весом из всех, проходящих через разрез $(A, B)$. Тогда $F \\cup\\{e\\}$ входит в некоторый минимальный остов графа $G$.\nДоказательство Пусть $T=\\left(V, E^{\\prime}\\right)$ - минимальный остов, содержащий $F$. Если $e$ тоже лежит в $T$, то доказывать нечего. Иначе рассмотрим граф $T^{\\prime}=\\left(V, E^{\\prime} \\cup\\{e\\}\\right)$. В $T^{\\prime}$ найдётся цикл, проходящий через ребро $e$, на этом цикле найдётся ещё хотя бы одно проходящее через разрез $(A, B)$ ребро $e^{\\prime}$. При этом $w_{e} \\leqslant w_{e^{\\prime}}$. Тогда $T^{\\prime \\prime}=\\left(V, E^{\\prime} \\cup\\{e\\} \\backslash\\left\\{e^{\\prime}\\right\\}\\right)$ - остов, при этом $w\\left(T^{\\prime \\prime}\\right) \\leqslant w(T), T^{\\prime \\prime}$ содержит $F \\cup\\{e\\}$.\nАлгоритм Прима Алгоритм Прима (Jarnik, 30; Prim, 57; Dijkstra, 59) постепенно расширяет множество вершин $A$ и множество рёбер $F$, образующих дерево на $A$. Вначале $A$ состоит из одной любой вершины, $F$ пусто. На каждом шаге алгоритм находит ребро $e=(u, v)$ минимального веса, ведущее из $A$ в $V \\backslash A$. Конец этого ребра $v \\in V \\backslash A$ алгоритм добавляет в $A$, а само ребро $e-$ к множеству $F$. При этом $F \\cup\\{e\\}$ образует дерево на $A \\cup\\{v\\}$.\nПо лемме о разрезе для множества рёбер $F$ и разреза ( $A, V \\backslash A$ ) если $F$ входило в какой-то минимальный остов, то и $F \\cup\\{e\\}$ тоже входит в какой-то минимальный остов. Когда $A=V$, рёбра $F$ образуют остовное дерево всего графа $G$, при этом $F$ входит в какое-то минимальное остовное дерево. Значит, минимальное остовное дерево состоит ровно из рёбер множества $F$.\nДля того, чтобы быстро находить вершину $v$ и вес ребра $w_{e}$, для каждой вершины $u \\in V$ алгоритм поддерживает вес минимального ребра, ведущего из $A$ в $u$ :\n$$ d_{u}=\\min \\left\\{w_{f}: f: a \\rightarrow u, a \\in A\\right\\} . $$Тогда на очередном шаге $v$ - это вершина из $V \\backslash A$ с минимальным значением $d_{v}$, а само $d_{v}$ - вес ребра $e$. Для перехода к следующему шагу нужно обновить значения $d_{u}$ весами рёбер, исходящих из $v$.\nПолучается алгоритм, практически идентичный алгоритму Дейкстры (меняются лишь приоритеты вершин). Соответственно, в зависимости от реализации, можно получить оценки времени работы $O\\left(V^{2}+E\\right), O((V+E) \\log V)(O(V \\log V+E)$ при использовании фибоначчиевой кучи).\n1 2 3 4 5 6 7 8 9 10 11 12 13 vector\u003cint\u003e d(n) # в графе n вершин fill(d, inf) d[0] = 0 for v = 0..(n - 1): q \u003c-- (v, d[v]) # кладём в очередь вершину v с приоритетом d[v] int W = O # сюда запишем вес минимального остова for i = 0..(n - 1): v \u003c-- q # достаём из очереди вершину с минимальным приоритетом W += d[v] for u, w in es[v]: if d[u] \u003e w: d[u] = w q.decreaseKey(u, d[u]) # уменьшаем значение приоритета u до d[u] Если нужно восстановить не только вес минимального остова, но и множество входящих в него рёбер, нужно для каждой вершины $u$ дополнительно запоминать, весу какого ребра равняется $d_{u}$.\nАлгоритм Краскала Алгоритм Kраскала (Kruskal, 56) действует тоже жадно, но по-другому: он начинает с пустого множества $F$, рассматривает рёбра в порядке возрастания веса и добавляет очередное ребро $e$ в $F$, если при этом в $F$ не появится цикла.\nВ любой момент времени рёбра $F$ образуют лес, и, поскольку граф связен, в конце алгоритма $F$ будет образовывать остовное дерево. Из леммы о разрезе снова следует, что в любой момент времени $F$ входит в какой-то минимальный остов (на этот раз нужно в качестве разреза взять ( $A, V \\backslash A$ ), где $A$ - компонента связности одного из концов ребра $e$ в графе, образованном рёбрами $F$ ).\nДля реализации алгоритма Краскала нам не хватает ещё одной детали: нужно научиться быстро понимать, образуется ли цикл при добавлении ребра $e$ к множеству $F$. Цикл образуется тогда и только тогда, когда концы ребра $e$ находятся в одной компоненте связности графа, образованного рёбрами $F$. Таким образом, наша ближайшая цель научиться быстро проверять, находятся ли две вершины в одной компоненте связности (а также быстро объединять две компоненты связности в одну).\nСистема непересекающихся множеств Система непересекающихся множеств (disjoint set union, DSU) - структура данных, позволяющая поддерживать разбиение $n$ элементов на непересекающиеся множества, а именно:\nвыдавать по элементу идентификатор множества, в котором лежит этот элемент; объединять два множества в одно. Будем считать, что элементы пронумерованы числами от 0 до $n-1$. Необходимо реализовать две функции: get (a) - функцию, возвращающую идентификатор множества, где лежит $a$ (в частности, с помощью этой функции можно проверять, лежат ли два элемента в одном множестве); а также join( $\\mathrm{a}, \\mathrm{b}$ ) - функцию, объединяющую множества, в которых лежат элементы $a$ и $b$. Пусть в случае, когда $a$ и $b$ лежат в одном множестве, функция join(a,b) ничего не делает.\nТакже будем считать, что после инициализации структуры каждое множество состоит из одного элемента и get(i) = i (если в решаемой задаче это не так, можно после инициализации объединить элементы в нужные множества с помощью функции join).\nРеализация связными списками Будем поддерживать $i d[i]$ - номер множества, в котором лежит элемент $i$, а для множества с номером $i$ будем поддерживать список его элементов elems $[i]$. Единственная хитрость: при объединении множеств сохраним для объединённого множеств идентификатор большего множества из объединяемых, тогда нужно поменять значения $i d$ лишь у элементов меньшего множества. Объединение списков же осуществляется за $O(1)$ (достаточно соединить ссылками конец одного списка и начало другого).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 init(n): for i = 0..(n - 1): id[i] = i elems[i] = {i} get(i): return id[i] join(a, b): a = id[a], b = id[b] if a == b: return if len(elems[a]) \u003c len(elems[b]): swap (a, b) for x in elems[b]: id[x] = a elems[a] = concatenate(elems[a], elems[b]) При такой реализации функция get работает за $O(1)$.\nПредложение Суммарное время работы $m$ вызовов јоіп есть $O(m+n \\log n)$.\nДоказательство Время работы одного вызова јоin пропорционально $O(1+x)$, где $x-$ число элементов, у которых поменялось значение $i d$. Заметим, что когда у элемента меняется значение $i d$, размер множества, в котором он находится, увеличивается хотя бы два раза. Значит значение $i d$ каждого элемента поменяется не более $\\log _{2} n$ раз.\nЗаметим, что такая реализация DSU уже позволяет реализовать алгоритм Краскала за время $O(E \\log E+E+E \\log V)=O(E \\log V)$ : первое слагаемое - сортировка рёбер по весу - мажорирует время работы последующих шагов.\n✍️ $O(E \\log E)=O(E \\log V)$, так как $E \\leqslant V^{2}$, если в графе нет кратных рёбер (а если они есть, от них можно предварительно избавиться, оставив лишь самое лёгкое ребро между каждой парой вершин).\nТем не менее, мы изучим ещё одну реализацию DSU, которая даёт лучшую амортизированную оценку времени работы операций, чем реализация связными списками, и ничуть не сложнее с точки зрения технической реализации.\nРеализация деревьями Каждое множество будем хранить в виде подвешенного дерева, вершины которого - это элементы множества. Для каждого элемента $v$ будем хранить ссылку $p[v]$ на родителя в его дереве-множестве; условимся, что для корня дерева ссылка на родителя ведёт на сам корень: $p[v]=v$. В качестве идентификатора множества удобно использовать корень дерева.\nТеперь функция get (v) - это просто подъём в корень дерева, где лежит $v$, а функцию join(a, b) можно реализовать, например, подвесив корень одного дерева ребёнком к корню другого.\n1 2 3 4 5 6 7 8 9 10 11 12 init(n): for i = 0..(n - 1): p[i] = i get(v): if p[v] == v: return v return get(p[v]) join(a, b): a = get(a), b = get(b) if a == b: return p[a] = b Время работы join - это $O(1)$ плюс время работы двух запусков get. K сожалению, время работы get пока в худшем случае оценивается как $\\Theta(n)$, так как высота дерева может равняться его размеру. Для улучшения оценки на время работы get применяются следующие две эвристики.\nРанговая эвристика При объединении деревьев логично подвешивать более низкое дерево к более высокому, тогда высота будет расти медленнее. Для каждой вершины $v$ будем дополнительно хранить ещё одно значение: ранг $r k[v]$, равный высоте поддерева вершины $v$. Вначале $r k[v]=0$ для всех вершин, а при объединении деревьев будем подвешивать корень с меньшим рангом ребёнком к корню с большим рангом. При равенстве рангов неважно, какой корень к какому подвешивать, но надо увеличить ранг корня получившегося дерева на единицу.\n1 2 3 4 5 6 7 8 9 10 11 12 init(n): for i = 0..(n - 1): p[i] = i, rk[i] = 0 join(a, b): a = get(a), b = get(b) if a == b: return if rk[a] \u003e rk[b]: swap (a, b) if rk[a] == rk[b]: rk[b] += 1 p[a] = b Предложение Для рангов вершин выполняются следующие свойства:\nесли $v \\neq p[v]$, то $r k[v]",
    "description": "Остовное дерево (spanning tree), или остов, связного неориентированного графа - это остовный подграф этого графа, являющийся деревом, или, другими словами, связный остовный подграф без циклов. Вес $w(T)$ остова $T$ взвешенного графа - это сумма весов его рёбер.\nПусть дан связный неориентированный граф с неотрицательными весами рёбер, и мы хотим выбрать подмножество рёбер как можно меньшего суммарного веса, с помощью которого можно было бы добраться из любой вершины в любую. Если в подмножестве рёбер есть цикл, из него всегда можно выкинуть одно из рёбер, не увеличив суммарный вес и не нарушив связности. Значит, мы хотим найти остовное дерево минимально возможного веса, или минимальное остовное дерево (minimum spanning tree).",
    "tags": [],
    "title": "Поиск минимального остовного дерева",
    "uri": "/basics/finding_the_minimum_spanning_tree/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Категории",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Краткий курс по основам программирования",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Теги",
    "uri": "/tags/index.html"
  }
]
